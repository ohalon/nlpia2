from torch import nnfrom torch import nnclass PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout=0.1, max_len=5000):        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model)        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2).float() *                             (-math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0).transpose(0, 1)        self.register_buffer('pe', pe)    def forward(self, x):        x = x + self.pe[:x.size(0), :]        return self.dropout(x)import spacyimport spacyspacy_de = spacy.load('de')spacy_de = spacy.load('de')spacy_en = spacy.load('en')spacy_en = spacy.load('en')def tokenize_de(text):    return [tok.text for tok in spacy_de.tokenizer(text)]def tokenize_de(text):    return [tok.text for tok in spacy_de.tokenizer(text)]def tokenize_de(text):    return [tok.text for tok in spacy_de.tokenizer(text)]def tokenize_en(text):    return [tok.text for tok in spacy_en.tokenizer(text)]def tokenize_en(text):    return [tok.text for tok in spacy_en.tokenizer(text)]def tokenize_en(text):    return [tok.text for tok in spacy_en.tokenizer(text)]import torchtextimport torchtextfrom torchtext.datasets import Multi30kfrom torchtext.datasets import Multi30kfrom torchtext.data import Field, BucketIteratorfrom torchtext.data import Field, BucketIteratorSRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)SRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)SRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)SRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)SRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)SRC = Field(tokenize = tokenize_de,            init_token = '<sos>',  #<1>            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)TRG = Field(tokenize = tokenize_en,            init_token = '<sos>',            eos_token = '<eos>',            lower = True,            batch_first = True)device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  #<1>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  #<1>train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),                                                    fields = (SRC, TRG))train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),                                                    fields = (SRC, TRG))train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),                                                    fields = (SRC, TRG))SRC.build_vocab(train_data, min_freq = 2)SRC.build_vocab(train_data, min_freq = 2)TRG.build_vocab(train_data, min_freq = 2)TRG.build_vocab(train_data, min_freq = 2)BATCH_SIZE = 128BATCH_SIZE = 128train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),     batch_size = BATCH_SIZE,     device = device)train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),     batch_size = BATCH_SIZE,     device = device)train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),     batch_size = BATCH_SIZE,     device = device)train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),     batch_size = BATCH_SIZE,     device = device)train_iterator, valid_iterator, test_iterator = BucketIterator.splits(    (train_data, valid_data, test_data),     batch_size = BATCH_SIZE,     device = device)from torch import Tensorfrom torch import Tensorfrom typing import Optional, Anyfrom typing import Optional, Anyclass CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoderLayer(nn.TransformerDecoderLayer):    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,                memory_mask: Optional[Tensor] = None,                tgt_key_padding_mask: Optional[Tensor] = None,                mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:        """Same as DecoderLayer but returns multi-head attention weights.        """        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,                              key_padding_mask=tgt_key_padding_mask)[0]        tgt = tgt + self.dropout1(tgt2)        tgt = self.norm1(tgt)        tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>                                                      attn_mask=memory_mask,                                                      key_padding_mask=mem_key_padding_mask,                                                      need_weights=True)        tgt = tgt + self.dropout2(tgt2)        tgt = self.norm2(tgt)        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))        tgt = tgt + self.dropout3(tgt2)        tgt = self.norm3(tgt)        return tgt, attention_weights  #<2>class CustomDecoder(nn.TransformerDecoder):    def __init__(self, decoder_layer, num_layers, norm=None):         super(CustomDecoder, self).__init__(decoder_layer, num_layers, norm)class CustomDecoder(nn.TransformerDecoder):    def __init__(self, decoder_layer, num_layers, norm=None):         super(CustomDecoder, self).__init__(decoder_layer, num_layers, norm)class CustomDecoder(nn.TransformerDecoder):    def __init__(self, decoder_layer, num_layers, norm=None):         super(CustomDecoder, self).__init__(decoder_layer, num_layers, norm)class CustomDecoder(nn.TransformerDecoder):    def __init__(self, decoder_layer, num_layers, norm=None):         super(CustomDecoder, self).__init__(decoder_layer, num_layers, norm)from einops import rearrange  #<1>from einops import rearrange  #<1>class TranslationTransformer(nn.Transformer):  #<2>class TranslationTransformer(nn.Transformer):  #<2>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,            num_encoder_layers=num_encoder_layers,            num_decoder_layers=num_decoder_layers,            dim_feedforward=dim_feedforward,            dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>        self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);  #<1>from einops import rearrangefrom einops import rearrangeclass TranslationTransformer(nn.Transformer):class TranslationTransformer(nn.Transformer):    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,                 tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,                 num_decoder_layers: int = 6, dim_feedforward: int = 2048,                 dropout: float = 0.1, activation: str = "relu"):        decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,                                           dropout, activation)        decoder_norm = nn.LayerNorm(d_model)        decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)        super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,                                                     num_encoder_layers=num_encoder_layers,                                                     num_decoder_layers=num_decoder_layers,                                                     dim_feedforward=dim_feedforward,                                                     dropout=dropout, custom_decoder=decoder)        self.src_pad_idx = src_pad_idx        self.tgt_pad_idx = tgt_pad_idx        self.device = device        self.src_emb = nn.Embedding(src_vocab_size, d_model)        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)        self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)        self.linear = nn.Linear(d_model, tgt_vocab_size)    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def init_weights(self):        def _init_weights(m):            if hasattr(m, 'weight') and m.weight.dim() > 1:            nn.init.xavier_uniform_(m.weight.data)        self.apply(_init_weights);    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def _make_key_padding_mask(self, t, pad_idx):        mask = (t == pad_idx).to(self.device)        return mask    def prepare_src(self, src, src_pad_idx):        src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)        src = rearrange(src, 'N S -> S N')        src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))        return src, src_key_padding_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def prepare_tgt(self, tgt, tgt_pad_idx):        tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)        tgt = rearrange(tgt, 'N T -> T N')        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)        tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))        return tgt, tgt_key_padding_mask, tgt_mask    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)    def forward(self, src, tgt):        src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)        memory_key_padding_mask = src_key_padding_mask.clone()        output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,                     src_key_padding_mask=src_key_padding_mask,                     tgt_key_padding_mask=tgt_key_padding_mask,                     memory_key_padding_mask=memory_key_padding_mask)        output = rearrange(output, 'T N E -> N T E')        return self.linear(output)SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]model = TranslationTransformer(device=device,                           src_vocab_size=len(SRC.vocab), src_pad_idx=SRC_PAD_IDX,                           tgt_vocab_size=len(TRG.vocab), tgt_pad_idx=TRG_PAD_IDX).to(device)model = TranslationTransformer(device=device,                           src_vocab_size=len(SRC.vocab), src_pad_idx=SRC_PAD_IDX,                           tgt_vocab_size=len(TRG.vocab), tgt_pad_idx=TRG_PAD_IDX).to(device)model = TranslationTransformer(device=device,                           src_vocab_size=len(SRC.vocab), src_pad_idx=SRC_PAD_IDX,                           tgt_vocab_size=len(TRG.vocab), tgt_pad_idx=TRG_PAD_IDX).to(device)model = TranslationTransformer(device=device,                           src_vocab_size=len(SRC.vocab), src_pad_idx=SRC_PAD_IDX,                           tgt_vocab_size=len(TRG.vocab), tgt_pad_idx=TRG_PAD_IDX).to(device)model.init_weights()model.init_weights()src = torch.randint(1, 100, (10, 5)).to('cuda')  #<1>src = torch.randint(1, 100, (10, 5)).to('cuda')  #<1>tgt = torch.randint(1, 100, (10, 7)).to('cuda')tgt = torch.randint(1, 100, (10, 7)).to('cuda')with torch.no_grad():    output = model(src, tgt)  #<2>with torch.no_grad():    output = model(src, tgt)  #<2>with torch.no_grad():    output = model(src, tgt)  #<2>print(output.shape)print(output.shape)LEARNING_RATE = 0.0001LEARNING_RATE = 0.0001optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)  <1>criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)  <1>def train(model, iterator, optimizer, criterion, clip):    model.train()  #<1>    epoch_loss = 0def train(model, iterator, optimizer, criterion, clip):    model.train()  #<1>    epoch_loss = 0def train(model, iterator, optimizer, criterion, clip):    model.train()  #<1>    epoch_loss = 0def train(model, iterator, optimizer, criterion, clip):    model.train()  #<1>    epoch_loss = 0def train(model, iterator, optimizer, criterion, clip):    model.train()  #<1>    epoch_loss = 0    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)    for i, batch in enumerate(iterator):        src = batch.src        trg = batch.trg        optimizer.zero_grad()        output = model(src, trg[:,:-1])  #<2>        output_dim = output.shape[-1]        output = output.contiguous().view(-1, output_dim)        trg = trg[:,1:].contiguous().view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(iterator)def evaluate(model, iterator, criterion):def evaluate(model, iterator, criterion):def epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsdef epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsdef epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsdef epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsdef epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsdef epoch_time(start_time, end_time):    elapsed_time = end_time - start_time    elapsed_mins = int(elapsed_time / 60)    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))    return elapsed_mins, elapsed_secsN_EPOCHS = 15N_EPOCHS = 15CLIP = 1CLIP = 1BEST_MODEL_FILE = 'best_model.pytorch'BEST_MODEL_FILE = 'best_model.pytorch'best_valid_loss = float('inf')best_valid_loss = float('inf')for epoch in range(N_EPOCHS):for epoch in range(N_EPOCHS):model.load_state_dict(torch.load(BEST_MODEL_FILE))model.load_state_dict(torch.load(BEST_MODEL_FILE))test_loss = evaluate(model, test_iterator, criterion)test_loss = evaluate(model, test_iterator, criterion)print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):example_idx = 10example_idx = 10src = vars(test_data.examples[example_idx])['src']src = vars(test_data.examples[example_idx])['src']trg = vars(test_data.examples[example_idx])['trg']trg = vars(test_data.examples[example_idx])['trg']print(f'src = {src}')print(f'src = {src}')print(f'trg = {trg}')print(f'trg = {trg}')translation, attention = translate_sentence(src, SRC, TRG, model, device)translation, attention = translate_sentence(src, SRC, TRG, model, device)print(f'translation = {translation}')print(f'translation = {translation}')import matplotlib.pyplot as pltimport matplotlib.pyplot as pltimport matplotlib.ticker as tickerimport matplotlib.ticker as tickerdef display_attention(sentence, translation, attention_weights):    n_attention = len(attention_weights)def display_attention(sentence, translation, attention_weights):    n_attention = len(attention_weights)def display_attention(sentence, translation, attention_weights):    n_attention = len(attention_weights)display_attention(src, translation, attention_weights)display_attention(src, translation, attention_weights)example_idx = 25example_idx = 25src = vars(valid_data.examples[example_idx])['src']src = vars(valid_data.examples[example_idx])['src']trg = vars(valid_data.examples[example_idx])['trg']trg = vars(valid_data.examples[example_idx])['trg']print(f'src = {src}')print(f'src = {src}')print(f'trg = {trg}')print(f'trg = {trg}')translation, attention = translate_sentence(src, SRC, TRG, model, device)translation, attention = translate_sentence(src, SRC, TRG, model, device)print(f'translation = {translation}')print(f'translation = {translation}')display_attention(src, translation, attention)display_attention(src, translation, attention)from torchtext.data.metrics import bleu_scorefrom torchtext.data.metrics import bleu_scoredef calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)print(f'BLEU score = {bleu_score*100:.2f}')print(f'BLEU score = {bleu_score*100:.2f}')from transformers import BertModelfrom transformers import BertModelmodel = BertModel.from_pretrained('bert-base-uncased')model = BertModel.from_pretrained('bert-base-uncased')print(model)print(model)import pandas as pdimport pandas as pddf = pd.read_csv('data/train.csv')  # <1>df = pd.read_csv('data/train.csv')  # <1>df.head()df.head()df.shapedf.shapefrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import train_test_splitrandom_state=42random_state=42labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']X = df[['comment_text']]X = df[['comment_text']]y = df[labels]y = df[labels]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,                                                    random_state=random_state)  # <1>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,                                                    random_state=random_state)  # <1>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,                                                    random_state=random_state)  # <1>def get_dataset(X, y):    data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]    return pd.DataFrame(data, columns=['text', 'labels'])def get_dataset(X, y):    data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]    return pd.DataFrame(data, columns=['text', 'labels'])def get_dataset(X, y):    data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]    return pd.DataFrame(data, columns=['text', 'labels'])def get_dataset(X, y):    data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]    return pd.DataFrame(data, columns=['text', 'labels'])train_df = get_dataset(X_train, y_train)train_df = get_dataset(X_train, y_train)eval_df = get_dataset(X_test, y_test)eval_df = get_dataset(X_test, y_test)train_df.shape, eval_df.shapetrain_df.shape, eval_df.shapetrain_df.head()  # <1>train_df.head()  # <1>import loggingimport logginglogging.basicConfig(level=logging.INFO)  # <1>logging.basicConfig(level=logging.INFO)  # <1>model_type = 'bert'  # <2>model_type = 'bert'  # <2>model_name = 'bert-base-cased'model_name = 'bert-base-cased'output_dir = f'{model_type}-example1-outputs'output_dir = f'{model_type}-example1-outputs'model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}model_args = {    'output_dir': output_dir, # where to save results    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir    'manual_seed': random_state, # <3>    'no_cache': True,}from sklearn.metrics import roc_auc_scorefrom sklearn.metrics import roc_auc_scorefrom simpletransformers.classification import MultiLabelClassificationModelfrom simpletransformers.classification import MultiLabelClassificationModelmodel = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)  # <1>model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)  # <1>model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)  # <1>model.train_model(train_df=train_df)model.train_model(train_df=train_df)result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=roc_auc_score) # <1>result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=roc_auc_score) # <1>resultresultfrom preprocessing.preprocessing import TextPreprocessorfrom preprocessing.preprocessing import TextPreprocessortp = TextPreprocessor()tp = TextPreprocessor()df = df.rename(columns={'comment_text':'original_text'})df = df.rename(columns={'comment_text':'original_text'})df['comment_text'] = df['original_text'].apply(lambda x: tp.preprocess(x)) # <1>df['comment_text'] = df['original_text'].apply(lambda x: tp.preprocess(x)) # <1>pd.set_option('display.max_colwidth', 45)pd.set_option('display.max_colwidth', 45)df[['original_text', 'comment_text']].head()df[['original_text', 'comment_text']].head()model_type = 'bert'model_type = 'bert'model_name = 'bert-base-cased'model_name = 'bert-base-cased'output_dir = f'{model_type}-example2-outputs'  # <1>output_dir = f'{model_type}-example2-outputs'  # <1>best_model_dir = f'{output_dir}/best_model'best_model_dir = f'{output_dir}/best_model'model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model_args = {    'output_dir': output_dir,    'overwrite_output_dir': True,    'manual_seed': random_state,    'no_cache': True,    'best_model_dir': best_model_dir,    'max_seq_length': 300,    'train_batch_size': 24,    'eval_batch_size': 24,    'gradient_accumulation_steps': 1,    'learning_rate': 5e-5,    'evaluate_during_training': True,    'evaluate_during_training_steps': 1000,    'save_eval_checkpoints': False,    "save_model_every_epoch": False,    'save_steps': -1,  # saving model unnecessarily takes time during training    'reprocess_input_data': True,    'num_train_epochs': 5,    'use_early_stopping': True,    'early_stopping_patience': 4,    'early_stopping_delta': 0,}model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),                                      args=model_args)model.train_model(train_df=train_df, eval_df=eval_df, acc=roc_auc_score,                  show_running_loss=False, verbose=False)model.train_model(train_df=train_df, eval_df=eval_df, acc=roc_auc_score,                  show_running_loss=False, verbose=False)model.train_model(train_df=train_df, eval_df=eval_df, acc=roc_auc_score,                  show_running_loss=False, verbose=False)best_model = MultiLabelClassificationModel(model_type, best_model_dir,                                           num_labels=len(labels), args=model_args)best_model = MultiLabelClassificationModel(model_type, best_model_dir,                                           num_labels=len(labels), args=model_args)best_model = MultiLabelClassificationModel(model_type, best_model_dir,                                           num_labels=len(labels), args=model_args)result, model_outputs, wrong_predictions = best_model.eval_model(eval_df, acc=roc_auc_score)result, model_outputs, wrong_predictions = best_model.eval_model(eval_df, acc=roc_auc_score)resultresult