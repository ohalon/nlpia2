{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b60b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Trust me, though, the words were on their way, and when \"\n",
    "...         \"they arrived, Liesel would hold them in her hands like \"\n",
    "...         \"the clouds, and she would wring them out, like the rain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17460a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.3.0\r\n",
      "anyio==3.6.2\r\n",
      "argon2-cffi==21.3.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "asttokens==2.2.0\r\n",
      "astunparse==1.6.3\r\n",
      "attrs==22.1.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.11.1\r\n",
      "bleach==5.0.1\r\n",
      "blis==0.7.9\r\n",
      "cachetools==5.2.0\r\n",
      "catalogue==2.0.8\r\n",
      "certifi==2022.12.7\r\n",
      "cffi==1.15.1\r\n",
      "charset-normalizer==2.1.1\r\n",
      "click==8.1.3\r\n",
      "confection==0.0.3\r\n",
      "contourpy==1.0.6\r\n",
      "cycler==0.11.0\r\n",
      "cymem==2.0.7\r\n",
      "debugpy==1.6.4\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl\r\n",
      "entrypoints==0.4\r\n",
      "executing==1.2.0\r\n",
      "fastjsonschema==2.16.2\r\n",
      "flatbuffers==22.12.6\r\n",
      "fonttools==4.38.0\r\n",
      "gast==0.4.0\r\n",
      "google-auth==2.15.0\r\n",
      "google-auth-oauthlib==0.4.6\r\n",
      "google-pasta==0.2.0\r\n",
      "grpcio==1.51.1\r\n",
      "h5py==3.7.0\r\n",
      "idna==3.4\r\n",
      "ipykernel==6.17.1\r\n",
      "ipython==8.7.0\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.7.2\r\n",
      "jedi==0.18.2\r\n",
      "Jinja2==3.1.2\r\n",
      "joblib==1.2.0\r\n",
      "jsonschema==4.17.3\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-console==6.4.4\r\n",
      "jupyter-contrib-core==0.4.2\r\n",
      "jupyter-contrib-nbextensions==0.7.0\r\n",
      "jupyter-highlight-selected-word==0.2.0\r\n",
      "jupyter-nbextensions-configurator==0.6.1\r\n",
      "jupyter-server==1.23.3\r\n",
      "jupyter_client==7.4.7\r\n",
      "jupyter_core==4.12.0\r\n",
      "jupyterlab-pygments==0.2.2\r\n",
      "jupyterlab-widgets==1.1.1\r\n",
      "keras==2.11.0\r\n",
      "kiwisolver==1.4.4\r\n",
      "langcodes==3.3.0\r\n",
      "libclang==14.0.6\r\n",
      "libxml2-python==2.10.3\r\n",
      "lxml==4.9.1\r\n",
      "Markdown==3.4.1\r\n",
      "MarkupSafe==2.1.1\r\n",
      "matplotlib==3.6.2\r\n",
      "matplotlib-inline==0.1.6\r\n",
      "mistune==2.0.4\r\n",
      "murmurhash==1.0.9\r\n",
      "nbclassic==0.4.8\r\n",
      "nbclient==0.7.2\r\n",
      "nbconvert==7.2.5\r\n",
      "nbformat==5.7.0\r\n",
      "nest-asyncio==1.5.6\r\n",
      "nltk==3.8\r\n",
      "notebook==6.5.2\r\n",
      "notebook_shim==0.2.2\r\n",
      "numpy==1.23.5\r\n",
      "oauthlib==3.2.2\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging==21.3\r\n",
      "pandas==1.5.2\r\n",
      "pandocfilters==1.5.0\r\n",
      "parso==0.8.3\r\n",
      "pathy==0.10.1\r\n",
      "pexpect==4.8.0\r\n",
      "pi==0.1.2\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==9.3.0\r\n",
      "preshed==3.0.8\r\n",
      "prometheus-client==0.15.0\r\n",
      "prompt-toolkit==3.0.33\r\n",
      "protobuf==3.19.6\r\n",
      "psutil==5.9.4\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure-eval==0.2.2\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycparser==2.21\r\n",
      "pydantic==1.10.2\r\n",
      "Pygments==2.13.0\r\n",
      "pyparsing==3.0.9\r\n",
      "pyrsistent==0.19.2\r\n",
      "python-dateutil==2.8.2\r\n",
      "pytz==2022.7\r\n",
      "PyYAML==6.0\r\n",
      "pyzmq==24.0.1\r\n",
      "qtconsole==5.4.0\r\n",
      "QtPy==2.3.0\r\n",
      "regex==2022.10.31\r\n",
      "requests==2.28.1\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "rsa==4.9\r\n",
      "scikit-learn==1.2.0\r\n",
      "scipy==1.9.3\r\n",
      "seaborn==0.12.1\r\n",
      "Send2Trash==1.8.0\r\n",
      "six==1.16.0\r\n",
      "smart-open==6.2.0\r\n",
      "sniffio==1.3.0\r\n",
      "soupsieve==2.3.2.post1\r\n",
      "spacy==3.4.4\r\n",
      "spacy-legacy==3.0.10\r\n",
      "spacy-loggers==1.0.4\r\n",
      "srsly==2.4.5\r\n",
      "stack-data==0.6.2\r\n",
      "tensorboard==2.11.0\r\n",
      "tensorboard-data-server==0.6.1\r\n",
      "tensorboard-plugin-wit==1.8.1\r\n",
      "tensorflow==2.11.0\r\n",
      "tensorflow-estimator==2.11.0\r\n",
      "tensorflow-io-gcs-filesystem==0.29.0\r\n",
      "termcolor==2.1.1\r\n",
      "terminado==0.17.0\r\n",
      "thinc==8.1.5\r\n",
      "threadpoolctl==3.1.0\r\n",
      "tinycss2==1.2.1\r\n",
      "tornado==6.2\r\n",
      "tqdm==4.64.1\r\n",
      "traitlets==5.6.0\r\n",
      "typer==0.7.0\r\n",
      "typing_extensions==4.4.0\r\n",
      "urllib3==1.26.13\r\n",
      "wasabi==0.10.1\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.4.2\r\n",
      "Werkzeug==2.2.2\r\n",
      "widgetsnbextension==3.6.1\r\n",
      "wrapt==1.14.1\r\n",
      "zmq==0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54528564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b00c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trust', 'me,', 'though,', 'the', 'words', 'were', 'on', 'their']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "tokens[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4918a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n",
    "texts = []\n",
    "texts.append(\"There's no such thing as survival of the fittest. \"  \"Survival of the most adequate, maybe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1452dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(re.findall(pattern, texts[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5760f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There's\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58497d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[8:16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924d8d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maybe', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1cd7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\w+(?:\\\\'\\\\w+){0,2}|[^\\\\w\\\\s]\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r'\\w+(?:\\'\\w+){0,2}|[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0bf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab = sorted(set(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4807d6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", . Survival There's adequate as fittest maybe most no of such\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(vocab[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af4a723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(tokens)\n",
    "num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "843c443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e8d69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 09:11:44.446378: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-30 09:11:44.791883: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-30 09:11:44.863462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:44.863504: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-30 09:11:45.862554: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:45.862622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:45.862625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-30 09:11:48.853393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-30 09:11:48.854123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854611: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.854960: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.855069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-30 09:11:48.855086: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73804dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: jinja2 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/linuxbrew/.linuxbrew/Cellar/jupyterlab/3.4.8/libexec/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python3.10 -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca70ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/maer/.local/lib/python3.10/site-packages (22.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999503e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6da1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4266d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b55ede6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tok.text for tok in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5d01ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There', \"'s\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "073ea178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[9:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04695dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's no such thing as survival of the fittest. Survival of the most adequate, maybe.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b3eb206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's no such thing as survival of the fittest. Survival of the most adequate, maybe.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e21029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There's no such thing as survival of the fittest. Survival of the most adequate, maybe."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e62e8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "492b7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(doc.sents)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cb05410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/spacy/displacy/__init__.py:103: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e47d455b75bd4671a847bb71d0baf953-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">There</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">no</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">such</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">thing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">as</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">survival</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">fittest.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e47d455b75bd4671a847bb71d0baf953-0-8\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e47d455b75bd4671a847bb71d0baf953-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,266.5 L1628.0,254.5 1612.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(sentence, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ea9c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GFX1-]: glxtest: VA-API test failed: failed to initialise VAAPI connection.\n",
      "ATTENTION: default value of option mesa_glthread overridden by environment.\n",
      "ATTENTION: default value of option mesa_glthread overridden by environment.\n",
      "ATTENTION: default value of option mesa_glthread overridden by environment.\n",
      "ATTENTION: default value of option mesa_glthread overridden by environment.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2022-12-30T08:12:54Z \u001b[0m\u001b[1m\u001b[31mERROR\u001b[0m glean_core::metrics::ping\u001b[0m\u001b[38;5;8m]\u001b[0m Invalid reason code startup for ping background-update\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!firefox 127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8b3c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "191272ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41 ms ± 41.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "f = \"Chapter 02 -- Tokens of thought (natural language words).adoc\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "%timeit nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07393537",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(f).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eba9163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= Tokens of thought (natural language words)\\n:chapter: 2\\n:part: 1\\n:imagesdir: .\\n:xrefstyle: short\\n:figure-caption: Figure {chapter}.\\n:listing-caption: Listing {chapter}.\\n:table-caption: Table {chapter}.\\n:stem: latexmath\\n\\nThis chapter covers\\n\\n* Parsing your text into words and _n_-grams (tokens)\\n* Tokenizing punctuation, emoticons, and even Chinese characters\\n* Consolidating your vocabulary with stemming, lemmatization, and case folding\\n* Building a structured numerical representation of natural language text\\n* Scoring text for sentiment and prosocial intent\\n* Using character frequency analysis to optimize your token vocabulary\\n* Dealing with variable length sequences of words and tokens\\n\\n\\nSo you want to help save the world with the power of natural language processing (NLP)?\\nFirst your NLP pipeline will need to compute something about text, and for that you\\'ll need a way to represent text in a numerical data structure.\\nThe part of an NLP pipeline that breaks up your text to create this structured numerical data is called a _parser_.\\nFor many NLP applications, you only need to convert your text to a sequence of words, and that can be enough for searching and classifying text.\\n\\nYou will now learn how to split a document, any string, into discrete tokens of meaning.\\nYou will be able to parse text documents as small as a single word and as large as an entire Encyclopedia.\\nAnd they will all produce a consistent representation that you can use to compare them.\\nFor this chapter your tokens will be words, punctuation marks, and even pictograms such as Chinese characters, emojis and emoticons.\\n\\nLater in the book you will see that you can use these same techniques to find packets of meaning in any discrete sequence.\\nFor example, your tokens could be the ASCII characters represented by a sequence of bytes, perhaps with ASCII emoticons.\\nOr they could be Unicode emojis, mathematical symbols, Egyption, hieroglyphics, pictographs from languages like Kanji  and Cantonese.\\nYou could even define the tokens for DNA and RNA sequences with letters for each of the five base nucleotides: adenine (A), guanine (G), cytosine \\\\(C), thymine (T), and uracil (U).\\nNatural language sequences of tokens are all around you ... and even inside you.\\n\\nIs there something you can do with tokens that doesn\\'t require a lot of complicated deep learning?\\nIf you have a good tokenizer you can use it to identify statistics about the occurrence of tokens in a set of documents, such as your blog posts or a business website.\\nThen you can build a search engine in pure Python with just a dictionary to represent to record links to the set of documents where those words occur.\\nThat Python dictionary that maps words to document links or pages is called a reverse index.\\nIt\\'s just like the index at the back of this book.\\nThis is called _information retrieval_ -- a really powerful tool in your NLP toolbox.\\n\\nStatistics about tokens are often all you need for keyword detection, full text search, and information retrieval.\\nYou can even build customer support chatbots using text search to find answers to customers\\' questions in your documentation or FAQ (frequently asked question) lists.\\nA chatbot can\\'t answer your questions until it knows where to look for the answer.\\nSearch is the foundation of many state of the art applications such as conversational AI and open domain question answering.\\nA tokenizer forms the foundation for almost all NLP pipelines.\\n\\n=== Tokens of emotion\\n\\nAnother practical use for your tokenizer is called _sentiment analysis_, or analysis of text to estimate emotion.\\nYou\\'ll see an example of a sentiment analysis pipeline later in this chapter.\\nFor now you just need to know how to build a tokenizer.\\nAnd your tokenizer will almost certainly need to handle the tokens of emotion called _emoticons_ and _emojis_.\\n\\n_Emoticons_ are a textual representations of a writer\\'s mood or facial expression, such as the _smiley_ emoticon: `:-)`.\\nThey are kind-of like a modern hieroglyph or picture-word for computer users that only have access to an ASCII terminal for communication.\\n_Emojis_ are the graphical representation of these characters.\\nFor example, the smilie emoji has a small yellow circle with two black dots for eyes and a U shaped curve for a mouth.\\nThe smiley emoji is a graphical representation of the `:-)` smiley emoticon.\\n\\nBoth emojis and emoticons have evolved into their own language.\\nThere are hundreds of popular emojis.\\nPeople have created emojis for everything from company logos to memes and innuendo.\\nNoncommercial social media networks such Mastodon even allow you to create your own custom emojis.footnote:[Mastodon servers you can join (https://proai.org/mastoserv)] footnote:[Mastodon custom emoji documentation (https://docs.joinmastodon.org/methods/custom_emojis/)] \\n\\n.Emojis and Emoticons\\n[NOTE]\\n====\\n_Emoticons_ were first typed into an ASCII text message in 1972 when Carnegie Mellon researchers mistakenly understood a text message about a mercury spill to be a joke.\\nThe professor, Dr. Scott E. Fahlman, suggested that `:-)` should be appended to messages that were jokes, and  `:-(` emoticons should be used for serious warning messages. \\nGosh, how far we\\'ve come.\\n====\\n\\nThe plural of \"emoji\" is either \"emoji\" (like \"sushi\") or \"emojis\" (like \"Tsunamis\"), however the the Atlantic and NY Times style editors prefer \"emojis\" to avoid ambiguity.\\nYour NLP pipeline will learn what you mean no matter how you type it.\\n\\nimage::../images/ch02/wikipedia-smiley-icon.svg[alt=\"Smiley icon from wikipedia article en.wikipedia.org/wiki/Smiley\",align=\"center\",width=100%,link=\"../images/ch02/wikipedia-smiley-icon.svg\"]\\n\\n== What is a token?\\n\\nA token can be almost any chunk of text that you want to treat as a packet of thought and emotion.\\nSo you need to break your text into chunks that capture individual thoughts.\\nYou may be thinking that _words_ are the obvious choice for tokens.\\nSo that\\'s what you will start with here.\\nYou\\'ll also learn how to include punctuation marks, emojis, numbers, and other word-like things in your vocabulary of words.\\nLater you\\'ll see that you can use these same techniques to find packets of meaning in any discrete sequence.\\nAnd later you will learn some even more powerful ways to split discrete sequences into meaningful packets.\\nYour tokenizers will be soon able to analyze and structure any text document or string, from a single word, to a sentence, to an entire book.\\n\\nThink about a collection of documents, called a _corpus_, that you want to process with NLP.\\nThink about the _vocabulary_ that would be important to your NLP algorithm -- the set of tokens you will need to keep track of.\\nFor example your tokens could be the characters for ASCII emoticons, if this is what is important in your NLP pipeline for a particular corpus.\\nOr your tokens could be Unicode emojis, mathematical symbols, hieroglyphics, even pictographs like Kanji and Cantonese characters.\\nYour tokenizer and your NLP pipeline would even be useful for the nucleotide sequences of DNA and RNA where your tokens might be A, C, T, G, U, and so on.\\nAnd neuroscientists sometimes create sequences of discrete symbols to represent neurons firing in your brain when you read text like this sentence.\\nNatural language sequences of tokens are inside you, all around you, and flowing through you.\\nSoon you\\'ll be flowing streams of tokens through your machine learning NLP pipeline.\\n\\nRetrieving tokens from a document will require some string manipulation beyond just the `str.split()` method employed in chapter 1.\\nYou\\'ll probably want to split contractions like \"you\\'ll\" into the words that were combined to form them, perhaps \"you\" and \"\\'ll\", or perhaps \"you\" and \"will.\"\\nYou\\'ll want to separate punctuation from words, like quotes at the beginning and end of quoted statements or words, such as those in the previous sentence.\\nAnd you need to treat some punctuation such as dashes (\"-\") as part of singly-hyphenated compound words such as \"singly-hyphenated.\"\\n\\nOnce you have identified the tokens in a document that you would like to include in your vocabulary, you will return to the regular expression toolbox to build a tokenizer.\\nAnd you can use regular expressions combine different forms of a word into a single token in your vocabulary -- a process called _stemming_.\\nThen you will assemble a vector representation of your documents called a _bag of words_.\\nFinally, you will try to use this bag of words vector to see if it can help you improve upon the basic greeting recognizer at the end of chapter 1.\\n\\n=== Alternative tokens\\n\\nWords aren\\'t the only packets of meaning we could use for our tokens.\\nThink for a moment about what a word or token represents to you.\\nDoes it represent a single concept, or some blurry cloud of concepts?\\nCould you always be sure to recognize where a word begins and ends?\\nAre natural language words like programming language keywords that have precise spellings, definitions and grammatical rules for how to use them?\\nCould you write software that reliably recognizes a word?\\n\\nDo you think of \"ice cream\" as one word or two?\\nOr maybe even three?\\nAren\\'t there at least two entries in your mental dictionary for \"ice\" and \"cream\" that are separate from your entry for the compound word \"ice cream\"?\\nWhat about the contraction \"don\\'t\"?\\nShould that string of characters be split into one, or two, or even three packets of meaning?\\n\\nYou might even want to divide words into even smaller meaningful parts.\\nWord pieces such as the prefix \"pre\", the suffix \"fix\", or the interior syllable \"la\" all have meaning.\\nYou can use these word pieces to transfer what you learn about the meaning of one word to another similar word in your vocabulary.\\nYour NLU pipeline can even use these pieces to understand new words.\\nAnd your NLG pipeline can use the pieces to create new words that succinctly capture ideas or memes circulating in the collective consciousness.\\n\\nYour pipeline could break words into even smaller pieces.\\nLetters, characters, or graphemes footnote:[(https://en.wikipedia.org/wiki/Grapheme)] carry sentiment and meaning too!footnote:[Suzi Park and Hyopil Shin _Grapheme-level Awareness in Word Embeddings for Morphologically Rich Languages_ (https://www.aclweb.org/anthology/L18-1471.pdf)]\\nWe haven\\'t yet found the perfect encoding for packets of thought.\\nAnd machines compute differently than brains.\\nWe explain language and concepts to each other in terms of words or terms.\\nBut machines can often see patterns in the use of characters that we miss.\\nAnd for machines to be able to squeeze huge vocabularies into their limited RAM there are more efficient encodings for natural language.\\n\\nThe optimal tokens for efficient computation are different from the packets of thought (words) that we humans use.\\nByte Pair Encoding (BPE), Word Piece Encoding, and Sentence Piece Encoding, each can help machines use natural language more efficiently.\\nBPE finds the optimal groupings of characters (bytes) for your particular set of documents and strings.\\nIf you want an *explainable* encoding, use the word tokenizers of the previous sections.\\nIf you want more flexible and accurate predictions and generation of text, then BPE, WPE, or SPE may be better for your application.\\nLike the bias variance trade-off, there\\'s often a explainability/accuracy trade-off in NLP.\\n\\nWhat about invisible or implied words?\\nCan you think of additional words that are implied by the single-word command \"Don\\'t!\"?\\nIf you can force yourself to think like a machine and then switch back to thinking like a human, you might realize that there are three invisible words in that command.\\nThe single statement \"Don\\'t!\" means \"Don\\'t you do that!\" or \"You, do not do that!\"\\nThat\\'s at least three hidden packets of meaning for a total of five tokens you\\'d like your machine to know about.\\n\\nBut don\\'t worry about invisible words for now.\\nAll you need for this chapter is a tokenizer that can recognize words that are spelled out.\\nYou will worry about implied words and connotation and even meaning itself in chapter 4 and beyond.footnote:[If you want to learn more about exactly what a \"word\" really is, check out the introduction to _The Morphology of Chinese_ by Jerome Packard where he discusses the concept of a \"word\" in detail. The concept of a \"word\" did not exist at all in the Chinese language until the 20th century when it was translated from English grammar into Chinese.]\\n\\nYour NLP pipeline can start with one of these five options as your tokens:\\n\\n1. **Bytes** - ASCII characters\\n2. **Characters** - multi-byte Unicode characters\\n3. **Subwords** (Word pieces) - syllables and common character clusters\\n4. **Words** - dictionary words or their roots (stems, lemmas)\\n5. **Sentence pieces** - short, common word and multi-word pieces\\n\\nAs you work your way down this list your vocabulary size increases and your NLP pipeline will need more and more data to train.\\nCharacter-based NLP pipelines are often used in translation problems or NLG tasks that need to generalize from a modest number of examples.\\nThe number of possible words that your pipeline can deal with is called its _vocabulary_.\\nA character-based NLP pipeline typically needs fewer than 200 possible tokens to process many Latin-based languages.\\nThat small vocabulary ensures that byte- and character-based NLP pipelines can handle new unseen test examples without too many meaningless OOV (out of vocabulary) tokens.\\n\\nFor word-based NLP pipelines your pipeline will need to start paying attention to how often tokens are used before deciding whether to \"count it.\"\\nYou don\\'t want you pipeline to do anything meaningful with junk words such `asdf` - the \\nBut even if you make sure your pipeline on pays attention to words that occur a lot, you could end up with a vocabulary that\\'s as large as a typical dictionary - 20 to 50 thousand words.\\n\\nSubwords are the optimal token to use for most Deep Learning NLP pipelines.\\nSubword (Word piece) tokenizers are built into many state of the art transformer pipelines.\\nWords are the token of choice for any linguistics project or academic research where your results need to be interpretable and explainable.\\n\\nSentence pieces take the subword algorithm to the extreme.\\nThe sentence piece tokenizer allows your algorithm to combine multiple word pieces together into a single token that can sometimes span multiple words.\\nThe only hard limit on sentence pieces is that they do not extend past the end of a sentence.\\nThis ensures that the meaning of a token is associated with only a single coherent thought and is useful on single sentences as well as longer documents.W\\n\\n==== _N_-grams\\n\\nNo matter which kind of token you use for your pipeline, you will likely extract pairs, triplets, quadruplets, and even quintuplets of tokens.\\nThese are called _n_-grams_.footnote:[Pairs of adjacent words are called 2-grams or bigrams. Three words in sequency are called 3-grams or trigrams. Four words in a row are called 4-grams.  5-grams are probably the longest _n_-grams you\\'ll find in an NLP pipeline. Google counts all the 1 to 5-grams in nearly all the books ever written (https://books.google.com/ngrams).]\\nUsing _n_-grams enables your machine to know about the token \"ice cream\" as well as the individual tokens \"ice\" and \"cream\" that make it up.\\nAnother 2-gram that you\\'d like to keep together is \"Mr. Smith\".\\nYour tokens and your vector representation of a document will likely want to have a place for \"Mr. Smith\" along with \"Mr.\" and \"Smith.\"\\n\\nYou will start with a short list of keywords as your vocabulary.\\nThis helps to keep your data structures small and understandable and can make it easier to explain your results.\\nExplainable models create insights that you can use to help your stakeholders, hopefully the users themselves (rather than investors), accomplish their goals.\\n\\nFor now, you can just keep track of all the short _n_-grams of words in your vocabulary.\\nBut in chapter 3, you will learn how to estimate the importance of words based on their document frequency, or how often they occur.\\nThat way you can filter out pairs and triplets of words that rarely occur together.\\nYou will find that the approaches we show are not perfect.\\nFeature extraction can rarely retain all the information content of the input data in any machine learning pipeline.\\nThat is part of the art of NLP, learning when your tokenizer needs to be adjusted to extract more or different information from your text for your particular applications.\\n\\nIn natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\\nNonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\\nThe techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email technology from a flood of spam that almost made it useless.\\n\\n== Challenges (a preview of stemming)\\n\\nAs an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\\nVery smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\\nImagine how difficult that is.\\nImagine trying to remove verb endings like \"ing\" from \"ending\" so you would have a stem called \"end\" to represent both words.\\nAnd you would like to stem the word \"running\" to \"run,\" so those two words are treated the same.\\nAnd that is tricky because you have removed not only the \"ing\" but also the extra \"n.\"\\nBut you want the word \"sing\" to stay whole.\\nYou would not want to remove the \"ing\" ending from \"sing\" or you would end up with a single-letter \"s.\"\\n\\nOr imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens.\"\\nDo isolated individual letters in a word or parts of a word provide any information at all about that word\\'s meaning?\\nCan the letters be misleading?\\nYes and yes.\\n\\nIn this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\\nLater, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you are interested in.\\nFrom that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\\n\\n=== Tokenization\\n\\nIn NLP, _tokenization_ is a particular kind of document _segmentation_.\\nSegmentation breaks up text into smaller chunks or segments.\\nThe segments of text have less information than the whole.\\nDocuments can be segmented into paragraphs, paragraphs into sentences, sentences into phrases, and phrases into tokens (usually words and punctuation).\\nIn this chapter, we focus on segmenting text into _tokens_ with a _tokenizer_.\\n\\nYou may have heard of tokenizers before.\\nIf you took a computer science class you likely learned about how programming language compilers work.\\nA tokenizer that is used to compile computer languages is called a _scanner_ or _lexer_.\\nIn some cases your computer language parser can work directly on the computer code and doesn\\'t need a tokenizer at all.\\nAnd for natural language processing, the only parser typically outputs a vector representation, rather than  If the tokenizer functionality is not separated from the compiler, the parser is often called a scannerless _parser_.\\n\\nThe set of valid tokens for a particular computer language is called the _vocabulary_ for that language, or more formally its _lexicon_.\\nLinguistics and NLP researchers use the term \"lexicon\" to refer to a set of natural language tokens.\\nThe term \"vocabulary\" is the more natural way to refer to a set of natural language words or tokens.\\nSo that\\'s what you will use here.\\n\\nThe natural language equivalent of a computer language compiler is a natural language parser.\\nA natural language tokenizer is called a _scanner_, or _lexer_, or _lexical analyzer_ in the computer language world.\\nModern computer language compilers combine the _lexer_ and _parser_ into a single lexer-parser algorithm.\\nThe vocabulary of a computer language is usually called a _lexicon_.\\nAnd computer language compilers sometimes refer to tokens as _symbols_.\\n\\nHere are five important NLP terms.\\nAlong side them are some roughly equivalent terms used in computer science when talking about programming language compilers:\\n\\n* _tokenizer_ -- scanner, lexer, lexical analyzer\\n* _vocabulary_ -- lexicon\\n* _parser_ -- compiler\\n* _token_, _term_, _word_, or _n-gram_ -- token or symbol\\n* _statement_ -- statement or expression\\n\\nTokenization is the first step in an NLP pipeline, so it can have a big impact on the rest of your pipeline.\\nA tokenizer breaks unstructured data, natural language text, into chunks of information which can be counted as discrete elements.\\nThese counts of token occurrences in a document can be used directly as a vector representing that document.\\nThis immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning.\\nThese counts can be used directly by a computer to trigger useful actions and responses.\\nOr they might also be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\\nThe most common use for bag-of-words vectors created this way is for document retrieval, or search.\\n\\n== Your tokenizer toolbox\\n\\nSo each application you encounter you will want to think about which kind of tokenizer is appropriate for your application.\\nAnd once you decide which kinds of tokens you want to try, you\\'ll need to configure a python package for accomplishing that goal.\\n\\nYou can chose from several tokenizer implementations: footnote:[Lysandre explains the various tokenizer options in the Huggingface documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\\n\\n. Python: `str.split`, `re.split`\\n. NLTK: `PennTreebankTokenizer`, `TweetTokenizer`\\n. spaCy: state of the art tokenization is its reason for being\\n. Stanford CoreNLP: linguistically accurate, requires Java interpreter\\n. Huggingface: `BertTokenizer`, a `WordPiece` tokenizer\\n\\n=== The simplest tokenizer\\n\\nThe simplest way to tokenize a sentence is to use whitespace within a string as the \"delimiter\" of words. In Python, this can be accomplished with the standard library method `split`, which is available on all `str` object instances as well as on the `str` built-in class itself.\\n\\nLet\\'s say your NLP pipeline needs to parse quotes from WikiQuote.org, and it\\'s having trouble with one titled _The Book Thief_.footnote:[Markus Zusak, _The Book Thief_, p. 85 (https://en.wikiquote.org/wiki/The_Book_Thief)]\\n\\n\\n[[book_thief_sentence_split_py]]\\n.Example quote from _The Book Thief_ split into tokens\\n[source,python]\\n----\\n>>> text = (\"Trust me, though, the words were on their way, and when \"\\n...         \"they arrived, Liesel would hold them in her hands like \"\\n...         \"the clouds, and she would wring them out, like the rain.\")\\n>>> tokens = text.split()\\n>>> tokens[:8]\\n[\\'Trust\\', \\'me,\\', \\'though,\\', \\'the\\', \\'words\\', \\'were\\', \\'on\\', \\'their\\']\\n----\\n\\n\\n.Tokenized phrase\\nimage::../images/ch02/book-thief-split.png[alt=\"Trust|me,|though,|the|words|were|on|their\",align=\"center\",width=100%,link=\"../images/ch02/book-thief-split.png\"]\\n\\nAs you can see, this built-in Python method does an OK job of tokenizing this sentence.\\nIts only \"mistake\" is to include commas within the tokens.\\nThis would prevent your keyword detector from detecting quite a few important tokens: `[\\'me\\', \\'though\\', \\'way\\', \\'arrived\\', \\'clouds\\', \\'out\\', \"rain\"]`.\\nThose words \"clouds\" and \"rain\" are pretty important to the meaning of this text.\\nSo you\\'ll need to do a bit better with your tokenizer to ensure you can catch all the important words and \"hold\" them like Liesel.\\n\\n=== Rule-based tokenization\\n\\nIt turns out there is a simple fix to the challenge of splitting punctuation from words.\\nYou can use a regular expression tokenizer to create rules to deal with common punctuation patterns.\\nHere\\'s just one particular regular expression you could use to deal with punctuation \"hanger-ons.\"\\nAnd while we\\'re at it, this regular expression will be smart about words that have internal punctuation, such as possessive words and contractions that contain apostrophes.\\n\\nYou\\'ll use a regular expression to tokenize some text from the book _Blindsight_ by Peter Watts.\\nThe text describes how the most _adequate_ humans tend to survive natural selection (and alien invasions).footnote:[Peter Watts, Blindsight, (https://rifters.com/real/Blindsight.htm)]\\nThe same goes for your tokenizer.\\nYou want to find an _adequate_ tokenizer that solves your problem, not the perfect tokenizer.\\nYou probably can\\'t even guess what the _right_ or _fittest_ token is.\\nYou will need an accuracy number to evaluate your NLP pipeline with and that will tell you which tokenizer should survive your selection process.\\nThe example here should help you start to develop your intuition about applications for regular expression tokenizers.\\n\\n[source,python]\\n----\\n>>> import re\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'  # <1>\\n>>> texts = [text]\\n>>> texts.append(\"There\\'s no such thing as survival of the fittest. \"\\n...              \"Survival of the most adequate, maybe.\")\\n>>> tokens = list(re.findall(pattern, texts[-1]))\\n>>> tokens[:8]\\n[\"There\\'s\", \\'no\\', \\'such\\', \\'thing\\', \\'as\\', \\'survival\\', \\'of\\', \\'the\\']\\n>>> tokens[8:16]\\n[\\'fittest\\', \\'.\\', \\'Survival\\', \\'of\\', \\'the\\', \\'most\\', \\'adequate\\', \\',\\']\\n>>> tokens[16:]\\n[\\'maybe\\', \\'.\\']\\n----\\n<1> The _look-ahead_ pattern `(?:\\\\\\'\\\\w+)?` detects whether or not the word contains a single apostrophe followed by 1 or more letters.footnote:[Thank you Wiktor Stribiżew (https://stackoverflow.com/a/43094210/623735).]\\n\\nMuch better.\\nNow the tokenizer separates punctuation from the end of a word, but doesn\\'t break up words that contain internal punctuation such as the apostrophe within the token \"There\\'s.\"\\nSo all of these words were tokenized the way we wanted: \"There\\'s\", \"fittest\", \"maybe\".\\nAnd this regular expression tokenizer will work fine on contractions even if they have more than one letter after the apostrophe such as \"can\\'t\", \"she\\'ll\", \"what\\'ve\".\\nIt will work even typos such as \\'can\"t\\' and \"she,ll\", and \"what`ve\".\\nBut this liberal matching of internal punctuation probably isn\\'t what you want if your text contains rare double contractions such as \"couldn\\'t\\'ve\", \"ya\\'ll\\'ll\", and \"y\\'ain\\'t\"\\n\\n[TIP]\\n=====\\nPro tip: You can accommodate double-contractions with the regular expression `r\\'\\\\w+(?:\\\\\\'\\\\w+){0,2}|[^\\\\w\\\\s]\\'`\\n=====\\n\\nThis is the main idea to keep in mind.\\nNo matter how carefully you craft your tokenizer, it will likely destroy some amount of information in your raw text.\\nAs you are cutting up text, you just want to make sure the information you leave on the cutting room floor isn\\'t necessary for your pipeline to do a good job.\\nAlso, it helps to think about your downstream NLP algorithms.\\nLater you may configure a case folding, stemming, lemmatizing, synonym substitution, or count vectorizing algorithm.\\nWhen you do, you\\'ll have to think about what your tokenizer is doing, so your whole pipeline works together to accomplish your desired output.\\n\\n\\n////\\n// too much regex detail?\\n\\n==== How regular expressions work\\n\\nHere is how the regular expression in <<listing_2_7>> works.\\n\\nThe square brackets (`[` and `]`) are used to indicate a _character class_, a set of characters.\\nThe plus sign after the closing square bracket (`]`) means that a match must contain one or more of the characters inside the square brackets.\\nThe `\\\\s` within the character class is a shortcut to a predefined character class that includes all whitespace characters like those created when you press the `[space]`, `[tab]`, and `[return]` keys.\\nThe character class `r\\'[\\\\s]\\'` is equivalent to `r\\'[ \\\\t\\\\r\\\\n\\\\f]\\'`.\\nThe six whitespace characters are space (`\\' \\'`), tab (`\\'\\\\t\\'`), return (`\\'\\\\r\\'`), newline  (`\\'\\\\n\\'`), and form-feed (`\\'\\\\f\\'`).\\n\\nYou did not use any character ranges here, but you may want to later.\\nA character range is a special kind of character class indicated within square brackets and a hyphen like `r\\'[a-z]\\'` to match all lowercase letters.\\nThe character range `r\\'[0-9]\\'` matches any digit 0 through 9 and is equivalent to `r\\'[0123456789]\\'`).\\nThe regular expression `r\\'[\\\\_a-zA-Z]\\'` would match any underscore character (`r\\'\\\\_\\'`) or letter of the English alphabet (upper or lower case).\\n\\nThe hyphen (`-`) right after the opening square bracket is a bit of quirk of regexes.\\nYou cannot put a hyphen just anywhere inside your square brackets because the regex parser may think you mean a character range like `r\\'[0-9]\\'`.\\nSo whenever you want to indicate an actual hyphen (dash) character in your character class, you need to make sure it is the first character after the open square bracket, or you need to escape it with a backslash (`\\\\`).\\n\\nThe `re.split` function goes through each character in the input string (the second argument, `sentence`) left to right looking for any matches based on the \"program\" or \"pattern\" in the regular expression (the first argument, `r\\'[-\\\\s.,;!?]+\\'`).\\nWhen it finds a match, it breaks the string right before that matched character and right after it, skipping over the matched character or characters.\\nSo the `re.split` line will work just like `str.split`, but it will work for any kind of character or multicharacter sequence that matches your regular expression.\\n\\nThe parentheses (`(` and `)`) are used to group regular expressions just like they are used to group mathematical, Python, and most other programming language expressions.\\nThese parentheses force the regular expression to match the entire expression within the parentheses before moving on to try to match the characters that follow the parentheses.\\n\\n// TODO: TMI?\\n////\\n\\nTake a look at the first few tokens in your lexographically sorted vocabulary for this short text:\\n\\n[source,python]\\n----\\n>>> import numpy as np  # <1>\\n>>> vocab = sorted(set(tokens))  # <2>\\n>>> \\' \\'.join(vocab[:12])  # <3>\\n\", . Survival There\\'s adequate as fittest maybe most no of such\"\\n>>> num_tokens = len(tokens)\\n>>> num_tokens\\n18\\n>>> vocab_size = len(vocab)\\n>>> vocab_size\\n15\\n----\\n<1> `str.split()` is your quick-and-dirty tokenizer.\\n<2> Coercing the `list` into a `set` ensures that your vocabulary contains only *unique* tokens that you want to keep track of.\\n<3> Sorted lexographically (lexically) so punctuation comes before letters, and capital letters come before lowercase letters.\\n\\nYou can see how you may want to consider lowercasing all your tokens so that \"Survival\" is recognized as the same word as \"survival\".\\nAnd you may want to have a synonym substitution algorithm to replace \"There\\'s\" with \"There is\" for similar reasons.\\nHowever, this would only work if your tokenizer kept contraction and possessive apostrophes attached to their parent token.\\n\\n[TIP]\\n=====\\nMake sure you take a look at your vocabulary whenever it seems your pipeline isn\\'t working well for a particular text.\\nYou may need to revise your tokenizer to make sure it can \"see\" all the tokens it needs to do well for your NLP task.\\n=====\\n\\n\\n=== SpaCy\\n\\nMaybe you don\\'t want your regular expression tokenizer to keep contractions together.\\nPerhaps you\\'d like to recognize the word \"isn\\'t\" as two separate words, \"is\" and \"n\\'t\".\\nThat way you could consolidate the synonyms \"n\\'t\" and \"not\" into a single token.\\nThis way your NLP pipeline would understand \"the ice cream isn\\'t bad\" to mean the same thing as \"the ice cream is not bad\".\\nFor some applications, such as full text search, intent recognition, and sentiment analysis, you want to be able to *uncontract* or expand contractions like this.\\nBy splitting contractions, you can use synonym substitution or contraction expansion to improve the recall of your search engine and the accuracy of your sentiment analysis.\\n\\n[IMPORTANT]\\n====\\nWe\\'ll discuss case folding, stemming, lemmatization, and synonym substitution later in this chapter.\\nBe careful about using these techniques for applications such as authorship attribution, style transfer, or text fingerprinting.\\nYou want your authorship attribution or style-transfer pipeline to stay true to the author\\'s writing style and the exact spelling of words that they use.\\n====\\n\\nSpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.\\nIn fact the name \"spaCy\" is based on the word \"space\", as in the separator used in Western languages to separate words.\\nAnd spaCy adds a lot of additional _tags_ to tokens at the same time that it is applying rules to split tokens apart.\\nSo spaCy is often the first and last tokenizer you\\'ll ever need to use.\\n\\nLet\\'s see how spaCy handles our collection of deep thinker quotes:\\n\\n[source,python]\\n----\\n>>> import spacy  # <1>\\n>>> nlp = spacy.load(\\'en_core_web_sm\\')  # <2>\\n>>> doc = nlp(texts[-1])\\n>>> type(doc)\\n<class \\'spacy.tokens.doc.Doc\\'>\\n\\n>>> tokens = [tok.text for tok in doc]\\n>>> tokens[:9]\\n[\\'There\\', \"\\'s\", \\'no\\', \\'such\\', \\'thing\\', \\'as\\', \\'survival\\', \\'of\\', \\'the\\']\\n\\n>>> tokens[9:17]\\n[\\'fittest\\', \\'.\\', \\'Survival\\', \\'of\\', \\'the\\', \\'most\\', \\'adequate\\', \\',\\']\\n----\\n<1> If this is your first time to use spacy you should download the small language model with `spacy.cli.download(\\'en_core_web_sm\\')`\\n<2> `sm` stands for \"small\" (17 MB), `md` is medium (45 MB), `lg` is \"large\" (780 MB)\\n\\nThat tokenization may be more useful to you if you\\'re comparing your results to academic papers or colleagues at work.\\nSpacy is doing a lot more under the hood.\\nThat small language model you downloaded is also identifying sentence breaks with some *sentence boundary detection* rules.\\nA language model is a collection of regular expressions and finite state automata (rules).\\nThese rules are a lot like the grammar and spelling rules you learned in English class.\\nThey are used in the algorithms that tokenize and label your words with useful things like their part of speech and their position in a syntax tree of relationships between words.\\n\\n[source,python]\\n----\\n>>> from spacy import displacy\\n>>> sentence = list(doc.sents)[0] # <1>\\n>>> displacy.serve(sentence, style=\"dep\")\\n>>> !firefox 127.0.0.1:5000\\n\\n----\\n<1> The first sentence begins with \"There\\'s no such thing...\"\\n\\nIf you browse to your `localhost` on port 5000 you should see a sentence diagram that may be even more correct than what you could produce in school:\\n\\nimage::../images/ch02/there-such-thing.png[alt=\"NOUN Survival -> ADV maybe. ADJ adequate -> ADV most\",align=\"center\",width=100%,link=\"../images/ch02/there-such-thing.png\"]\\n\\nYou can see that spaCy does a lot more than simply separate text into tokens.\\nIt identifies sentence boundaries to automatically segment your text into sentences.\\nAnd it tags tokens with various attributes like their part of speech (PoS) and even their role within the syntax of a sentence.\\nYou can see the lemmas displayed by `displacy`  beneath the literal text for each token.footnote:[nlpia2 source code for chapter 2 (https://proai.org/nlpia2-ch2) has additional spaCy and displacy options and examples.]\\nLater in the chapter we\\'ll explain how lemmatization and case folding and other vocabulary *compression* approaches can be helpful for some applications.\\n\\nSo spaCy seems pretty great in terms of accuracy and some \"batteries included\" features, such as all those token tags for lemmas and dependencies.\\nWhat about speed?\\n\\n=== Tokenizer race\\n\\nSpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.\\nFirst download the AsciiDoc text file for this chapter:\\n\\n[source,python]\\n----\\n>>> import requests\\n>>> text = requests.get(\\'https://proai.org/nlpia2-ch2.adoc\\').text\\n>>> f\\'{round(len(text) / 10_000)}0k\\'\\n\\'160k\\'\\n----\\n<1> I divided by 10,000 and rounded it, so that Doctests would continue to pass as I revise this text.\\n\\nThere were about 160 thousand ASCII characters in this AsciiDoc file where I wrote this sentence that you are reading right now.\\nWhat does that mean in terms of words-per-second, the standard benchmark for tokenizer speed?\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\\'en_core_web_sm\\')\\n>>> %timeit nlp(text)  # <1>\\n4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\\n\\n>>> f\\'{round(len(text) / 10_000)}0k\\'\\n\\'160k\\'\\n>>> doc = nlp(text)\\n>>> f\\'{round(len(list(doc)) / 10_000)}0k\\'\\n\\'30k\\'\\n>>> f\\'{round(len(doc) / 1_000 / 4.67)}kWPS\\'  # <2> \\n\\'7kWPS\\'\\n----\\n<1> `%timeit` is a magic function within `jupyter notebook`, `jupyter console` or `ipython`\\n<2> kWPS is for thousands of words (tokens) per second\\n\\nThat\\'s nearly 5 seconds for about 150,000 characters or 34,000 words of English and Python text or about 7000 words per second.\\n\\nThat may seem fast enough for you on your personal projects.\\nBut on a medical records summarization project we needed to process thousands of large documents with a comparable amount of text as you find in this entire book.\\nAnd the latency in our medical record summarization pipeline was a critical metric for the project.\\nSo this, full-featured spaCy pipeline would require at least 5 days to process 10,000 books such as NLPIA or typical medical records for 10,000 patients.\\n\\nIf that\\'s not fast enough for your application you can disable any of the tagging features of the spaCy pipeline that you do not need.\\n\\n[source,python]\\n----\\n>>> nlp.pipe_names  # <1>\\n[\\'tok2vec\\', \\'tagger\\', \\'parser\\', \\'attribute_ruler\\', \\'lemmatizer\\', \\'ner\\']\\n>>> nlp = spacy.load(\\'en_core_web_sm\\', disable=nlp.pipe_names)\\n>>> %timeit nlp(text)\\n199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\\n----\\n<1> The `pipe_names` lists all the currently enabled elements of your spaCy `nlp` pipeline\\n\\nYou can disable the pipeline elements you don\\'t need to speed up the tokenizer:\\n\\n- `tok2vec`: word embeddings\\n- `tagger`: part-of-speech (`.pos` and `.pos_`)\\n- `parser`: syntax tree role\\n- `attribute_ruler`: fine-grained POS and other tags\\n- `lemmatizer`: lemma tagger\\n- `ner`: named entity recognition tagger\\n\\nNLTK\\'s `word_tokenize` method is often used as the pace setter in tokenizer benchmark speed comparisons:\\n\\n[source,python]\\n----\\n>>> import nltk\\n>>> nltk.download(\\'punkt\\')\\nTrue\\n>>> from nltk.tokenize import word_tokenize\\n>>> %timeit word_tokenize(text)\\n156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\\n>>> tokens = word_tokenize(text)\\n>>> f\\'{round(len(tokens) / 10_000)}0k\\'\\n\\'30k\\'\\n----\\n\\nCould it be that you found a winner for the tokenizer race?\\nNot so fast.\\nYour regular expression tokenizer has some pretty simple rules, so it should run pretty fast as well:\\n\\n[source,python]\\n----\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'\\n>>> tokens = re.findall(pattern, text)  # <1>\\n>>> f\\'{round(len(tokens) / 10_000)}0k\\'\\n\\'30k\\'\\n>>> %timeit re.findall(pattern, text)\\n8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\\n----\\n<1> Try precompiling with `re.compile` to learn something about how smart the core Python developers are\\n\\nNow that\\'s not surprising.\\nRegular expressions can be compiled and run very efficiently within low level C routines in Python.\\n\\n[TIP]\\n================\\nUse a regular expression tokenizer when speed is more import than accuracy.\\nIf you do not need the additional linguistic tags that spaCy and other pipelines provide your tokenizer doesn\\'t need to waste time trying to figure out those tags.footnote:[Andrew Long, \"Benchmarking Python NLP Tokenizers\" (https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5)]\\nAnd each time you use a regular expression in the `re` or `regex` packages, a compiled and optimized version of it is cached in RAM.\\nSo there\\'s usually no need to _precompile_ (using `re.compile()`) your regexes.\\n================\\n\\n== Wordpiece tokenizers\\n\\nIt probably felt natural to think of words as indivisible atomic chunks of meaning and thought.\\nHowever, you did find some words that didn\\'t clearly split on spaces or punctuation.\\nAnd many compound words or named entities that you\\'d like to keep together have spaces within them.\\nSo it can help to dig a little deeper and think about the statistics of what makes a word.\\nThink about how we can build up words from neighboring characters instead of cleaving text at separators such as spaces and punctuation.\\n\\n=== Clumping characters into sentence pieces\\n\\nInstead of thinking about breaking strings up into tokens, your tokenizer can look for characters that are used a lot right next to each other, such as \"i\" before \"e\".\\nYou can pair up characters and sequences of characters that belong together.footnote:[In many applications the term \"_n_-gram\" refers to character _n_-grams rather than word n-grams. For example the leading relational database PostgreSQL has a Trigram index which tokenizes your text into character 3-grams not word 3-grams. In this book, we use \"_n_-gram\" to refer to sequences of word grams and \"character _n_-grams\" when talking about sequences of characters.]\\nThese clumps of characters can become your tokens.\\nAn NLP pipeline only pays attention to the statistics of tokens.\\nAnd hopefully these statistics will line up with our expectations for what a word is.\\n\\nMany of these character sequences will be whole words, or even compound words, but many will be pieces of words.\\nIn fact, all _subword tokenizers_ maintain a token within the vocabulary for every individual character in your vocabulary.\\nThis means it never needs to use an OOV (Out-of-Vocabulary) token, as long as any new text doesn\\'t contain any new characters it hasn\\'t seen before.\\nSubword tokenizers attempt to optimally clump characters together to create tokens.\\nUsing the statistics of character n-gram counts it\\'s possible for these algorithms to identify wordpieces and even sentence pieces that make good tokens.\\n\\nIt may seem odd to identify words by clumping characters.\\nBut to a machine, the only obvious, consistent division between elements of meaning in a text is the boundary between bytes or characters.\\nAnd the frequency with which characters are used together can help the machine identify the meaning associated with subword tokens such as individual syllables or parts of compound words.\\n\\nIn English, even individual letters have subtle emotion (sentiment) and meaning (semantics) associated with them.\\nHowever, there are only 26 unique letters in the English language.\\nThat doesn\\'t leave room for individual letters to _specialize_ on any one topic or emotion.\\nNonetheless savvy marketers know that some letters are cooler than others.\\nBrands will try to portray themselves as technologically advanced by choosing names with exotic letters like \"Q\" and \"X\" or \"Z\".\\nThis also helps with SEO (Search Engine Optimization) because rarer letters are more easily found among the sea of possible company and product names.\\nYour NLP pipeline will pick up all these hints of meaning, connotation, and intent.\\nYour token counters will provide the machine with the statistics it needs to infer the meaning of clumps of letters that are used together often.\\n\\nThe only disadvantage for subword tokenizers is the fact that they must pass through your corpus of text many times before converging on an optimal vocabulary and tokenizer.\\nA subword tokenizer has to be trained or fit to your text just like a CountVectorizer.\\nIn fact you\\'ll use a CountVectorizer in the next section to see how subword tokenizers work.\\n\\nThere are two main approaches to subword tokenization: BPE (Byte-Pair Encoding) and Wordpiece tokenization.\\n\\n==== BPE\\n\\nIn the previous edition of the book we insisted that words were the smallest unit of meaning in English that you need consider.\\nWith the rise of Transformers and other deep learning models that use BPE and similar techniques, we\\'ve changed our minds.footnote:[Hannes and Cole are probably screaming \"We told you so!\" as they read this.]\\nCharacter-based subword tokenizers have proven to be more versatile and robust for most NLP problems.\\nBy building up a vocabulary from building blocks of Unicode multi-byte characters you can construct a vocabulary that can handle every possible natural language string you\\'ll ever see, all with a vocabulary of as few as 50,000 tokens.\\n\\nYou may think that Unicode characters are the smallest packet of meaning in natural language text.\\nTo a human, maybe, but to a machine, no way.\\nJust as the BPE name suggests, characters don\\'t have to be your fundamental atom of meaning for your _base vocabulary_.\\nYou can split characters into 8-bit bytes.\\nGPT-2 uses a byte-level BPE tokenizer to naturally compose all the unicode characters you need from the bytes that make them up.\\nThough some special rules are required to handle unicode punctuation within a byte-based vocabulary, no other adjustment to the character-based BPE algorithm is required.\\nA byte-level BPE tokenizer allows you to represent all possible texts with a base (minimum) vocabulary size of 256 tokens.\\nThe GPT-2 model can achieve state-of-the-art performance with it\\'s default BPE vocabulary of only 50,000 multibyte _merge tokens_ plus 256 individual byte tokens.\\n\\nYou can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker or the hub in a social network.\\nIt connects characters together that appear next to each other a lot.\\nIt then creates a new token for these character combinations.\\nAnd it keeps doing this until it has a many frequently used character sequences as you\\'ve allowed in your vocabulary size limit.\\n\\n\\nBPE is transforming the way we think about natural language tokens.\\nNLP engineers are finally letting the data do the talking.\\nStatistical thinking is better than human intuition when building an NLP pipeline.\\nA machine can see how _most_ people use language.\\nYou are only familiar with what _you_ mean when you use particular words or syllables.\\nTransformers have now surpassed human readers and writers at some natural language understanding and generation tasks, including finding meaning in subword tokens.\\n\\nOne complication you have not yet encounter is the dilemma of what to do when you encounter a new word.\\nIn the previous examples, we just keep adding new words to our vocabulary.\\nBut in the real world your pipeline will have been trained on an initial corpus of documents that may or may not represent all the kinds of tokens it will ever see.\\nIf your initial corpus is missing some of the words that you encounter later on, you will not have a slot in your vocabulary to put your counts of that new word.\\nSo when you train you initial pipeline, you will always reserve a slot (dimension) to hold the counts of your _out-of-vocabulary_ (OOV) tokens.\\nSo if your original set of documents did not contain the girl\\'s name \"Aphra\", all counts of the name Aphra would be lumped into the OOV dimension as counts of Amandine and other rare words.\\n\\nTo give Aphra equal representation in your vector space, you can use BPE.\\nBPE breaks down rare words into smaller pieces to create a _periodic table_ of the elements for natural language in your corpus.\\nSo, because \"aphr\" is a common english prefix, your BPE tokenizer would probably give Aphra *two* slots for her counts in your vocabulary: one for \"aphr\" and one for \"a\".\\nActually, you might actually discover that the vobcabulary slots are for \" aphr\" and \"a \", because BPE keeps track of spaces no differently than any other character in your alphabet.footnote:[Actually, the string representation of tokens used for BPE and Wordpiece tokenizer place marker characters at the beginning or end of the token string indicate the absence of a word boundary (typically a space or punctuation). So you may see the \"aphr##\" token in your BPE vocabulary for the prefix \"aphr\" in aphrodesiac (https://stackoverflow.com/a/55416944/623735)]\\n\\nBPE gives you multilingual flexibility to deal with Hebrew names like Aphra.\\nAnd it give your pipeline robustness against common misspellings and typos, such as \"aphradesiac.\"\\nEvery word, including minority 2-grams such as \"African American\", have representation in the voting system of BPE.footnote:[Discriminatory voting restriction laws have recently been passed in US: (https://proai.org/apnews-wisconsin-restricts-blacks)]\\nGone are the days of using the kluge of OOV (Out-of-Vocabulary) tokens to handle the rare quirks of human communication.\\nBecause of this, state of the art deep learning NLP pipelines such as transformers all use word piece tokenization similar to BPE.footnote:[See chapter 12 for information about another similar tokenizer -- sentence piece tokenizer]\\n\\nBPE preserves some of the meaning of new words by using character tokens and word-piece tokens to spell out any unknown words or parts of words.\\nFor example, if \"syzygy\" is not in our vocabulary, we could represent it as the six tokens \"s\", \"y\", \"z\", \"y\", \"g\", and \"y\".\\nPerhaps \"smartz\" could be represented as the two tokens \"smart\" and \"z\".\\n\\nThat sounds smart.\\nLet\\'s see how it works on our text corpus:\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n>>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer=\\'char\\')\\n>>> vectorizer.fit(texts)\\nCountVectorizer(analyzer=\\'char\\', ngram_range=(1, 2))\\n\\n>>> bpevocab = vectorizer.get_feature_names()\\n>>> bpevocab[:7]\\n[\\' \\', \\' a\\', \\' c\\', \\' f\\', \\' h\\', \\' i\\', \\' l\\']\\n----\\n\\nWe configured the `CountVectorizer` to split the text into all the possible character 1-grams and 2-grams found in the texts.\\nAnd `CountVectorizer` organizes the vocabulary in lexical order, so n-grams that start with a space character (`\\' \\'`) come first.\\nOnce the vectorizer knows what tokens it needs to be able to count, it can transform text strings into vectors, with one dimension for every token in your character n-gram vocabulary.\\n\\n[source,python]\\n----\\n>>> vectors = vectorizer.transform(texts)\\n>>> df = pd.DataFrame(vectors.todense(), columns=bpevocab)\\n>>> df.index = [t[:8] + \\'...\\' for t in texts]\\n>>> df = df.T\\n>>> df[\\'total\\'] = df.T.sum()\\n>>> df\\n    Trust me...  There\\'s ...  total\\n             31           14     45\\n a            3            2      5\\n c            1            0      1\\n f            0            1      1\\n h            3            0      3\\n..          ...          ...    ...\\nwr            1            0      1\\ny             2            1      3\\ny             1            0      1\\ny,            1            0      1\\nyb            0            1      1\\n<BLANKLINE>\\n[148 rows x 3 columns]\\n----\\n\\nThe DataFrame contains a column for each sentence and a row for each character 2-gram.\\nCheck out the top four rows where the byte pair (character 2-gram) of \" a\" is seen to occur five times in these two sentences.\\nSo even spaces count as \"characters\" when you\\'re building a BPE tokenizer.\\nThis is one of the advantages of BPE, it will figure out what your token delimiters are, so it will work even in languages where there is no whitespace between words.\\nAnd BPE will work on substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13 characters forward.\\n\\n[source,python]\\n----\\n>>> df.sort_values(\\'total\\').tail()\\n        Trust me...  There\\'s ...  total\\n    he           10            3     13\\n    h            14            5     19\\n    t            11            9     20\\n    e            18            8     26\\n                 31           14     45\\n----\\n\\nA BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent vocabulary.\\nOver time it deletes the less frequent character pairs as it gets less and less likely that they won\\'t come up a lot more later in your text.\\n\\n----\\n>>> df[\\'n\\'] = [len(tok) for tok in bpevocab]\\n>>> df[df[\\'n\\'] > 1].sort_values(\\'total\\').tail()\\n    Trust me...  There\\'s ...  total  n\\n,             6            1      7  2\\ne             7            2      9  2\\n t            8            3     11  2\\nth            8            4     12  2\\nhe           10            3     13  2\\n----\\n\\nSo the next round of preprocessing in the BPE tokenizer would retain the character 2-grams \"he\" and \"th\" and even \" t\" and \"e \".\\nThen the BPE algorithm would make another pass through the text with this smaller character bigram vocabulary.\\nIt would look for frequent pairings of these character bigrams with each other and individual characters.\\nThis process would continue until the maximum number of tokens is reached and the longest possible character sequences have been incorporated into the vocabulary.\\n\\n[NOTE]\\n====\\nYou may see mention of _wordpiece_ tokenizers which are used within some advanced language models such as `BERT` and its derivatives.footnote:[Lysandre Debut explains all the variations on subword tokenizers in the Hugging Face transformers documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\\nIt works the same as BPE, but it actually uses the underlying language model to predict the neighboring characters in string.\\nIt eliminates the characters from its vocabulary that hurt the accuracy of this language model the least.\\nThe math is subtly different and it produces subtly different token vocabularies, but you don\\'t need to select this tokenizer intentionally.\\nThe models that use it will come with it built into their pipelines.\\n====\\n\\nOne big challenge of BPE-based tokenizers is that they must be trained on your individual corpus.\\nSo BPE tokenizers are usually only used for Transformers and Large Language Models (LLMs) which you will learn about in chapter 9.\\n\\nAnother challenge of BPE tokenizers is all the book keeping you need to do to keep track of which trained tokenizer goes with each of your trained models.\\nThis was one of the big innovations of Huggingface.\\nThey made it easy to store and share all the preprocessing data, such as the tokenizer vocabulary, along side the language model.\\nThis makes it easier to reuse and share BPE tokenizers. \\nIf you want to become an NLP expert, you may want to imitate what they\\'ve done at HuggingFace with your own NLP preprocessing pipelines.footnote:[Huggingface documentation on tokenizers (https://huggingface.co/docs/transformers/tokenizer_summary)]\\n\\n== Vectors of tokens\\n\\nNow that you have broken your text into tokens of meaning, what do you do with them?\\nHow can you convert them to numbers that will be meaningful to the machine?\\nThe simplest most basic thing to do would be to detect whether a particular token you are interested in was present or not.\\nYou could hard-code the logic to check for important tokens, called a _keywords_.\\n\\nThis might work well for your greeting intent recognizer in chapter 1.\\nOur greeting intent recognizer at the end of chapter 1 looked for words like \"Hi\" and \"Hello\" at the beginning of a text string.\\nYour new tokenized text would help you detect the presence or absence of words such as \"Hi\" and \"Hello\" without getting confused by words like \"Hiking\" and \"Hell.\"\\nWith your new tokenizer in place, your NLP pipeline wouldn\\'t misinterpret the word \"Hiking\" as the greeting \"Hi king\":\\n\\n[source,python]\\n----\\n>>> hi_text = \\'Hiking home now\\'\\n>>> hi_text.startswith(\\'Hi\\')\\nTrue\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'  # <1>\\n>>> \\'Hi\\' in re.findall(pattern, hi_text)  # <2>\\nFalse\\n>>> \\'Hi\\' == re.findall(pattern, hi_text)[0]  # <3>\\nFalse\\n----\\n<1> You can reuse the regular expression pattern from earlier to create a one-line tokenizer\\n<2> \\'Hi\\' is not among the 3 words (tokens) in this phrase\\n<3> \\'Hi\\' is definitely not the first word in this phrase\\n\\nSo tokenization can help you reduce the number of false positives in your simple intent recognition pipeline that looks for the presence of greeting words.\\nThis is often called keyword detection, because your vocabulary of words is limited to a set of words you think are important.\\nHowever, it\\'s quite cumbersome to have to think of all the words that might appear in a greeting in order to recognize them all, including slang, misspellngs and typoos.\\nAnd creating a for loop to iterate through them all would be inefficient.\\nWe can use the math of linear algebra and the vectorized operations of `numpy` to speed this process up.\\n\\nIn order to detect tokens efficiently you will want to use three new tricks:\\n\\n. matrix and vector representations of documents\\n. vectorized operations in numpy\\n. indexing of discrete vectors\\n\\nYou\\'ll first learn the most basic, direct, raw and lossless way to represent words as a matrix, one-hot encoding.\\n\\n=== One-hot Vectors\\n\\nNow that you\\'ve successfully split your document into the kinds of words you want, you\\'re ready to create vectors out of them.\\nVectors of numbers are what we need to do the math or processing of NL*P* on natural language text.\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> onehot_vectors = np.zeros(\\n...     (len(tokens), vocab_size), int)  # <2>\\n>>> for i, word in enumerate(tokens):\\n...     onehot_vectors[i, vocab.index(word)] = 1  # <3>\\n>>> df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\\n>>> df_onehot.shape\\n(18, 15)\\n>>> df_onehot.iloc[:,:8].replace(0, \\'\\')  # <4>\\n    ,  .  Survival  There\\'s  adequate  as  fittest  maybe\\n0                       1\\n1\\n2\\n3\\n4                                   1\\n5\\n6\\n7\\n8                                           1\\n9      1\\n10              1\\n11\\n12\\n13\\n14                               1\\n15  1\\n16                                                1\\n17     1\\n----\\n<2> The table is as wide as your count of unique vocabulary terms and as tall as the length of your document: 18 rows, 15 columns\\n<3> For each token in the sentence, mark the column for it with a `1`.\\n<4> For brevity we\\'re only showing the first 8 columns of the DataFrame and replaced 0\\'s with \\'\\'.\\n\\nIn this representation of this two-sentence quote, each row is a vector representation of a single word from the text.\\nThe table has the 15 columns because this is the number of unique words in your vocabulary.\\nThe table has 18 rows, one for each word in the document.\\nA \"1\" in a column indicates a vocabulary word that was present at that position in the document.\\n\\nYou can \"read\" a one-hot encoded (vectorized) text from top to bottom.\\nYou can tell that the first word in the text was the word \"There\\'s\", because the `1` on the first row is positioned under the column label \"There\\'s\".\\nThe next three rows (row indexes 1, 2, and 3) are blank, because we\\'ve truncated the table on the right to help it fit on the page.\\nThe fifth row of the text, with the 0-offset index number of `4` shows us that the fifth word in the text was the word \"adequate\", because there\\'s a `1` in that column.\\n\\nOne-hot vectors are super-sparse, containing only one nonzero value in each row vector.\\nFor display, this code replaces the `0`\\'s with empty strings (`\\'\\'`), to make it easier to read.\\nBut the code did not actually alter the `DataFrame` of data you are processing in your NLP pipeline.\\nThe Python code above was just to to make it easier to read, so you can see that it looks a bit like a player piano paper roll, or maybe a music box drum.\\n\\nThe Pandas `DataFrame` made this output a little easier to read and interpret.\\nThe `DataFrame.columns` keep track of labels for each column.\\nThis allows you to label each column in your table with a string, such as the token or word it represents.\\nA `DataFrame` can also keep track of labels for each row in an the `DataFrame.index`, for speedy lookup.\\n\\n[IMPORTANT]\\n====\\nDon\\'t add strings to any `DataFrame` you intend to use in your machine learning pipeline.\\nThe purpose of a tokenizer and vectorizer, like this one-hot vectorizer, is to create a numerical array that your NLP pipeline can do math on.\\nYou can\\'t do math on strings.\\n====\\n\\nEach row of the table is a binary row vector, and you can see why it\\'s also called a one-hot vector: all but one of the positions (columns) in a row are `0` or blank.\\nOnly one column, or position in the vector is \"hot\" (\"1\").\\nA one (`1`) means on, or hot.\\nA zero (`0`) mean off, or absent.\\n\\nOne nice feature of this vector representation of words and tabular representation of documents is that no information is lost.\\nThe exact sequence of tokens is encoded in the order of the one-hot vectors in the table representing a document.\\nAs long as you keep track of which words are indicated by which column, you can reconstruct the original sequence of tokens from this table of one-hot vectors perfectly.\\nAnd this reconstruction process is 100% accurate even though your tokenizer was only 90% accurate at generating the tokens you thought would be useful.\\nAs a result, one-hot word vectors like this are typically used in neural nets, sequence-to-sequence language models, and generative language models.\\nThey are a good choice for any model or NLP pipeline that needs to retain all the meaning inherent in the original text.\\n\\n[TIP]\\n====\\nThe one-hot encoder (vectorizer) did not discard any information from the text, but our tokenizer did.\\nOur regular expression tokenizer discarded the whitespace characters (`\\\\s`) that sometimes occur between words.\\nSo you could not perfectly reconstruct the original text with a _detokenizer_.\\nTokenizers like spaCy, however, keep track of these whitespace characters and can in fact detokenize a sequence of tokens perfectly.\\nSpaCy was named for this feature of accurately accounting for white-*space* efficiently and accurately.\\n====\\n\\nThis sequence of one-hot vectors is like a digital recording of the original text.\\nIf you squint hard enough you might be able to imagine that the matrix of ones and zeros above is a player piano roll.footnote:[See the \"Player piano\" article on Wikipedia (https://en.wikipedia.org/wiki/Player_piano).].\\nOr maybe it\\'s the bumps on the metal drum of a music box.footnote:[See the web page titled \"Music box - Wikipedia\" (https://en.wikipedia.org/wiki/Music_box).]\\nThe vocabulary key at the top tells the machine which \"note\" or word to play for each row in the sequence of words or piano music.\\n\\n[[player_piano_roll_jpg]]\\n.Player piano roll\\nimage::../images/ch02/piano_roll.jpg[Player piano music roll with parallel sequences of holes running vertically down the paper. The holes meander left and right to indicate the rising and falling of the tones in the melody of a song. Image licensed from Wikimedia CC BY-SA 3.0 (https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg),width=100%,align=\"center\",link=\"https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg\"]\\n\\nUnlike a player-piano or a music box, your mechanical word recorder and player is only allowed to use one \"finger\" at a time.\\nIt can only play one \"note\" or word at a time.\\nIt\\'s one-hot.\\nAnd there is no variation in the spacing of the words.\\n\\nThe important thing is that you\\'ve turned a sentence of natural language words into a sequence of numbers, or vectors.\\nNow you can have the computer read and do math on the vectors just like any other vector or list of numbers.\\nThis allows your vectors to be input into any natural language processing pipeline that requires this kind of vector.\\nThe Deep Learning pipelines of chapter 5 through 10 typically require this representation, because they can be designed to extract \"features\" of meaning from these raw representations of text.\\nAnd Deep Learning pipelines can generate text from numerical representations of meaning.\\nSo the stream of words emanating from your NLG pipelines in later chapters will often be represented by streams of one-hot encoded vectors, just like a player piano might play a song for a less artificial audience in West World.footnote:[West World is a television series about particularly malevolent humans and human-like robots, including one that plays a piano in the main bar.]\\n\\nNow all you need to do is figure out how to build a \"player piano\" that can _understand_ and combine those word vectors in new ways.\\nUltimately, you\\'d like your chatbot or NLP pipeline to play us a song, or say something, you haven\\'t heard before.\\nYou\\'ll get to do that in chapters 9 and 10 when you learn about recurrent neural networks that are effective for sequences of one-hot encoded tokens like this.\\n\\nThis representation of a sentence in one-hot word vectors retains all the detail, grammar, and order of the original sentence.\\nAnd you have successfully turned words into numbers that a computer can \"understand.\"\\nThey are also a particular kind of number that computers like a lot: binary numbers.\\nBut this is a big table for a short sentence.\\nIf you think about it, you have expanded the file size that would be required to store your document.\\nFor a long document this might not be practical.\\n\\nHow big is this *lossless* numerical representation of your collection of documents?\\nYour vocabulary size (the length of the vectors) would get huge.\\nThe English language contains at least 20,000 common words, millions if you include names and other proper nouns.\\nAnd your one-hot vector representation requires a new table (matrix) for every document you want to process.\\nThis is almost like a raw \"image\" of your document.\\nIf you have done any image processing, you know that you need to do dimension reduction if you want to extract useful information from the data.\\n\\nLet\\'s run through the math to give you an appreciation for just how big and unwieldy these \"piano rolls\" are.\\nIn most cases, the vocabulary of tokens you\\'ll use in an NLP pipeline will be much more than 10,000 or 20,000 tokens.\\nSometimes it can be hundreds of thousands or even millions of tokens.\\nLet\\'s assume you have a million tokens in your NLP pipeline vocabulary.\\nAnd let\\'s say you have a meager 3000 books with 3500 sentences each and 15 words per sentence -- reasonable averages for short books.\\nThat\\'s a whole lot of big tables (matrices), one for each book.\\nThat would use 157.5 terabytes.\\nYou probably couldn\\'t even store that on disk.\\n\\nThat is more than a million million bytes, even if you are super-efficient and use only one byte for each number in your matrix.\\nAt one byte per cell, you would need nearly 20 terabytes of storage for a small bookshelf of books processed this way.\\nFortunately you do not ever use this data structure for storing documents.\\nYou only use it temporarily, in RAM, while you are processing documents one word at a time.\\n\\nSo storing all those zeros, and recording the order of the words in all your documents does not make much sense.\\nIt is not practical.\\nAnd it\\'s not very useful.\\nYour data structure hasn\\'t abstracted or generalized from the natural language text.\\nAn NLP pipeline like this doesn\\'t yet do any real feature extraction or dimension reduction to help your machine learning work well in the real world.\\n\\nWhat you really want to do is compress the meaning of a document down to its essence.\\nYou would like to compress your document down to a single vector rather than a big table.\\nAnd you are willing to give up perfect \"recall.\"\\nYou just want to capture most of the meaning (information) in a document, not all of it.\\n\\n=== BOW (Bag-of-Words) Vectors\\n\\nIs there any way to squeeze all those _player piano music rolls_ into a single vector?\\nVectors are a great way to represent any object.\\nWith vectors we could compare documents to each other just be checking the Euclidian distance between them.\\nVectors allow us to use all your linear algebra tools on natural language.\\nAnd that\\'s really the goal of NLP, doing math on text.\\n\\nLet us assume you can ignore the order of the words in our texts.\\nFor this first cut at a vector representation of text you can just jumble them all up together into a \"bag,\" one bag for each sentence or short document.\\nIt turns out just knowing what words are present in a document can give your NLU pipeline a lot of information about what\\'s in it.\\nThis is in fact the representation that power big Internet search engine companies.\\nEven for documents several pages long, a bag-of-words vector is useful for summarizing the essence of a document.\\n\\nLet\\'s see what happens when we jumble and count the words in our text from _The Book Thief_:\\n\\n[source,python]\\n----\\n>>> bow = sorted(set(re.findall(pattern, text)))\\n>>> bow[:9]\\n[\\',\\', \\'.\\', \\'Liesel\\', \\'Trust\\', \\'and\\', \\'arrived\\', \\'clouds\\', \\'hands\\', \\'her\\']\\n>>> bow[9:19]\\n[\\'hold\\', \\'in\\', \\'like\\', \\'me\\', \\'on\\', \\'out\\', \\'rain\\', \\'she\\', \\'the\\', \\'their\\']\\n>>> bow[19:27]\\n[\\'them\\', \\'they\\', \\'though\\', \\'way\\', \\'were\\', \\'when\\', \\'words\\', \\'would\\']\\n----\\n\\nEven with this jumbled up bag of words, you can get a general sense that this sentence is about:  \"Trust\", \"words\", \"clouds\", \"rain\", and someone named \"Liesel\".\\nOne thing you might notice is that Python\\'s `sorted()` puts punctuation before characters, and capitalized words before lowercase words.\\nThis is the ordering of characters in the ASCII and Unicode character sets.\\nHowever, the order of your vocabulary is unimportant.\\nAs long as you are consistent across all the documents you tokenize this way, a machine learning pipeline will work equally well with any vocabulary order.\\n\\nYou can use this new bag-of-words vector approach to compress the information content for each document into a data structure that is easier to work with.\\nFor keyword search, you could **OR** your one-hot word vectors from the player piano roll representation into a binary bag-of-words vector.\\nIn the play piano analogy this is like playing several notes of a melody all at once, to create a \"chord\".\\nRather than \"replaying\" them one at a time in your NLU pipeline, you would create a single bag-of-words vector for each document.\\n\\nYou could use this single vector to represent the whole document in a single vector.\\nBecause vectors all need to be the same length, your BOW vector would need to be as long your vocabulary size which is the number of unique tokens in your documents.\\nAnd you could ignore a lot of words that would not be interesting as search terms or keywords.\\nThis is why stop words are often ignored when doing BOW tokenization.\\nThis is an extremely efficient representation for a search engine index or the first filter for an information retrieval system.\\nSearch indexes only need to know the presence or absence of each word in each document to help you find those documents later.\\n\\nThis approach turns out to be critical to helping a machine \"understand\" a collection of words as a single mathematical object.\\nAnd if you limit your tokens to the 10,000 most important words, you can compress your numerical representation of your imaginary 3500 sentence book down to 10 kilobytes, or about 30 megabytes for your imaginary 3000-book corpus.\\nOne-hot vector sequences for such a modest-sized corpus would require hundreds of gigabytes.\\n\\nAnother advantage of the BOW representation of text is that it allows you to find similar documents in your corpus in constant time (`O(1)`).\\nYou can\\'t get any faster than this.\\nBOW vectors are the precursor to a reverse index which is what makes this speed possible.\\nIn computer science and software engineering, you are always on the lookout for data structures that enable this kind of speed.\\nAll major full text search tools use BOW vectors to find what you\\'re looking for fast.\\nYou can see this numerical representation of natural language in EllasticSearch, Solr,footnote:[Apache Solr home page and Java source code (https://solr.apache.org/)] PostgreSQL, and even state of the art web search engines such as Qwant,footnote:[Qwant web search engine based in Europe (https://www.qwant.com/)], SearX,footnote:[SearX git repository (https://github.com/searx/searx) and web search (https://searx.thegpm.org/)], and Wolfram Alpha footnote:[(https://www.wolframalpha.com/)].\\n\\nFortunately, the words in your vocabulary are sparsely utilized in any given text.\\nAnd for most bag-of-words applications, we keep the documents short, sometimes just a sentence will do.\\nSo rather than hitting all the notes on a piano at once, your bag-of-words vector is more like a broad and pleasant piano chord, a combination of notes (words) that work well together and contain meaning.\\nYour NLG pipeline or chatbot can handle these chords even if there is a lot of \"dissonance\" from words in the same statement that are not normally used together.\\nEven dissonance (odd word usage) is useful information about a statement that a machine learning pipeline can make use of.\\n\\nHere is how you can put the tokens into a binary vector indicating the presence or absence of a particular word in a particular sentence.\\nThis vector representation of a set of sentences could be \"indexed\" to indicate which words were used in which document.\\nThis index is equivalent to the index you find at the end of many textbooks, except that instead of keeping track of which page a word occurs on, you can keep track of the sentence (or the associated vector) where it occurred.\\nWhereas a textbook index generally only cares about important words relevant to the subject of the book, you keep track of every single word (at least for now).\\n\\n==== Sparse representations\\n\\nYou might be thinking that if you process a huge corpus you\\'ll probably end up with thousands or even millions of unique tokens in your vocabulary.\\nThis would mean you would have to store a lot of zeros in your vector representation of our 20-token sentence about Liesel.\\nA `dict` would use much less memory than a vector.\\nAny paired mapping of words to their 0/1 values would be more efficient than a vector.\\nBut you can\\'t do math on `dict`\\'s.\\nSo this is why CountVectorizer uses a sparse numpy array to hold the counts of words in a word fequency vector.\\nUsing a dictionary or sparse array for your vector ensures that it only has to store a 1 when any one of the millions of possible words in your dictionary appear in a particular document.\\n\\nBut if you want to look at an individual vector to make sure everything is working correctly, a Pandas `Series` is the way to go.\\nAnd you will wrap that up in a Pandas DataFrame so you can add more sentences to your binary vector \"corpus\" of quotes.\\n\\n=== Dot product\\n\\n// TODO: some of this may belong in the discussion of keyword matching and one-hot vectors?\\nYou\\'ll use the dot product a lot in NLP, so make sure you understand what it is.\\nSkip this section if you can already do dot products in your head.\\n\\nThe dot product is also called the _inner product_ because the \"inner\" dimension of the two vectors (the number of elements in each vector) or matrices (the rows of the first matrix and the columns of the second matrix) must be the same because that is where the products happen.\\nThis is analogous to an \"inner join\" on two relational database tables.\\n\\nThe dot product is also called the _scalar product_ because it produces a single scalar value as its output.\\nThis helps distinguish it from the _cross product_, which produces a vector as its output.\\nObviously, these names reflect the shape of the symbols used to indicate the dot product (latexmath:[\\\\cdot]) and cross product (latexmath:[\\\\times]) in formal mathematical notation.\\nThe scalar value output by the scalar product can be calculated by multiplying all the elements of one vector by all the elements of a second vector and then adding up those normal multiplication products.\\n\\nHere is a Python snippet you can run in your Pythonic head to make sure you understand what a dot product is:\\n\\n[[example_dot_product_code]]\\n.Example dot product calculation\\n[source,python]\\n----\\n>>> v1 = pd.np.array([1, 2, 3])\\n>>> v2 = pd.np.array([2, 3, 4])\\n>>> v1.dot(v2)\\n20\\n>>> (v1 * v2).sum()  # <1>\\n20\\n>>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # <2>\\n20\\n----\\n<1> Multiplication of NumPy arrays is a \"vectorized\" operation that is very efficient.\\n<2> You should not iterate through vectors this way unless you want to slow down your pipeline.\\n\\n[TIP]\\n================\\nThe dot product is equivalent to the _matrix product_, which can be accomplished in NumPy with the `np.matmul()` function or the `@` operator. Since all vectors can be turned into Nx1 or 1xN matrices, you can use this shorthand operator on two column vectors (Nx1) by transposing the first one so their inner dimensions line up, like this: `v1.reshape((-1, 1)).T @ v2.reshape((-1, 1))`, which outputs your scalar product within a 1x1 matrix: `array([[20]])`\\n================\\n\\n// IDEA: Consider talking about BOW overlap to explain cosine similarity\\n\\nThis is your first vector space model of natural language documents (sentences).\\nNot only are dot products possible, but other vector operations are defined for these bag-of-word vectors: addition, subtraction, OR, AND, and so on.\\nYou can even compute things such as Euclidean distance or the angle between these vectors. This representation of a document as a binary vector has a lot of power.\\nIt was a mainstay for document retrieval and search for many years.\\nAll modern CPUs have hardwired memory addressing instructions that can efficiently hash, index, and search a large set of binary vectors like this.\\nThough these instructions were built for another purpose (indexing memory locations to retrieve data from RAM), they are equally efficient at binary vector operations for search and retrieval of text.\\n\\n\\n\\nNLTK and Stanford CoreNLP have been around the longest and are the most widely used for comparison of NLP algorithms in academic papers.\\nEven though the Stanford CoreNLP has a Python API, it relies on the Java 8 CoreNLP backend, which must be installed and configured separately.\\nSo if you want to publish the results of your work in an academic paper and compare it to what other researchers are doing, you may need to use NLTK.\\nThe most common tokenizer used in academia is the PennTreebank tokenizer:\\n\\n[source,python]\\n----\\n>>> from nltk.tokenize import TreebankWordTokenizer\\n>>> texts.append(\\n...   \"If conscience and empathy were impediments to the advancement of \"\\n...   \"self-interest, then we would have evolved to be amoral sociopaths.\"\\n...   )  # <1>\\n>>> tokenizer = TreebankWordTokenizer()\\n>>> tokens = tokenizer.tokenize(texts[-1])[:6]\\n>>> tokens[:8]\\n[\\'If\\', \\'conscience\\', \\'and\\', \\'empathy\\', \\'were\\', \\'impediments\\', \\'to\\', \\'the\\']\\n>>> tokens[8:16]\\n[\\'advancement\\', \\'of\\', \\'self-interest\\', \\',\\', \\'then\\', \\'we\\', \\'would\\', \\'have\\']\\n>>> tokens[16:]\\n[\\'evolved\\', \\'to\\', \\'be\\', \\'amoral\\', \\'sociopaths\\', \\'.\\']\\n----\\n<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield in _SuperCooperators_: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\\n\\n// IDEA: Diagram of Nowak quote with vertical bars breaking up sent into words\\n\\nThe spaCy Python library contains a natural language processing pipeline that includes a tokenizer.\\nIn fact, the name of the package comes from the words \"space\" and \"Cython\".\\nSpaCy was built using the Cython package to speed the tokenization of text, often using the *space* character (\" \") as the delimmiter.\\nSpaCy has become the *multitool* of NLP, because of its versatility and the elegance of its API.\\nTo use spaCy, you can start by creating an callable parser object, typically named `nlp`.\\nYou can customize your NLP pipeline by modifying the Pipeline elements within that parser object.\\n\\nAnd spaCy has \"batteries included.\"\\nSo even with the default smallest spaCy language model loaded, you can do tokenization and sentence segementation, plus *part-of-speech* and *abstract-syntax-tree* tagging -- all with a single function call.\\nWhen you call `nlp()` on a string, spaCy tokenizes the text and returns a `Doc` (document) object.\\nA `Doc` object is a container for the sequence of sentences and tokens that it found in the text.\\n\\n\\n// IDEA: example spacy code for tokenization\\n\\nThe spaCy package tags each token with their linguistic function to provide you with information about the text\\'s grammatical structure.\\nEach token object within a `Doc` object has attributes that provide these tags.\\n\\nFor example:\\n* `token.text` the original text of the word\\n* `token.pos_` grammatical part of speech tag as a human-readable string\\n* `token.pos`  integer for the grammar part of speech tag\\n* `token.dep_` indicates the tokens role in the syntactic dependency tree\\n* `token.dep`  integer corresponding to the syntactic dependency tree location\\n\\nThe `.text` attribute provides the original text for the token.\\nThis is what is provided when you request the __str__ representation of a token.\\nA spaCy `Doc` object is allowing you to detokenize a document object to recreate the entire input text. i.e., the relation between tokens\\nYou can use these functions to examine the text in more depth.\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\"en_core_web_sm\")\\n>>> text = \"Nice guys finish first.\"  # <1>\\n>>> doc = nlp(text)\\n>>> for token in doc:\\n>>>     print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\\nNice            ADJ       amod\\nguys            NOUN      nsubj\\nfinish          VERB      ROOT\\nfirst           ADV       advmod\\n.               PUNCT     punct\\n----\\n<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\\n\\n== Challenging tokens\\n\\nChinese, Japanese, and other pictograph languages aren\\'t limited to a small small number letters in alphabets used to compose tokens or words.\\nCharacters in these traditional languages look more like drawings and are called \"pictographs.\"\\nThere are many thousands of unique characters in the Chinese and Japanese languages.\\nAnd these characters are used much like we use words in alphabet-based languages such as English.\\nBut each Chinese character is usually not a complete word on its own.\\nA character\\'s meaning depends on the characters to either side.\\nAnd words are not delimited with spaces.\\nThis makes it challenging to tokenize Chinese text into words or other packets of thought and meaning. \\n\\nThe `jieba` package is a Python package you can use to segment traditional Chinese text into words.\\nIt supports three segmentation modes: 1) \"full mode\" for retrieving all possible words from a sentence, 2) \"accurate mode\" for cutting the sentence into the most accurate segments, 3) \"search engine mode\" for splitting long words into shorter ones, sort-of like splitting compound words or finding the roots of words in English.\\nIn the example below, the Chinese sentence \"西安是一座举世闻名的文化古城\" translates into \"Xi\\'an is a city famous world-wide for it\\'s ancient culture.\"\\nOr, a more compact and literal translation might be \"Xi\\'an is a world-famous city for her ancient culture.\"\\n\\nFrom a grammatical perspective, you can split the sentence into: 西安 (Xi\\'an), 是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective suffix), 文化 (culture), 古城 (ancient city).\\nThe character \"座\" is the quantifier meaning \"ancient\" that is normally used to modify the word \"city.\"\\nThe `accurate mode` in `jieba` causes it to segment the sentence this way so that you can correctly extract a precise interpretation of the text.\\n\\n.Jieba in accurate mode\\n[source,python]\\n----\\n>>> seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\") # <1>\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> the default mode for jieba is accurate or precise mode\\n\\nJieba\\'s accurate mode minimizes the total number of tokens or words.\\nThis gave you 7 tokens for this short\\nJieba attempts to keep as many possible characters together.\\nThis will reduce the false positive rate or type 1 errors for detecting boundaries between words.\\n\\nIn full mode, jieba will attempt to split the text into smaller words, and more of them.\\n\\n.Jieba in full mode\\n[source,python]\\n----\\n>>> import jieba\\n... seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # <1>\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世\\', \\'举世闻名\\', \\'闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> `cut_all==True` means \"full mode\"\\n\\nNow you can try search engine mode to see if it\\'s possible to break up these tokens even further:\\n\\n.Jieba in search engine mode\\n[source,python]\\n----\\n>>> seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世\\', \\'闻名\\', \\'举世闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> Accurate mode is the default mode.\\n\\nUnfortunately later versions of Python (3.5+) aren\\'t supported by Jieba\\'s part-of-speech tagging model.\\n\\n[source,python]\\n----\\n>>> import jieba\\n>>> from jieba import posseg\\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\")\\n>>> jieba.enable_paddle()  # <1>\\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\\n>>> list(words)\\n[pair(\\'西安\\', \\'ns\\'),\\n pair(\\'是\\', \\'v\\'),\\n pair(\\'一座\\', \\'m\\'),\\n pair(\\'举世闻名\\', \\'i\\'),\\n pair(\\'的\\', \\'uj\\'),\\n pair(\\'文化\\', \\'n\\'),\\n pair(\\'古城\\', \\'ns\\')]\\n----\\n<1> Activate paddle mode\\n\\nYou can find more information about jieba at (https://github.com/fxsjy/jieba).\\nSpaCy also contains Chinese language models that do a decent job of segmenting and tagging Chinese text.\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> spacy.cli.download(\"zh_core_web_sm\")  # <1>\\n>>> nlpzh = spacy.load(\"zh_core_web_sm\")\\n>>> doc = nlpzh(\"西安是一座举世闻名的文化古城\")\\n>>> [(tok.text, tok.pos_) for tok in doc]\\n[(\\'西安\\', \\'PROPN\\'),\\n (\\'是\\', \\'VERB\\'),\\n (\\'一\\', \\'NUM\\'),\\n (\\'座\\', \\'NUM\\'),\\n (\\'举世闻名\\', \\'VERB\\'),\\n (\\'的\\', \\'PART\\'),\\n (\\'文化\\', \\'NOUN\\'),\\n (\\'古城\\', \\'NOUN\\')]\\n----\\n<1> Only need download the Chinese (zh) language model if this is your first time processing Chinese text \\n\\nAs you may notice, spaCy provides slightly different tokenization and tagging, which is more attached to the original meaning of each word rather than the context of this sentence.\\n\\n=== A complicated picture\\n\\nUnlike English, there is no concept of stemming or lemmatization in pictographic languages such as Chinese and Japanese (Kanji).\\nHowever, there’s a related concept.\\nThe most essential building blocks of Chinese characters are called _radicals_.\\nTo better understand _radicals_, you must first see how Chinese characters are constructed.\\nThere are six types of Chinese characters: 1) pictographs, 2) pictophonetic characters, 3) associative compounds, 4) self-explanatory characters, 5) phonetic loan characters, and 6) mutually explanatory characters.\\nThe top four categories are the most important and encompass most Chinese characters.\\n\\n1. Pictographs (象形字)\\n2. Pictophonetic characters (形声字)\\n3. Associative compounds (会意字)\\n\\n==== 1. Pictographs (象形字)\\n\\n_Pictographs_ were created from images of real objects, such as the characters for 口 (mouth) and 门 (door).\\n\\n\\n==== 2. Pictophonetic characters (形声字)\\n\\n_Pictophonetic characters_ were created from a radical and a single Chinese character.\\nOne part represents its meaning and the other indicates its pronunciation.\\nFor example, 妈 (mā, mother) = 女 (female) + 马 (mǎ, horse).\\nSqueezing 女 into 马 gives 妈.\\nThe character 女 is the semantic radical that indicates the meaning of the character (female).\\n马 is a single character that has a similar pronunciation (mǎ).\\nYou can see that the character for mother (妈) is a combination of the characters for female an\\nThis is comparable to the English concept of homophones -- words that sound alike but mean completely different things.\\nBut in Chinese use additional characters to disambiguate homophones.\\nThe character for female\\n\\n==== 3. Associative compounds (会意字)\\n\\nAssociative compounds can be divided into two parts: one symbolizes the image, the other indicates the meaning.\\n\\nFor example, 旦 (dawn), the upper part (日) is the sun and the lower part (一) is like the horizon line.\\n\\n\\n==== Self-explanatory characters (指事字)\\n\\n\\nSelf-explanatory characters cannot be easily represented by an image, so they are shown by a single abstract symbol.\\nFor example, 上 (up), 下 (down).\\n\\nAs you can see, procedures like stemming and lemmatization are harder or impossible for many Chinese characters.\\nSeparating the parts of a character may radically ;) change its meaning.\\nAnd there\\'s not prescribed order or rule for combining radicals to create Chinese characters.\\n\\nNonetheless, some kinds of stemming are harder in English than they are in Chinese\\nFor example, automatically removing the pluralization from words like \"we\", \"us\", \"they\" and \"them\" is hard in English but straightforward in Chinese.\\nChinese uses inflection to construct the plural form of characters, similar to adding s to the end of English words.\\nIn Chinese the pluralization suffix character is 们.\\nThe character 朋友 (friend) becomes 朋友们 (friends).\\n\\nEven the characters for \"we/us\", \"they/them\", and \"y\\'all\" use the same pluralization suffix: 我们 (we/us), 他们 (they/them), 你们 (you).\\nBut in in English, you can remove the \\'ing\\' or \\'ed\\' from many verbs to get the root word.\\nHowever, in Chinese, verb conjugation uses an additional character in the front or the end to indicate tense.\\nThere\\'s no prescribed rule for verb conjugation.\\nFor example, examine the character 学 (learn), 在学 (learning), and 学过 (learned).\\nIn Chinese, you can also use a suffix 学 to denote an academic discipline, such as 心理学 (psychology) or 社会学 (sociology).\\nIn most cases, you want to keep the integrated Chinese character together rather than reducing it to its components.\\n\\nIt turns out this is a good rule of thumb for all languages.\\nLet the data do the talking.\\nDo not stem or lemmatize unless the statistics indicate that it will help your NLP pipeline perform better.\\nIs there not a small amount of meaning that is lost when \"smarter\" and \"smartest\" reduce to \"smart\"?\\nMake sure stemming does not leave your NLP pipeline dumb.\\n\\nLet the statistics of how of how characters and words are used together help you decide how, or if, to decompose any particular word or n-gram.\\nIn the next chapter we\\'ll show you some tools like Scikit-Learn\\'s `TfidfVectorizer` that handle all the tedious account required to get this right.\\n\\n\\n==== Contractions\\n\\n// TODO: clean this up\\nYou might be wondering why you would want to split the contraction `wasn\\'t` into `was` and `n\\'t`.\\nFor some applications, like grammar-based NLP models that use syntax trees, it is important to separate the words `was` and `not` to allow the syntax tree parser to have a consistent, predictable set of tokens with known grammar rules as its input.\\nThere are a variety of standard and nonstandard ways to contract words, by reducing contractions to their constituent words, a dependency tree parser or syntax parser only need to be programmed to anticipate the various spellings of individual words rather than all possible contractions.\\n\\n\\n[TIP]\\n.Tokenize informal text from social networks such as Twitter and Facebook\\n====\\nThe NLTK library includes a rule-based tokenizer to deal with short, informal, emoji-laced texts from social networks: `casual_tokenize`\\n\\nIt handles emojis, emoticons, and usernames.\\nThe `reduce_len` option deletes less meaningful character repetitions.\\nThe `reduce_len` algorithm retains three repetitions, to approximate the intent and sentiment of the original text.\\n\\n[source,python]\\n----\\n>>> from nltk.tokenize.casual import casual_tokenize\\n>>> texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\\n>>> casual_tokenize(texts[-1], reduce_len=True)\\n[\\'@rickrau\\', \\'mind\\', \\'BLOOOWWWN\\', \\'by\\', \\'latest\\', \\'lex\\', \\':*)\\', \\'!\\', \\'!\\', \\'!\\']\\n----\\n\\n====\\n\\n=== Extending your vocabulary with _n_-grams\\n\\nLet\\'s revisit that \"ice cream\" problem from the beginning of the chapter.\\nRemember we talked about trying to keep \"ice\" and \"cream\" together.\\n\\n____\\nI scream, you scream, we all scream for ice cream.\\n____\\n\\nBut I do not know many people that scream for \"cream\".\\nAnd nobody screams for \"ice\", unless they\\'re about to slip and fall on it.\\nSo you need a way for your word-vectors to keep \"ice\" and \"cream\" together.\\n\\n==== We all gram for _n_-grams\\n\\nAn _n_-gram is a sequence containing up to _n_ elements that have been extracted from a sequence of those elements, usually a string.\\nIn general the \"elements\" of an _n_-gram can be characters, syllables, words, or even symbols like \"A\", \"D\", and \"G\" used to represent the chemical amino acid markers in a DNA or RNA sequence.footnote:[Linguistic and NLP techniques are often used to glean information from DNA and RNA, this site provides a list of amino acid symbols that can help you translate amino acid language into a human-readable language: \"Amino Acid - Wikipedia\" (https://en.wikipedia.org/wiki/Amino_acid#Table_of_standard_amino_acid_abbreviations_and_properties).]\\n\\nIn this book, we\\'re only interested in _n_-grams of words, not characters.footnote:[You may have learned about trigram indexes in your database class or the documentation for PostgreSQL (`postgres`). But these are triplets of characters. They help you quickly retrieve fuzzy matches for strings in a massive database of strings using the `%` and `~*` SQL full text search queries.]\\nSo in this book, when we say 2-gram, we mean a pair of words, like \"ice cream\".\\nWhen we say 3-gram, we mean a triplet of words like \"beyond the pale\" or \"Johann Sebastian Bach\" or \"riddle me this\".\\n_n_-grams do not have to mean something special together, like compound words.\\nThey have to be frequent enough together to catch the attention of your token counters.\\n\\nWhy bother with _n_-grams?\\nAs you saw earlier, when a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of those words.\\nBy extending your concept of a token to include multiword tokens, _n_-grams, your NLP pipeline can retain much of the meaning inherent in the order of words in your statements.\\nFor example, the meaning-inverting word \"not\" will remain attached to its neighboring words, where it belongs.\\nWithout _n_-gram tokenization, it would be free floating.\\nIts meaning would be associated with the entire sentence or document rather than its neighboring words.\\nThe 2-gram \"was not\" retains much more of the meaning of the individual words \"not\" and \"was\" than those 1-grams alone in a bag-of-words vector.\\nA bit of the context of a word is retained when you tie it to its neighbor(s) in your pipeline.\\n\\nIn the next chapter, we show you how to recognize which of these _n_-grams contain the most information relative to the others, which you can use to reduce the number of tokens (_n_-grams) your NLP pipeline has to keep track of.\\nOtherwise it would have to store and maintain a list of every single word sequence it came across.\\nThis prioritization of _n_-grams will help it recognize \"Three Body Problem\" and \"ice cream\", without paying particular attention to \"three bodies\" or \"ice shattered\".\\nIn chapter 4, we associate word pairs, and even longer sequences, with their actual meaning, independent of the meaning of their individual words.\\nBut for now, you need your tokenizer to generate these sequences, these _n_-grams.\\n\\n==== Stop words\\n\\nStop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase.\\nExamples of some common stop words include footnote:[A more comprehensive list of stop words for various languages can be found in NLTK\\'s corpora (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip).]\\n\\n* a, an\\n* the, this\\n* and, or\\n* of, on\\n\\nHistorically stop words have been excluded from NLP pipelines in order to reduce the computational effort to extract information from a text.\\nEven though the words themselves carry little information, the stop words can provide important relational information as part of an _n_-gram.\\nConsider these two examples:\\n\\n* `Mark reported to the CEO`\\n* `Suzanne reported as the CEO to the board`\\n\\nIn your NLP pipeline, you might create 4-grams such as `reported to the CEO` and `reported as the CEO`.\\nIf you remove the stop words from the 4-grams, both examples would be reduced to `reported CEO`, and you would lack the information about the professional hierarchy.\\nIn the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board.\\nUnfortunately, retaining the stop words within your pipeline creates another problem: It increases the length of the _n_-grams required to make use of these connections formed by the otherwise meaningless stop words.\\nThis issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\\n\\nDesigning a filter for stop words depends on your particular application.\\nVocabulary size will drive the computational complexity and memory requirements of all subsequent steps in the NLP pipeline.\\nBut stop words are only a small portion of your total vocabulary size.\\nA typical stop word list has only 100 or so frequent and unimportant words listed in it.\\nBut a vocabulary size of 20,000 words would be required to keep track of 95% of the words seen in a large corpus of tweets, blog posts, and news articles.footnote:[See the web page titled \"Analysis of text data and Natural Language Processing\" (http://rstudio-pubs-static.s3.amazonaws.com/41251_4c55dff8747c4850a7fb26fb9a969c8f.html).]\\nAnd that is just for 1-grams or single-word tokens.\\nA 2-gram vocabulary designed to catch 95% of the 2-grams in a large English corpus will generally have more than 1 million unique 2-gram tokens in it.\\n\\nYou may be worried that vocabulary size drives the required size of any training set you must acquire to avoid overfitting to any particular word or combination of words.\\nAnd you know that the size of your training set drives the amount of processing required to process it all.\\nHowever, getting rid of 100 stop words out of 20,000 is not going to significantly speed up your work.\\nAnd for a 2-gram vocabulary, the savings you would achieve by removing stop words is minuscule.\\nIn addition, for 2-grams you lose a lot more information when you get rid of stop words arbitrarily, without checking for the frequency of the 2-grams that use those stop words in your text.\\nFor example, you might miss mentions of \"The Shining\" as a unique title and instead treat texts about that violent, disturbing movie the same as you treat documents that mention \"Shining Light\" or \"shoe shining\".\\n\\nSo if you have sufficient memory and processing bandwidth to run all the NLP steps in your pipeline on the larger vocabulary, you probably do not want to worry about ignoring a few unimportant words here and there.\\nAnd if you are worried about overfitting a small training set with a large vocabulary, there are better ways to select your vocabulary or reduce your dimensionality than ignoring stop words.\\nIncluding stop words in your vocabulary allows the document frequency filters (discussed in chapter 3) to more accurately identify and ignore the words and _n_-grams with the least information content within your particular domain.\\n\\nThe SpaCy and NLTK packages include a variety of predefined sets of stop words for various use cases. footnote:[The spaCy package contains a list of stopwords that you can modify using this Stack Overflow answer (https://stackoverflow.com/a/51627002/623735)]\\nYou probably won\\'t need a broad list of stopwords like the one we created for listing <<listing-broad-stop-words>>, but if you do you\\'ll want to check out the SpaCy and NLTK stopwords lists.\\nAnd if you need an even broader set of stopwords you can `SearX`\\nfootnote:[If you want to help others find SearX you can get in the habbit of saying \"SearX\" (pronounced \"see Ricks\") when talking or writing about doing a web search. You can shift the meaning of words in your world to make it a better place!]\\nfootnote:[The NLTK package (https://pypi.org/project/nltk) contains the list of stopwords you\\'ll see in most tutorials]  for SEO companies that maintain lists of stopwords in many languages.\\n\\nIf your NLP pipeline relies on a fine-tuned list of stop words to achieve high accuracy, it can be a significant maintenance headache.\\nHumans and machines (search engines) are constantly changing which words they ignore.\\n footnote:[Damien Doyle maintains a list of search engine stopwords ranked by popularity and categorized by language (https://www.ranks.nl/stopwords)]\\n// HL: to Karen, Matko & Ivan: how can I use references (anchors) to refer to the correct listing number?\\nListing <<listing-broad-stop-words>> uses an exhaustive list from all these lists so you can get a feel for the amount of meaning that can be lost if your hand-crafted list of stop words isn\\'t well crafted and kept up to date.\\n\\n////\\n\\nHL to Ivan and Matko:\\n\\nThese are the things I tried based on the examples in Manning\\'s ADOC documentation:\\n\\n[[listing-broad-stop-words]] -> \"Listing 2.6\" (capital L)\\n\\n[#listing-broad-stop-words, reftext={chapter}.{counter:listing}] -> \"2.\" (without the counter:listing integer in my Browser plugin)\\n\\n////\\n[#listing-broad-stop-words, reftext={chapter}.{counter:listing}]\\n.Broad list of stop words\\n[source,python]\\n----\\n>>> import requests\\n>>> url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\\n...        \"src/nlpia/data/stopword_lists.json\")\\n>>> response = requests.get(url)\\n>>> stop_words = response.json()[\\'exhaustive\\']  # <1>\\n>>> tokens = \\'the words were just as I remembered them\\'.split()  # <2>\\n>>> tokens_without_stopwords = [x for x in tokens if x not in stop_words]\\n>>> print(tokens_without_stopwords)\\n[\\'I\\', \\'remembered\\']\\n----\\n<1> This exhaustive list of stop words was compiled from various public search engine optimization lists as well as NLP toolkits like spaCy and NLTK.\\n\\nYou can see that some words carry more meaning than others.\\nThis is a sentence from a short story by Ted Chiang about machines helping us remember our statements so we don\\'t have to rely on flawed memories.footnote:[from Ted Chiang, _Exhalation_, \"Truth of Fact, Truth of Fiction\"]\\nIn this phrase you lost two thirds of the words and still retained the bulk of the phrase\\'s meaning.\\nHowever you can see that an import token \"words\" was discarded by this particular stop words list.\\nYou can often get your point across without articles, prepositions, or even forms of the verb \"to be\".\\nImagine someone doing sign language or in a hurry to write a note to themselves.\\nWhich words would they choose to always skip? That is how stop words are chosen.\\n\\nHere\\'s another common stop words list that isn\\'t quite as exhaustive:\\n\\n[[nltk_stop_words_code]]\\n.NLTK list of stop words\\n[source,python]\\n----\\n>>> import nltk\\n>>> nltk.download(\\'stopwords\\')\\n>>> stop_words = nltk.corpus.stopwords.words(\\'english\\')\\n>>> len(stop_words)\\n179\\n>>> stop_words[:7]\\n[\\'i\\', \\'me\\', \\'my\\', \\'myself\\', \\'we\\', \\'our\\', \\'ours\\']\\n>>> [sw for sw in stopwords if len(sw) == 1]\\n[\\'i\\', \\'a\\', \\'s\\', \\'t\\', \\'d\\', \\'m\\', \\'o\\', \\'y\\']\\n----\\n\\nA document that dwells on the first person is pretty boring, and more importantly for you, has low information content.\\nThe NLTK package includes pronouns (not just first person ones) in its list of stop words.\\nAnd these one-letter stop words are even more curious, but they make sense if you have used the NLTK tokenizer and Porter stemmer a lot.\\nThese single-letter tokens pop up a lot when contractions are split and stemmed using NLTK tokenizers and stemmers.\\n\\n[WARNING]\\n====\\nThe set of English stop words in `sklearn`, `spacy`, `nltk`, and SEO tools are very different, and they are constantly evolving.\\nAt the time of this writing, `sklearn` has 318 stop words, NLTK has 179 stop words, spaCy has 326, and our \\'exhaustive\\' SEO list includes 667 stop words.\\n\\nThis is a good reason to consider *not* filtering stop words.\\nIf you do, others may not be able to reproduce your results.\\n====\\n\\nDepending on how much natural language information you want to discard ;), you can take the union or the intersection of multiple stop word lists for your pipeline.\\nHere are some stop_words lists we found, though we rarely use any of them in production:\\n\\n[[collection_of_stop_words_lists_code]]\\n.Collection of stop words lists\\n[source,python]\\n----\\n>>> resp = requests.get(url)\\n>>> len(resp.json()[\\'exhaustive\\'])\\n667\\n>>> len(resp.json()[\\'sklearn\\'])\\n318\\n>>> len(resp.json()[\\'spacy\\'])\\n326\\n>>> len(resp.json()[\\'nltk\\'])\\n179\\n>>> len(resp.json()[\\'reuters\\'])\\n28\\n----\\n\\n=== Normalizing your vocabulary\\n\\nSo you have seen how important vocabulary size is to the performance of an NLP pipeline. Another vocabulary reduction technique is to normalize your vocabulary so that tokens that mean similar things are combined into a single, normalized form. Doing so reduces the number of tokens you need to retain in your vocabulary and also improves the association of meaning across those different \"spellings\" of a token or _n_-gram in your corpus. And as we mentioned before, reducing your vocabulary can reduce the likelihood of overfitting.\\n\\n==== Case folding\\n\\nCase folding is when you consolidate multiple \"spellings\" of a word that differ only in their capitalization.\\nSo why would we use case folding at all?\\nWords can become case \"denormalized\" when they are capitalized because of their presence at the beginning of a sentence, or when they\\'re written in `ALL CAPS` for emphasis.\\n// TODO: Discuss _ normalization, Unicode normalization, and asciification, diacritics, accented e in resume\\', etc\\nUndoing this denormalization is called _case normalization_, or more commonly, _case folding_.\\nNormalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline.\\nIt helps you consolidate words that are intended to mean (and be spelled) the same thing under a single token.\\n\\nHowever, some information is often communicated by capitalization of a word -- for example,  \\'doctor\\' and \\'Doctor\\' often have different meanings.\\nOften capitalization is used to indicate that a word is a proper noun, the name of a person, place, or thing.\\nYou will want to be able to recognize proper nouns as distinct from other words, if named entity recognition is important to your pipeline.\\nHowever, if tokens are not case normalized, your vocabulary will be approximately twice as large, consume twice as much memory and processing time, and might increase the amount of training data you need to have labeled for your machine learning pipeline to converge to an accurate, general solution.\\nJust as in any other machine learning pipeline, your labeled dataset used for training must be \"representative\" of the space of all possible feature vectors your model must deal with, including variations in capitalization.\\nFor 100000-D bag-of-words vectors, you usually must have 100000 labeled examples, and sometimes even more than that, to train a supervised machine learning pipeline without overfitting.\\nIn some situations, cutting your vocabulary size by half can sometimes be worth the loss of information content.\\n\\nIn Python, you can easily normalize the capitalization of your tokens with a list comprehension.\\n\\n[source,python]\\n----\\n>>> tokens = [\\'House\\', \\'Visitor\\', \\'Center\\']\\n>>> normalized_tokens = [x.lower() for x in tokens]\\n>>> print(normalized_tokens)\\n[\\'house\\', \\'visitor\\', \\'center\\']\\n----\\n\\nAnd if you are certain that you want to normalize the case for an entire document, you can `lower()` the text string in one operation, before tokenization.\\nBut this will prevent advanced tokenizers that can split _camel case_ words like \"WordPerfect\", \"FedEx\", or \"stringVariableName.\"footnote:[See the web page titled \"Camel case case - Wikipedia\" (https://en.wikipedia.org/wiki/Camel_case_case).]]\\nMaybe you want WordPerfect to be its own unique thing (token), or maybe you want to reminisce about a more perfect word processing era.\\nIt is up to you to decide when and how to apply case folding.\\n\\nWith case normalization, you are attempting to return these tokens to their \"normal\" state before grammar rules and their position in a sentence affected their capitalization.\\nThe simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Python\\'s built-in `str.lower()`.footnote:[We\\'re assuming the behavior of str.lower() in Python 3. In Python 2, bytes (strings) could be lowercased by just shifting all alpha characters in the ASCII number (`ord`) space, but in Python 3 `str.lower` properly translates characters so it can handle embellished English characters (like the \"acute accent\" diactric mark over the e in resumé) as well as the particulars of capitalization in non-English languages.]\\nUnfortunately this approach will also \"normalize\" away a lot of meaningful capitalization in addition to the less meaningful first-word-in-sentence capitalization you intended to normalize away.\\nA better approach for case normalization is to lowercase only the first word of a sentence and allow all other words to retain their capitalization.\\n\\nLowercasing on the first word in a sentence preserves the meaning of a proper nouns in the middle of a sentence, like \"Joe\" and \"Smith\" in \"Joe Smith\".\\nAnd it properly groups words together that belong together, because they are only capitalized when they are at the beginning of a sentence, since they are not proper nouns.\\nThis prevents \"Joe\" from being confused with \"coffee\" (\"joe\")footnote:[The trigram \"cup of joe\" (https://en.wiktionary.org/wiki/cup_of_joe) is slang for \"cup of coffee.\"] during tokenization.\\nAnd this approach prevents the blacksmith connotation of \"smith\" being confused the the proper name \"Smith\" in a sentence like \"A word smith had a cup of joe.\"\\nEven with this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still need to introduce capitalization errors for the rare proper nouns that start a sentence.\\n\"Joe Smith, the word smith, with a cup of joe.\" will produce a different set of tokens than \"Smith the word with a cup of joe, Joe Smith.\"\\nAnd you may not  want that.\\nIn addition, case normalization is useless for languages that do not have a concept of capitalization, like Arabic or Hindi.\\n\\nTo avoid this potential loss of information, many NLP pipelines do not normalize for case at all.\\nFor many applications, the efficiency gain (in storage and processing) for reducing one\\'s vocabulary size by about half is outweighed by the loss of information for proper nouns.\\nBut some information may be \"lost\" even without case normalization.\\nIf you do not identify the word \"The\" at the start of a sentence as a stop word, that can be a problem for some applications.\\nReally sophisticated pipelines will detect proper nouns before selectively normalizing the case for words at the beginning of sentences that are clearly not proper nouns.\\nYou should implement whatever case normalization approach makes sense for your application.\\nIf you do not have a lot of \"Smith\"s and \"word smiths\" in your corpus, and you do not care if they get assigned to the the same tokens, you can just lowercase everything.\\nThe best way to find out what works is to try several different approaches, and see which approach gives you the best performance for the objectives of your NLP project.\\n\\nBy generalizing your model to work with text that has odd capitalization, case normalization can reduce overfitting for your machine learning pipeline.\\nCase normalization is particularly useful for a search engine.\\nFor search, normalization increases the number of matches found for a particular query.\\nThis is often called the \"recall\" performance metric for a search engine (or any other classification model).footnote:[Check our Appendix D to learn more about _precision_ and _recall_. Here\\'s a comparison of the recall of various search engines on the Webology site (http://www.webology.org/2005/v2n2/a12.html).]\\n\\nFor a search engine without normalization if you searched for \"Age\" you will get a different set of documents than if you searched for \"age.\"\\n\"Age\" would likely occur in phrases like \"New Age\" or \"Age of Reason\".\\nIn contrast, \"age\" would be more likely to occur in phrases like \"at the age of\" in your sentence about Thomas Jefferson.\\nBy normalizing the vocabulary in your search index (as well as the query), you can ensure that both kinds of documents about \"age\" are returned regardless of the capitalization in the query from the user.\\n\\nHowever, this additional recall accuracy comes at the cost of precision, returning many documents that the user may not be interested in. Because of this issue, modern search engines allow users to turn off normalization with each query, typically by quoting those words for which they want only exact matches returned. If you are building such a search engine pipeline, in order to accommodate both types of queries you will have to build two indexes for your documents: one with case-normalized _n_-grams, and another with the original capitalization.\\n\\n==== Stemming\\n\\nAnother common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms.\\nThis normalization, identifying a common stem among various forms of a word, is called stemming.\\nFor example, the words `housing` and `houses` share the same stem, `house`.\\nStemming removes suffixes from words in an attempt to combine words with similar meanings together under their common stem.\\nA stem is not required to be a properly spelled word, but merely a token, or label, representing several possible spellings of a word.\\n\\nA human can easily see that \"house\" and \"houses\" are the singular and plural forms of the same noun.\\nHowever, you need some way to provide this information to the machine. One of its main benefits is in the compression of the number of words whose meaning your software or language model needs to keep track of.\\nIt reduces the size of your vocabulary while limiting the loss of information and meaning, as much as possible.\\nIn machine learning this is referred to as dimension reduction.\\nIt helps generalize your language model, enabling the model to behave identically for all the words included in a stem.\\nSo, as long as your application does not require your machine to distinguish between \"house\" and \"houses\", this stem will reduce your programming or dataset size by half or even more, depending on the aggressiveness of the stemmer you chose.\\n\\nStemming is important for keyword search or information retrieval.\\nIt allows you to search for \"developing houses in Portland\" and get web pages or documents that use both the word \"house\" and \"houses\" and even the word \"housing\" because these words are all stemmed to the \"hous\" token.\\nLikewise you might receive pages with the words \"developer\" and \"development\" rather than \"developing\" because all these words typically reduce to the stem \"develop\".\\nAs you can see, this is a \"broadening\" of your search, ensuring that you are less likely to miss a relevant document or web page.\\nThis broadening of your search results would be a big improvement in the \"recall\" score for how well your search engine is doing its job at returning all the relevant documents.footnote:[Review Appendix D if you have forgotten how to measure recall or visit the wikipedia page to learn more (https://en.wikipedia.org/wiki/Precision_and_recall).]\\n\\nBut stemming could greatly reduce the \"precision\" score for your search engine because it might return many more irrelevant documents along with the relevant ones.\\nIn some applications this \"false-positive rate\" (proportion of the pages returned that you do not find useful) can be a problem.\\nSo most search engines allow you to turn off stemming and even case normalization by putting quotes around a word or phrase.\\nQuoting indicates that you only want pages containing the exact spelling of a phrase such as \"\\'Portland Housing Development software\\'.\"\\nThat would return a different sort of document than one that talks about a \"\\'a Portland software developer\\'s house\\'.\"\\nAnd there are times when you want to search for \"Dr. House\\'s calls\" and not \"dr house call\", which might be the effective query if you used a stemmer on that query.\\n\\nHere\\'s a simple stemmer implementation in pure Python that can handle trailing S\\'s.\\n\\n[source,python]\\n----\\n>>> def stem(phrase):\\n...     return \\' \\'.join([re.findall(\\'^(.*ss|.*?)(s)?$\\',\\n...         word)[0][0].strip(\"\\'\") for word in phrase.lower().split()])\\n>>> stem(\\'houses\\')\\n\\'house\\'\\n>>> stem(\"Doctor House\\'s calls\")\\n\\'doctor house call\\'\\n----\\n\\nThe preceding stemmer function follows a few simple rules within that one short regular expression:\\n\\n* If a word ends with more than one `s`, the stem is the word and the suffix is a blank string.\\n* If a word ends with a single `s`, the stem is the word without the `s` and the suffix is the `s`.\\n* If a word does not end on an `s`, the stem is the word and no suffix is returned.\\n\\nThe strip method ensures that some possessive words can be stemmed along with plurals.\\n\\nThis function works well for regular cases, but is unable to address more complex cases. For example, the rules would fail with words like `dishes` or `heroes`. For more complex cases like these, the NLTK package provides other stemmers.\\n\\nIt also does not handle the \"housing\" example from your \"Portland Housing\" search.\\n\\nTwo of the most popular stemming algorithms are the Porter and Snowball stemmers.\\nThe Porter stemmer is named for the computer scientist Martin Porter.footnote:[See \"An algorithm for suffix stripping\", 1993 (http://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf) by M.F. Porter.]\\nPorter is also also responsible for enhancing the Porter stemmer to create the Snowball stemmer.footnote:[See the web page titled \"Snowball: A language for stemming algorithms\" (http://snowball.tartarus.org/texts/introduction.html).]\\nPorter dedicated much of his lengthy career to documenting and improving stemmers, due to their value in information retrieval (keyword search).\\nThese stemmers implement more complex rules than our simple regular expression.\\nThis enables the stemmer to handle the complexities of English spelling and word ending rules.\\n\\n[source,python]\\n----\\n>>> from nltk.stem.porter import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> \\' \\'.join([stemmer.stem(w).strip(\"\\'\") for w in\\n...   \"dish washer\\'s fairly washed dishes\".split()])\\n\\'dish washer fairli wash dish\\'\\n----\\n\\nNotice that the Porter stemmer, like the regular expression stemmer, retains the trailing apostrophe (unless you explicitly strip it), which ensures that possessive words will be distinguishable from nonpossessive words.\\nPossessive words are often proper nouns, so this feature can be important for applications where you want to treat names differently than other nouns.\\n\\n.More on the Porter stemmer\\n****\\nJulia Menchavez has graciously shared her translation of Porter\\'s original stemmer algorithm into pure python (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py). If you are ever tempted to develop your own stemmer, consider these 300 lines of code and the lifetime of refinement that Porter put into them.\\n\\nThere are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4, 5a, and 5b.\\nStep 1a is a bit like your regular expression for dealing with trailing \"S\"es:footnote:[This is a trivially abbreviated version of Julia Menchavez\\'s implementation `porter-stemmer` on GitHub (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py).]\\n\\n[source,python]\\n----\\ndef step1a(self, word):\\n    if word.endswith(\\'sses\\'):\\n        word = self.replace(word, \\'sses\\', \\'ss\\')  # <1>\\n    elif word.endswith(\\'ies\\'):\\n        word = self.replace(word, \\'ies\\', \\'i\\')\\n    elif word.endswith(\\'ss\\'):\\n        word = self.replace(word, \\'ss\\', \\'ss\\')\\n    elif word.endswith(\\'s\\'):\\n        word = self.replace(word, \\'s\\', \\'\\')\\n    return word\\n----\\n<1> This is not at all like `str.replace()`. Julia\\'s `self.replace()` modifies only the ending of a word.\\n\\nThe remainining seven steps are much more complicated because they have to deal with the complicated English spelling rules for the following:\\n\\n* *Step 1a*: \"s\" and \"es\" endings\\n* *Step 1b*: \"ed\", \"ing\", and \"at\" endings\\n* *Step 1c*: \"y\" endings\\n* *Step 2*: \"nounifying\" endings such as \"ational\", \"tional\", \"ence\", and \"able\"\\n* *Step 3*: adjective endings such as \"icate\",footnote:[Sorry Chick, Porter doesn\\'t like your `obsfucate` username ;)], \"ful\", and \"alize\"\\n* *Step 4*: adjective and noun endings such as \"ive\", \"ible\", \"ent\", and \"ism\"\\n* *Step 5a*: stubborn \"e\" endings, still hanging around\\n* *Step 5b*: trailing double-consonants for which the stem will end in a single \"l\"\\n****\\n\\nSnowball stemmer is more aggressive than the Porter stemmer.\\nNotice that it stems \\'fairly\\' to \\'fair\\', which is more accurate than the Porter stemmer.\\n\\n[source,python]\\n----\\n>>> from nltk.stem.snowball import SnowballStemmer\\n>>> stemmer = SnowballStemmer(language=\\'english\\')\\n>>> \\' \\'.join([stemmer.stem(w).strip(\"\\'\") for w in\\n...   \"dish washer\\'s fairly washed dishes\".split()])\\n\\'dish washer fair wash dish\\'\\n----\\n\\n==== Lemmatization\\n\\nIf you have access to information about connections between the meanings of various words, you might be able to associate several words together even if their spelling is quite different.\\nThis more extensive normalization down to the semantic root of a word -- its lemma -- is called lemmatization.\\n\\nIn chapter 12, we show how you can use lemmatization to reduce the complexity of the logic required to respond to a statement with a chatbot.\\nAny NLP pipeline that wants to \"react\" the same for multiple different spellings of the same basic root word can benefit from a lemmatizer.\\nIt reduces the number of words you have to respond to, the dimensionality of your language model.\\nUsing it can make your model more general, but it can also make your model less precise, because it will treat all spelling variations of a given root word the same.\\nFor example \"chat\", \"chatter\", \"chatty\", \"chatting\", and perhaps even \"chatbot\" would all be treated the same in an NLP pipeline with lemmatization, even though they have different meanings.\\nLikewise \"bank\", \"banked\", and \"banking\" would be treated the same by a stemming pipeline despite the river meaning of \"bank\", the motorcycle meaning of \"banked\" and the finance meaning of \"banking.\"\\n\\nAs you work through this section, think about words where lemmatization would drastically alter the meaning of a word, perhaps even inverting its meaning and producing the opposite of the intended response from your pipeline.\\nThis scenario is called _spoofing_ -- when you try to elicit the wrong response from a machine learning pipeline by cleverly constructing a difficult input.\\n\\nSometimes lemmatization will be a better way to normalize the words in your vocabulary.\\nYou may find that for your application stemming and case folding create stems and tokens that do not take into account a word\\'s meaning.\\nA lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.\\n\\nSome lemmatizers use the word\\'s part of speech (POS) tag in addition to its spelling to help improve accuracy.\\nThe POS tag for a word indicates its role in the grammar of a phrase or sentence.\\nFor example, the noun POS is for words that refer to \"people, places, or things\" within a phrase.\\nAn adjective POS is for a word that modifies or describes a noun.\\nA verb refers to an action.\\nThe POS of a word in isolation cannot be determined.\\nThe context of a word must be known for its POS to be identified.\\nSo some advanced lemmatizers cannot be run on words in isolation.\\n\\nCan you think of ways you can use the part of speech to identify a better \"root\" of a word than stemming could?\\nConsider the word `better`.\\nStemmers would strip the \"er\" ending from \"better\" and return the stem \"bett\" or \"bet\".\\nHowever, this would lump the word \"better\" with words like \"betting\", \"bets\", and \"Bet\\'s\", rather than more similar words like \"betterment\", \"best\", or even \"good\" and \"goods\".\\n\\nSo lemmatizers are better than stemmers for most applications.\\nStemmers are only really used in large scale information retrieval applications (keyword search).\\nAnd if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer.\\nBecause the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer.\\nThis trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone.footnote:[Thank you Kyle Gorman for pointing this out]\\n\\nHow can you identify word lemmas in Python?\\nThe NLTK package provides functions for this.\\nNotice that you must tell the WordNetLemmatizer which part of speech you are interested in, if you want to find the most accurate lemma:\\n\\n[source,python]\\n----\\n>>> nltk.download(\\'wordnet\\')\\nTrue\\n>>> nltk.download(\\'omw-1.4\\')\\nTrue\\n>>> from nltk.stem import WordNetLemmatizer\\n>>> lemmatizer = WordNetLemmatizer()\\n>>> lemmatizer.lemmatize(\"better\")  # <1>\\n\\'better\\'\\n>>> lemmatizer.lemmatize(\"better\", pos=\"a\")  # <2>\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"good\", pos=\"a\")\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"goods\", pos=\"a\")\\n\\'goods\\'\\n>>> lemmatizer.lemmatize(\"goods\", pos=\"n\")\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"goodness\", pos=\"n\")\\n\\'goodness\\'\\n>>> lemmatizer.lemmatize(\"best\", pos=\"a\")\\n\\'best\\'\\n----\\n<1> The default part of speech is \"n\" for \"noun\"\\n<2> \"a\" indicates the \"adjective\" part of speech\\n\\nYou might be surprised that the first attempt to lemmatize the word \"better\" did not change it at all. This is because the part of speech of a word can have a big effect on its meaning. If a POS is not specified for a word, then the NLTK lemmatizer assumes it is a noun. Once you specify the correct POS, \\'a\\' for adjective, the lemmatizer returns the correct lemma. Unfortunately, the NLTK lemmatizer is restricted to the connections within the Princeton WordNet graph of word meanings. So the word \"best\" does not lemmatize to the same root as \"better\". This graph is also missing the connection between \"goodness\" and \"good\". A Porter stemmer, on the other hand, would make this connection by blindly stripping off the \"ness\" ending of all words.\\n\\n[source,python]\\n----\\n>>> stemmer.stem(\\'goodness\\')\\n\\'good\\'\\n----\\n\\nYou can easily implement lemmatization in spaCy by the following:\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\"en_core_web_sm\")\\n>>> doc = nlp(\"better good goods goodness best\")\\n>>> for token in doc:\\n>>> print(token.text, token.lemma_)\\nbetter well\\ngood good\\ngoods good\\ngoodness goodness\\nbest good\\n----\\nUnlike NLTK, spaCy lemmatizes \"better\" to \"well\" by assuming it is an adverb and returns the correct lemma for \"best\" (\"good\").\\n\\n==== Synonym substitution\\n\\nThere are five kinds of \"synonyms\" that are sometime helpful in creating a consistent smaller vocabulary to help your NLP pipeline generalize well.\\n\\n. Typo correction\\n. Spelling correction\\n. Synonym substitution\\n. Contraction expansion\\n. Emoji expansion\\n\\nEach of these synonym substitution algorithms can be designed to be more or less agressive.\\nAnd you will want to think about the language used by your users in your domain.\\nFor example, in the legal, technical, or medical fields, it\\'s rarely a good idea to substitute synonyms.\\nA doctor wouldn\\'t want a chatbot telling his patient their \"heart is broken\" because of some synonym substitutions on the heart emoticon (\"<3\").\\n\\nNonetheless, the use cases for lemmatization and stemming apply to synonym substitution.\\n\\n==== Use cases\\n\\nWhen should you use a lemmatizer, stemmer, or synonym substitution?\\nStemmers are generally faster to compute and require less-complex code and datasets.\\nBut stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would.\\nBoth stemmers and lemmatizers will reduce your vocabulary size and increase the ambiguity of the text.\\nBut lemmatizers do a better job retaining as much of the information content as possible based on how the word was used within the text and its intended meaning.\\nAs a result, some state of the art NLP packages, such as spaCy, do not provide stemming functions and only offer lemmatization methods.\\n\\nIf your application involves search, stemming and lemmatization will improve the recall of your searches by associating more documents with the same query words.\\nHowever, stemming, lemmatization, and even case folding will usually reduce the precision and accuracy of your search results.\\nThese vocabulary compression approaches may cause your information retrieval system (search engine) to return many documents not relevant to the words\\' original meanings.\\nThese are called \"false positives\", a incorrect matches to your search query.\\nSometimes \"false positives\" are less important than false negatives.\\nA false negative for a search engine is when it fails to list the document you are looking for at all.\\n\\nBecause search results can be ranked according to relevance, search engines and document indexes typically use lemmatization when they process your query and index your documents.\\nBecause search results can be ranked according to relevance, search engines and document indexes typically use lemmatization in their NLP pipeline.\\nThis means a search engine will use lemmatization when they tokenize your search text as well as when they index their collection of documents, such as the web pages they crawl.\\n\\nBut they combine search results for unstemmed versions of words to rank the search results that they present to you.footnote:[Additional metadata is also used to adjust the ranking of search results.\\nDuck Duck Go and other popular web search engines combine more than 400 independent algorithms (including user-contributed algorithms) to rank your search results (https://duck.co/help/results/sources).]\\n\\nFor a search-based chatbot, precision is usually more important than recall.\\nA false positive match can cause your chatbot says something inappropriate.\\nFalse negatives just cause your chatbot to have to humbly admit that it cannot find anything appropriate to say.\\nYour chatbot will sound better if your NLP pipeline first searches for matches to your user\\'s questions using unstemmed, unnormalized words.\\nYour search algorithm can fall back to normalized token matches if it cannot find anything else to say.\\nAnd you can rank these *fallback* matches for normalized tokens lower than the unnormalized token matches.\\nYou can even give your bot humility and transparency by introducing lower ranked responses with a caveat, such as \"I haven\\'t heard a phrase like that before, but using my stemmer I found...\"\\nIn a modern world crowded with blowhard chatbots, your humbler chatbot can make a name for itself and win out!footnote:[\"Nice guys finish first!\" -- M.A. Nowak author of _SuperCooperators_\"]\\n\\nThere are 4 situations when synonym substitution of some sort may make sense.\\n\\n. Search engines\\n. Data augmentation\\n. Scoring the robustness of your NLP\\n. Adversarial NLP\\n\\nSearch engines can improve their recall for rare terms by using synonym substitution.\\nWhen you have limited labeled data, you can often expand your dataset 10 fold (10x) with synonym substitution alone.\\nIf you want to find a lower bound on the accuracy of your model you can aggressively substitute synonyms in your test set to see how robust your model is to these changes.\\nAnd if you are searching for ways to poison or evade detection by an NLP algorithm, synonyms can give you a large number of probing texts to try.\\nYou can imagine that substituting the \"currency\" for the word \"cash\", \"dollars\", or \"$$$$\" might help evade a spam detector.\\n\\n[IMPORTANT]\\nBottom line, try to avoid stemming, lemmatization, case folding, or synonym substitution, unless you have a limited amount of text with contains usages and capitalizations of the words you are interested in.\\nAnd with the explosion of NLP datasets, this is rarely the case for English documents, unless your documents use a lot of jargon or are from a very small subfield of science, technology, or literature.\\nNonetheless, for languages other than English, you may still find uses for lemmatization.\\nThe Stanford information retrieval course dismisses stemming and lemmatization entirely, due to the negligible recall accuracy improvement and the significant reduction in precision.footnote:[See the Stanford NLP Information Retrieval (IR) book section titled \"Stemming and lemmatization\" (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).]\\n\\n\\n\\n== Sentiment\\n\\nWhether you use raw single-word tokens, _n_-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information.\\nAn important part of this information is the word\\'s sentiment -- the overall feeling or emotion that word invokes.\\nThis _sentiment analysis_ -- measuring the sentiment of phrases or chunks of text -- is a common application of NLP.\\nIn many companies it is the main thing an NLP engineer is asked to do.\\n\\nCompanies like to know what users think of their products.\\nSo they often will provide some way for you to give feedback.\\nA star rating on Amazon or Rotten Tomatoes is one way to get quantitative data about how people feel about products they\\'ve purchased.\\nBut a more natural way is to use natural language comments.\\nGiving your user a blank slate (an empty text box) to fill up with comments about your product can produce more detailed feedback.\\n\\nIn the past you would have to read all that feedback.\\nOnly a human can understand something like emotion and sentiment in natural language text, right?\\nHowever, if you had to read thousands of reviews you would see how tedious and error-prone a human reader can be.\\nHumans are remarkably bad at reading feedback, especially criticism or negative feedback.\\nAnd customers are not generally very good at communicating feedback in a way that can get past your natural human triggers and filters.\\n\\nBut machines do not have those biases and emotional triggers.\\nAnd humans are not the only things that can process natural language text and extract information, even meaning from it.\\nAn NLP pipeline can process a large quantity of user feedback quickly and objectively, with less chance for bias.\\nAnd an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\\n\\nAnother common application of sentiment analysis is junk mail and troll message filtering.\\nYou would like your chatbot to be able to measure the sentiment in the chat messages it processes so it can respond appropriately.\\nAnd even more importantly, you want your chatbot to measure its own sentiment of the statements it is about to send out, which you can use to steer your bot to be kind and pro-social with the statements it makes.\\nThe simplest way to do this might be to do what Moms told us to do: If you cannot say something nice, do not say anything at all.\\nSo you need your bot to measure the niceness of everything you are about to say and use that to decide whether to respond.\\n\\nWhat kind of pipeline would you create to measure the sentiment of a block of text and produce this sentiment positivity number?\\nSay you just want to measure the positivity or favorability of a text -- how much someone likes a product or service that they are writing about.\\nSay you want your NLP pipeline and sentiment analysis algorithm to output a single floating point number between -1 and +1.\\nYour algorithm would output +1 for text with positive sentiment like \"Absolutely perfect! Love it! :-) :-) :-)\".\\nAnd your algorithm should output -1 for text with negative sentiment like \"Horrible! Completely useless. :(\".\\nYour NLP pipeline could use values near 0, like say +0.1, for a statement like \"It was OK. Some good and some bad things\".\\n\\nThere are two approaches to sentiment analysis:\\n\\n* A rule-based algorithm composed by a human\\n* A _machine learning_ model learned from data by a machine\\n\\nThe first approach to sentiment analysis uses human-designed rules, sometimes called heuristics, to measure sentiment.\\nA common rule-based approach to sentiment analysis is to find keywords in the text and map each one to numerical scores or weights in a dictionary or \"mapping\" -- a Python `dict`, for example.\\nNow that you know how to do tokenization, you can use stems, lemmas, or _n_-gram tokens in your dictionary, rather than just words.\\nThe \"rule\" in your algorithm would be to add up these scores for each keyword in a document that you can find in your dictionary of sentiment scores.\\nOf course you need to hand-compose this dictionary of keywords and their sentiment scores before you can run this algorithm on a body of text.\\nWe show you how to do this using the VADER algorithm (in `sklearn`) in the upcoming listing.\\n\\nThe second approach, machine learning, relies on a labeled set of statements or documents to train a machine learning model to create those rules.\\nA machine learning sentiment model is trained to process input text and output a numerical value for the sentiment you are trying to measure, like positivity or spamminess or trolliness.\\nFor the machine learning approach, you need a lot of data, text labeled with the \"right\" sentiment score.\\nTwitter feeds are often used for this approach because the hash tags, such as `\\\\#awesome` or `\\\\#happy` or `\\\\#sarcasm`, can often be used to create a \"self-labeled\" dataset.\\nYour company may have product reviews with five-star ratings that you could associate with reviewer comments.\\nYou can use the star ratings as a numerical score for the positivity of each text.\\nWe show you shortly how to process a dataset like this and train a token-based machine learning algorithm called _Naive Bayes_ to measure the positivity of the sentiment in a set of reviews after you are done with VADER.\\n\\n=== VADER -- A rule-based sentiment analyzer\\n\\nHutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms.\\nThey called their algorithm VADER, for **V**alence **A**ware **D**ictionary for\\ns**E**ntiment **R**easoning.footnote:[\"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\" by Hutto and Gilbert (http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).]\\nMany NLP packages implement some form of this algorithm.\\nThe NLTK package has an implementation of the VADER algorithm in `nltk.sentiment.vader`.\\nHutto himself maintains the Python package `vaderSentiment`.\\nYou will go straight to the source and use `vaderSentiment` here.\\n\\nYou will need to `pip install vaderSentiment` to run the following example.footnote:[You can find more detailed installation instructions with the package source code on github (https://github.com/cjhutto/vaderSentiment).]\\nYou have not included it in the `nlpia` package.\\n\\n[source,python]\\n----\\n>>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n>>> sa = SentimentIntensityAnalyzer()\\n>>> sa.lexicon  # <1>\\n{ ...\\n\\':(\\': -1.9,  # <2>\\n\\':)\\': 2.0,\\n...\\n\\'pls\\': 0.3,  # <3>\\n\\'plz\\': 0.3,\\n...\\n\\'great\\': 3.1,\\n... }\\n>>> [(tok, score) for tok, score in sa.lexicon.items()\\n...   if \" \" in tok]  # <4>\\n[(\"( \\'}{\\' )\", 1.6),\\n (\"can\\'t stand\", -2.0),\\n (\\'fed up\\', -1.8),\\n (\\'screwed up\\', -1.5)]\\n>>> sa.polarity_scores(text=\\\\\\n...   \"Python is very readable and it\\'s great for NLP.\")\\n{\\'compound\\': 0.6249, \\'neg\\': 0.0, \\'neu\\': 0.661,\\n\\'pos\\': 0.339}  # <5>\\n>>> sa.polarity_scores(text=\\\\\\n...   \"Python is not a bad choice for most applications.\")\\n{\\'compound\\': 0.431, \\'neg\\': 0.0, \\'neu\\': 0.711,\\n\\'pos\\': 0.289}  # <6>\\n----\\n<1> SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores that we talked about.\\n<2> A tokenizer better be good at dealing with punctuation and emoticons (emojis) for VADER to work well. After all, emoticons are designed to convey a lot of sentiment (emotion).\\n<3> If you use a stemmer (or lemmatizer) in your pipeline, you would need to apply that stemmer to the VADER lexicon, too, combining the scores for all the words that go together in a single stem or lemma.\\n<4> Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually _n_-grams; the other is an emoticon for \"kiss\".\\n<5> The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them together into a compound positivity sentiment.\\n<6> Notice that VADER handles negation pretty well -- \"great\" has a slightly more positive sentiment than \"not bad\". VADER\\'s built-in tokenizer ignores any words that aren\\'t in its lexicon, and it does not consider _n_-grams at all.\\n\\nLet us see how well this rule-based approach does for the example statements we mentioned earlier.\\n\\n[source,python]\\n----\\n>>> corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\\n...           \"Horrible! Completely useless. :(\",\\n...           \"It was OK. Some good and some bad things.\"]\\n>>> for doc in corpus:\\n...     scores = sa.polarity_scores(doc)\\n...     print(\\'{:+}: {}\\'.format(scores[\\'compound\\'], doc))\\n+0.9428: Absolutely perfect! Love it! :-) :-) :-)\\n-0.8768: Horrible! Completely useless. :(\\n+0.3254: It was OK. Some good and some bad things.\\n----\\n\\nThis looks a lot like what you wanted.\\nSo the only drawback is that VADER does not look at all the words in a document.\\nVADER only \"knows\" about the 7,500 words or so that were hard-coded into its algorithm.\\nWhat if you want all the words to help add to the sentiment score?\\nAnd what if you do not want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in `SentimentIntensityAnalyzer.lexicon`?\\nThe rule-based approach might be impossible if you do not understand the language because you would not know what scores to put in the dictionary (lexicon)!\\n\\nThat is what machine learning sentiment analyzers are for.\\n\\n=== Closeness of vectors\\n\\nWhy do we use bags of words rather than bags of characters to represent natural language text?\\nFor a cryptographer trying to decrypt an unknown message, frequency analysis of the characters in the text would be a good way to go.\\nBut for natural language text in your native language, words turn out to be a better representation.\\nYou can see this if you think about what we are using these BOW vectors for.\\n\\nIf you think about it, you have a lot of different ways to measure the closeness of things.\\nYou probably have a good feel for what a close family relative would be.\\nOr the closeness of the cafes where you can meet your friend to collaborate on writing a book about AI.\\nFor cafes your brain probably uses Euclidean distance on the 2D position of the cafes you know about.\\nOr maybe Manhattan or taxi-cab distance.\\n\\nBut do you know how to measure the closeness of two pieces of text?\\nIn chapter 4 you\\'ll learn about edit distances that check the similarity of two strings of characters.\\nBut that doesn\\'t really capture the essence of what you care about.\\n\\nHow close are these sentences to each other, in your mind?\\n\\n> I am now coming over to see you.\\n\\n> I am not coming over to see you.\\n\\nDo you see the difference?\\nWhich one would you prefer to receive an e-mail from your friend.\\nThe words \"now\" and \"not\" are very far apart in meaning.\\nBut they are very close in spelling.\\nThis is an example about how a single character can change the meaning of an entire sentence.\\n\\nIf you just counted up the characters that were different you\\'d get a distance of 1.\\nAnd then you could divide by the length of the longest sentence to make sure your distance value is between 0 and 1.\\nSo your character difference or distance calculation would be 1 divided by 32 which gives 0.03125, or about 3%.\\nThen, to turn a distance into a closeness you just subtract it from 1.\\nSo do you think these two sentences are 0.96875, or about 97% the same?\\nThey mean the opposite.\\nSo we\\'d like a better measure than that.\\n\\nWhat if you compared words instead of characters?\\nIn that case you would have one word out of seven that was changed.\\nThat is a little better than one character out of 32.\\nThe sentences would now have a closeness score of six divided by seven or about 85%.\\nThat\\'s a little lower, which is what we want.\\n\\nFor natural language you don\\'t want your closeness or distance measure to rely only on a count of the differences in individual characters.\\nThis is one reason why you want to use words as your tokens of meaning when processing natural language text.\\n\\nWhat about these two sentences?\\n\\n> She and I will come over to your place at 3:00.\\n\\n> At 3:00, she and I will stop by your apartment.\\n\\nAre these two sentences close to each other in meaning?\\nThey have the exact same length in characters.\\nAnd they use some of the same words, or at least synonyms.\\nBut those words and characters are not in the same order.\\nSo we need to make sure that our representation of the sentences does not rely on the precise position of words in a sentence.\\n\\nBag of words vectors accomplish this by creating a position or slot in a vector for every word you\\'ve seen in your vocabulary.\\nYou may have learned of a few measures of closeness in geometry and linear algebra.\\n\\nAs an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\\nVery smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\\nImagine how difficult that is.\\nImagine trying to remove verb endings like \"ing\" from \"ending\" so you\\'d have a stem called \"end\" to represent both words.\\nAnd you\\'d like to stem the word \"running\" to \"run,\" so those two words are treated the same.\\nAnd that\\'s tricky, because you have remove not only the \"ing\" but also the extra \"n\".\\nBut you want the word \"sing\" to stay whole.\\nYou wouldn\\'t want to remove the \"ing\" ending from \"sing\" or you\\'d end up with a single-letter \"s\".\\n\\nOr imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens\".\\nDo isolated individual letters in a word or parts of a word provide any information at all about that word\\'s meaning?\\nCan the letters be misleading?\\nYes and yes.\\n\\nIn this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\\nLater, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you\\'re interested in.\\nFrom that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\\n\\n=== Count vectorizing\\n\\nIn the previous sections you\\'ve only been concerned with keyword detection.\\nYour vectors indicated the presence or absence of words.\\nIn order to handle longer documents and improve the accuracy of your NLP pipeline, you\\'re going to start counting the occurrences of words in your documents.\\n\\nYou can put these counts into a sort-of histogram.\\nJust like before you will create a vector for each document in your pipeline.\\nOnly instead of 0\\'s and 1\\'s in your vectors there will be counts.\\nThis will improve the accuracy of all the similarity and distance calculations you are doing with these counts.\\nAnd just like normalizing histograms can improve your ability to compare two histograms, normalizing your word counts is also a good idea.\\nOtherwise a really short wikipedia article that use Barak Obama\\'s name only once along side all the other presidents might get as much \"Barack Obama\" credit as a much longer page about Barack Obama that uses his name many times.\\nUsers and Question Answering bots like `qary` trying to answer questions about Obama might get distracted by pages listing all the presidents and might miss the main Barack Obama page entirely.\\nSo it\\'s a good idea to normalize your count vectors by dividing the counts by the total length of the document.\\nThis more fairly represents the distribution of tokens in the document and will create better similarity scores with other documents, including the text from a search query from `qary`.footnote:[Qary is an open source virtual assistant that actually assists you instead of manipulating and misinforming you (https://docs.qary.ai).]\\n\\nEach position in your vector represents the count for one of your keywords.\\nAnd having a small vocabulary keeps this vector small, low-dimensional, and easy to reason about.\\nAnd you can use this _count vectorizing_ approach even for large vocabularies.\\n\\nAnd you can organize these counts of those keywords into\\nYou need to organize the counts into a vector.\\nThis opens up a whole range of powerful tools for doing vector algebra.\\n\\nIn natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\\nNonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\\nThe techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email from a flood of spam that almost made it useless.\\n\\n\\n=== Naive Bayes\\n\\nA Naive Bayes model tries to find keywords in a set of documents that are predictive of your target (output) variable.\\nWhen your target variable is the sentiment you are trying to predict, the model will find words that predict that sentiment.\\nThe nice thing about a Naive Bayes model is that the internal coefficients will map words or tokens to scores just like VADER does.\\nOnly this time you will not have to be limited to just what an individual human decided those scores should be.\\nThe machine will find the \"best\" scores for any problem.\\n\\nFor any machine learning algorithm, you first need to find a dataset.\\nYou need a bunch of text documents that have labels for their positive emotional content (positivity sentiment).\\nHutto compiled four different sentiment datasets for us when he and his collaborators built VADER.\\nYou will load them from the `nlpia` package.footnote:[If you have not already installed `nlpia`, check out the installation instructions at http://gitlab.com/tangibleai/nlpia2.]\\n\\n[source,python]\\n----\\n>>> movies = pd.read_csv(\\'https://proai.org/movie-reviews.csv.gz\\', \\\\\\n...     index_col=0)\\n>>> movies.head().round(2)\\n    sentiment                                               text\\nid                                                              \\n1        2.27  The Rock is destined to be the 21st Century\\'s ...\\n2        3.53  The gorgeously elaborate continuation of \\'\\'The...\\n3       -0.60                     Effective but too tepid biopic\\n4        1.47  If you sometimes like to go to the movies to h...\\n5        1.73  Emerges as something rare, an issue movie that...\\n\\n>>> movies.describe().round(2)\\n       sentiment\\ncount   10605.00\\nmean        0.00  # <1>\\nstd         1.92\\nmin        -3.88  # <2>\\n...\\nmax         3.94  # <3>\\n----\\n<1> Sentiment scores (movie ratings) have been \"centered\" (mean is zero)\\n<2> It looks like the scale starts around -4 for the worst movies\\n<3> Seems like +4 is the maximum rating for the best movies\\n\\nIt looks like the movie reviews have been _centered_: normalized by subtracting the mean so that the new mean will be zero and they aren\\'t biased to one side or the other.\\nAnd it seems the range of movie ratings allowed was -4 to +4.\\n\\nNow you can tokenize all those movie review texts to create a bag of words for each one.\\nIf you put them all into a Pandas DataFrame that will make them easier to work with.\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> pd.options.display.width = 75  # <1>\\n>>> from nltk.tokenize import casual_tokenize  # <2>\\n>>> bags_of_words = []\\n>>> from collections import Counter  # <3>\\n>>> for text in movies.text:\\n...     bags_of_words.append(Counter(casual_tokenize(text)))\\n>>> df_bows = pd.DataFrame.from_records(bags_of_words)  # <4>\\n>>> df_bows = df_bows.fillna(0).astype(int)  # <5>\\n>>> df_bows.shape  # <6>\\n(10605, 20756)\\n\\n>>> df_bows.head()\\n   !  \"  #  $  %  &  \\' ...  zone  zoning  zzzzzzzzz  ½  élan  –  ’\\n0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\\n1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\\n2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n\\n>>> df_bows.head()[list(bags_of_words[0].keys())]\\n   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  .\\n0    1     1   1         1   2   1 ...    1      1   1       1      1  1\\n1    2     0   1         0   0   0 ...    0      0   0       0      0  4\\n2    0     0   0         0   0   0 ...    0      0   0       0      0  0\\n3    0     0   1         0   4   0 ...    0      0   0       0      0  1\\n4    0     0   0         0   0   0 ...    0      0   0       0      0  1\\n----\\n<1> This prints a wide `DataFrame` in the console so they look prettier.\\n<2> NLTK\\'s `casual_tokenize` can handle emoticons, unusual punctuation, and slang better than `TreebankWordTokenizer`\\n<3> `Counter` takes a list (or iterable) of objects and counts them up, returning a `dict` where the keys are the objects (tokens in your case) and the values are the counts.\\n<4> The `from_records()` DataFrame constructor takes a sequence of dict objects. The `dict` keys become columns, and the values for missing keys are set to `NaN`.\\n<5> NumPy and Pandas can only represent NaNs within a float dtype. So fill NaNs with zeros before converting to integers.\\n<6> A BOW table can get really big if you don\\'t do dimension reduction or feature selection.\\n\\nWhen you do not use case normalization, stop word filters, stemming, or lemmatization your vocabulary can be quite huge because you are keeping track of every little difference in spelling or capitalization of words.\\nTry inserting some dimension reduction steps into your pipeline to see how they affect your pipeline\\'s accuracy and the amount of memory required to store all these BOWs.\\n\\nNow you have all the data that a Naive Bayes model needs to find the keywords that predict sentiment from natural language text.\\n\\n[source,python]\\n----\\n>>> from sklearn.naive_bayes import MultinomialNB\\n>>> nb = MultinomialNB()\\n>>> nb = nb.fit(df_bows, movies.sentiment > 0)  # <1>\\n>>> movies[\\'pred_senti\\'] = (\\n...   nb.predict_proba(df_bows))[:, 1] * 8 - 4  # <2>\\n>>> movies[\\'error\\'] = movies.pred_senti - movies.sentiment\\n>>> mae = movies[\\'error\\'].abs().mean().round(1)  # <3>\\n>>> mae\\n1.9\\n----\\n\\nTo create a binary classification label you can use the fact that the centered movie ratings (sentiment labels) are positive (greater than zero) when the sentiment of the review is positive.\\n\\n[source,python]\\n----\\n>>> movies[\\'senti_ispos\\'] = (movies[\\'sentiment\\'] > 0).astype(int)\\n>>> movies[\\'pred_ispos\\'] = (movies[\\'pred_senti\\'] > 0).astype(int)\\n>>> columns = [c for c in movies.columns if \\'senti\\' in c or \\'pred\\' in c]\\n>>> movies[columns].head(8)\\n    sentiment  pred_senti  senti_ispos  pred_ispos\\nid\\n1    2.266667                    4                     1                     1\\n2    3.533333                    4                     1                     1\\n3   -0.600000                   -4                     0                     0\\n4    1.466667                    4                     1                     1\\n5    1.733333                    4                     1                     1\\n6    2.533333                    4                     1                     1\\n7    2.466667                    4                     1                     1\\n8    1.266667                   -4                     1                     0\\n>>> (movies.pred_ispos ==\\n...   movies.senti_ispos).sum() / len(movies)\\n0.9344648750589345  # <4>\\n----\\n<1> Naive Bayes models are classifiers, so you need to convert your output variable (sentiment float) to a discrete label (integer, string, or bool).\\n<2> Convert your discrete classification variable back to a real value between -4 and +4 so you can compare it to the \"ground truth\" sentiment.\\n<3> Average absolute value of the prediction error or mean absolute error (MAE)\\n<4> You got the \"thumbs up\" rating correct 93% of the time.\\n\\nThis is a pretty good start at building a sentiment analyzer with only a few lines of code (and a lot of data).\\nYou did not have to guess at the sentiment associated with a list of 7500 words and hard code them into an algorithm such as VADER.\\nInstead you told the machine the sentiment ratings for whole text snippets.\\nAnd then the machine did all the work to figure out the sentiment associated with each word in those texts.\\nThat is the power of machine learning and NLP!\\n\\nHow well do you think this model will generalize to a completely different set text examples such as product reviews?\\nDo people use the same words to describe things they like in movie and product reviews such as electronics and household goods? \\nProbably not.\\nBut it\\'s a good idea to check the robustness of your language models by running it against challenging text from a different domain.\\nAnd by testing your model on new domains, you can get ideas for more examples and datasets to use in your training and test sets.\\n\\nFirst you need to load the product reviews.\\n\\n[source,python]\\n----\\n>>> products = pd.read_csv(\\'https://proai.org/product-reviews.csv.gz\\')\\n>>> for text in products[\\'text\\']:\\n...     bags_of_words.append(Counter(casual_tokenize(text)))\\n>>> df_product_bows = pd.DataFrame.from_records(bags_of_words)\\n>>> df_product_bows = df_product_bows.fillna(0).astype(int)\\n>>> df_all_bows = df_bows.append(df_product_bows)\\n>>> df_all_bows.columns  # <1>\\nIndex([\\'!\\', \\'\"\\', \\'#\\', \\'#38\\', \\'$\\', \\'%\\', \\'&\\', \\'\\'\\', \\'(\\', \\'(8\\',\\n       ...\\n       \\'zoomed\\', \\'zooming\\', \\'zooms\\', \\'zx\\', \\'zzzzzzzzz\\', \\'~\\', \\'½\\', \\'élan\\',\\n       \\'–\\', \\'’\\'],\\n      dtype=\\'object\\', length=23302)\\n>>> df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]  # <2>\\n>>> df_product_bows.shape\\n(3546, 20756)\\n\\n>>> df_bows.shape  # <3>\\n(10605, 20756)\\n----\\n<1> Your new bags of words have some tokens that were not in the original bags of words DataFrame (23302 columns now instead of 20756 before).\\n<2> You need to make sure your new product DataFrame of bags of words has the exact same columns (tokens) in the exact same order as the original one used to train your Naive Bayes model.\\n<3> The movie bags of words have the same size vocabulary (columns) as for products.\\n\\nNow you need to convert the labels to mimic the binary classification data that you trained your model on.\\n\\n[source,python]\\n----\\n>>> products[\\'senti_ispos\\'] = (products[\\'sentiment\\'] > 0).astype(int)\\n>>> products[\\'pred_ispos\\'] = nb.predict(df_product_bows).astype(int)\\n>>> products.head()\\n    id  sentiment                                               text  senti_ispos\\n0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...                     0\\n1  1_2      -0.15  repost from january 13, 2004 with a better fit...                     0\\n2  1_3      -0.20  does your apex dvd player only play dvd audio ...                     0\\n3  1_4      -0.10  or does it play audio and video but scrolling ...                     0\\n4  1_5      -0.50  before you try to return the player or waste h...                     0\\n\\n>>> tp = products[\\'pred_ispos\\'] == products[\\'senti_ispos\\']  # <1>\\n>>> tp.sum() / len(products)\\n0.5572476029328821\\n----\\n<1> True positive predictions are when both the predicted and true sentiment are positive\\n\\n\\nSo your Naive Bayes model does a  poor job of predicting whether a product review is positive (thumbs up).\\nOne reason for this subpar performance is that your vocabulary from the `casual_tokenize` product texts has 2546 tokens that were not in the movie reviews.\\nThat is about 10% of the tokens in your original movie review tokenization, which means that all those words will not have any weights or scores in your Naive Bayes model.\\nAlso the Naive Bayes model does not deal with negation as well as VADER does.\\nYou would need to incorporate _n_-grams into your tokenizer to connect negation words (such as \"not\" or \"never\") to the positive words they might be used to qualify.\\n\\nWe leave it to you to continue the NLP action by improving on this machine learning model.\\nAnd you can check your progress relative to VADER at each step of the way to see if you think machine learning is a better approach than hard-coding algorithms for NLP.\\n\\n== Review\\n\\n. How does a lemmatizer increase the likelihood that your DuckDuckGo search results contain what you are looking for?\\n. Is there a way to optimally decide the _n_ in the _n_-gram range you use to tokenize your documents?\\n. Does lemmatization, case folding, or stopword removal help or hurt your performance on a model to predict misleading news articles with this Kaggle dataset:\\n. How could your find out the best sizes for the word pieces or sentence pieces for your tokenizer?\\n. Is there a website where you can download the token frequencies for most of the words and n-grams ever published?footnote:[Hint: A company that aspired to \"do no evil\", but now does, created this massive NLP corpus.]\\n. What are the risks and possible benefits of pair coding AI assistants built with NLP? What sort of organizations and algorithms do you trust with your mind and your code?\\n\\n=== Summary\\n\\n* You implemented tokenization and configured a tokenizer for your application.\\n* _n_-gram tokenization helps retain some of the \"word order\" information in a document.\\n* Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\\n* Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\\n* Stop words can contain useful information, and discarding them is not always helpful.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d4699fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161474"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5717fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e701fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "= Tokens of thought (natural language words)\n",
       ":chapter: 2\n",
       ":part: 1\n",
       ":imagesdir: .\n",
       ":xrefstyle: short\n",
       ":figure-caption: Figure {chapter}.\n",
       ":listing-caption: Listing {chapter}.\n",
       ":table-caption: Table {chapter}.\n",
       ":stem: latexmath\n",
       "\n",
       "This chapter covers\n",
       "\n",
       "* Parsing your text into words and _n_-grams (tokens)\n",
       "* Tokenizing punctuation, emoticons, and even Chinese characters\n",
       "* Consolidating your vocabulary with stemming, lemmatization, and case folding\n",
       "* Building a structured numerical representation of natural language text\n",
       "* Scoring text for sentiment and prosocial intent\n",
       "* Using character frequency analysis to optimize your token vocabulary\n",
       "* Dealing with variable length sequences of words and tokens\n",
       "\n",
       "\n",
       "So you want to help save the world with the power of natural language processing (NLP)?\n",
       "First your NLP pipeline will need to compute something about text, and for that you'll need a way to represent text in a numerical data structure.\n",
       "The part of an NLP pipeline that breaks up your text to create this structured numerical data is called a _parser_.\n",
       "For many NLP applications, you only need to convert your text to a sequence of words, and that can be enough for searching and classifying text.\n",
       "\n",
       "You will now learn how to split a document, any string, into discrete tokens of meaning.\n",
       "You will be able to parse text documents as small as a single word and as large as an entire Encyclopedia.\n",
       "And they will all produce a consistent representation that you can use to compare them.\n",
       "For this chapter your tokens will be words, punctuation marks, and even pictograms such as Chinese characters, emojis and emoticons.\n",
       "\n",
       "Later in the book you will see that you can use these same techniques to find packets of meaning in any discrete sequence.\n",
       "For example, your tokens could be the ASCII characters represented by a sequence of bytes, perhaps with ASCII emoticons.\n",
       "Or they could be Unicode emojis, mathematical symbols, Egyption, hieroglyphics, pictographs from languages like Kanji  and Cantonese.\n",
       "You could even define the tokens for DNA and RNA sequences with letters for each of the five base nucleotides: adenine (A), guanine (G), cytosine \\(C), thymine (T), and uracil (U).\n",
       "Natural language sequences of tokens are all around you ... and even inside you.\n",
       "\n",
       "Is there something you can do with tokens that doesn't require a lot of complicated deep learning?\n",
       "If you have a good tokenizer you can use it to identify statistics about the occurrence of tokens in a set of documents, such as your blog posts or a business website.\n",
       "Then you can build a search engine in pure Python with just a dictionary to represent to record links to the set of documents where those words occur.\n",
       "That Python dictionary that maps words to document links or pages is called a reverse index.\n",
       "It's just like the index at the back of this book.\n",
       "This is called _information retrieval_ -- a really powerful tool in your NLP toolbox.\n",
       "\n",
       "Statistics about tokens are often all you need for keyword detection, full text search, and information retrieval.\n",
       "You can even build customer support chatbots using text search to find answers to customers' questions in your documentation or FAQ (frequently asked question) lists.\n",
       "A chatbot can't answer your questions until it knows where to look for the answer.\n",
       "Search is the foundation of many state of the art applications such as conversational AI and open domain question answering.\n",
       "A tokenizer forms the foundation for almost all NLP pipelines.\n",
       "\n",
       "=== Tokens of emotion\n",
       "\n",
       "Another practical use for your tokenizer is called _sentiment analysis_, or analysis of text to estimate emotion.\n",
       "You'll see an example of a sentiment analysis pipeline later in this chapter.\n",
       "For now you just need to know how to build a tokenizer.\n",
       "And your tokenizer will almost certainly need to handle the tokens of emotion called _emoticons_ and _emojis_.\n",
       "\n",
       "_Emoticons_ are a textual representations of a writer's mood or facial expression, such as the _smiley_ emoticon: `:-)`.\n",
       "They are kind-of like a modern hieroglyph or picture-word for computer users that only have access to an ASCII terminal for communication.\n",
       "_Emojis_ are the graphical representation of these characters.\n",
       "For example, the smilie emoji has a small yellow circle with two black dots for eyes and a U shaped curve for a mouth.\n",
       "The smiley emoji is a graphical representation of the `:-)` smiley emoticon.\n",
       "\n",
       "Both emojis and emoticons have evolved into their own language.\n",
       "There are hundreds of popular emojis.\n",
       "People have created emojis for everything from company logos to memes and innuendo.\n",
       "Noncommercial social media networks such Mastodon even allow you to create your own custom emojis.footnote:[Mastodon servers you can join (https://proai.org/mastoserv)] footnote:[Mastodon custom emoji documentation (https://docs.joinmastodon.org/methods/custom_emojis/)] \n",
       "\n",
       ".Emojis and Emoticons\n",
       "[NOTE]\n",
       "====\n",
       "_Emoticons_ were first typed into an ASCII text message in 1972 when Carnegie Mellon researchers mistakenly understood a text message about a mercury spill to be a joke.\n",
       "The professor, Dr. Scott E. Fahlman, suggested that `:-)` should be appended to messages that were jokes, and  `:-(` emoticons should be used for serious warning messages. \n",
       "Gosh, how far we've come.\n",
       "====\n",
       "\n",
       "The plural of \"emoji\" is either \"emoji\" (like \"sushi\") or \"emojis\" (like \"Tsunamis\"), however the the Atlantic and NY Times style editors prefer \"emojis\" to avoid ambiguity.\n",
       "Your NLP pipeline will learn what you mean no matter how you type it.\n",
       "\n",
       "image::../images/ch02/wikipedia-smiley-icon.svg[alt=\"Smiley icon from wikipedia article en.wikipedia.org/wiki/Smiley\",align=\"center\",width=100%,link=\"../images/ch02/wikipedia-smiley-icon.svg\"]\n",
       "\n",
       "== What is a token?\n",
       "\n",
       "A token can be almost any chunk of text that you want to treat as a packet of thought and emotion.\n",
       "So you need to break your text into chunks that capture individual thoughts.\n",
       "You may be thinking that _words_ are the obvious choice for tokens.\n",
       "So that's what you will start with here.\n",
       "You'll also learn how to include punctuation marks, emojis, numbers, and other word-like things in your vocabulary of words.\n",
       "Later you'll see that you can use these same techniques to find packets of meaning in any discrete sequence.\n",
       "And later you will learn some even more powerful ways to split discrete sequences into meaningful packets.\n",
       "Your tokenizers will be soon able to analyze and structure any text document or string, from a single word, to a sentence, to an entire book.\n",
       "\n",
       "Think about a collection of documents, called a _corpus_, that you want to process with NLP.\n",
       "Think about the _vocabulary_ that would be important to your NLP algorithm -- the set of tokens you will need to keep track of.\n",
       "For example your tokens could be the characters for ASCII emoticons, if this is what is important in your NLP pipeline for a particular corpus.\n",
       "Or your tokens could be Unicode emojis, mathematical symbols, hieroglyphics, even pictographs like Kanji and Cantonese characters.\n",
       "Your tokenizer and your NLP pipeline would even be useful for the nucleotide sequences of DNA and RNA where your tokens might be A, C, T, G, U, and so on.\n",
       "And neuroscientists sometimes create sequences of discrete symbols to represent neurons firing in your brain when you read text like this sentence.\n",
       "Natural language sequences of tokens are inside you, all around you, and flowing through you.\n",
       "Soon you'll be flowing streams of tokens through your machine learning NLP pipeline.\n",
       "\n",
       "Retrieving tokens from a document will require some string manipulation beyond just the `str.split()` method employed in chapter 1.\n",
       "You'll probably want to split contractions like \"you'll\" into the words that were combined to form them, perhaps \"you\" and \"'ll\", or perhaps \"you\" and \"will.\"\n",
       "You'll want to separate punctuation from words, like quotes at the beginning and end of quoted statements or words, such as those in the previous sentence.\n",
       "And you need to treat some punctuation such as dashes (\"-\") as part of singly-hyphenated compound words such as \"singly-hyphenated.\"\n",
       "\n",
       "Once you have identified the tokens in a document that you would like to include in your vocabulary, you will return to the regular expression toolbox to build a tokenizer.\n",
       "And you can use regular expressions combine different forms of a word into a single token in your vocabulary -- a process called _stemming_.\n",
       "Then you will assemble a vector representation of your documents called a _bag of words_.\n",
       "Finally, you will try to use this bag of words vector to see if it can help you improve upon the basic greeting recognizer at the end of chapter 1.\n",
       "\n",
       "=== Alternative tokens\n",
       "\n",
       "Words aren't the only packets of meaning we could use for our tokens.\n",
       "Think for a moment about what a word or token represents to you.\n",
       "Does it represent a single concept, or some blurry cloud of concepts?\n",
       "Could you always be sure to recognize where a word begins and ends?\n",
       "Are natural language words like programming language keywords that have precise spellings, definitions and grammatical rules for how to use them?\n",
       "Could you write software that reliably recognizes a word?\n",
       "\n",
       "Do you think of \"ice cream\" as one word or two?\n",
       "Or maybe even three?\n",
       "Aren't there at least two entries in your mental dictionary for \"ice\" and \"cream\" that are separate from your entry for the compound word \"ice cream\"?\n",
       "What about the contraction \"don't\"?\n",
       "Should that string of characters be split into one, or two, or even three packets of meaning?\n",
       "\n",
       "You might even want to divide words into even smaller meaningful parts.\n",
       "Word pieces such as the prefix \"pre\", the suffix \"fix\", or the interior syllable \"la\" all have meaning.\n",
       "You can use these word pieces to transfer what you learn about the meaning of one word to another similar word in your vocabulary.\n",
       "Your NLU pipeline can even use these pieces to understand new words.\n",
       "And your NLG pipeline can use the pieces to create new words that succinctly capture ideas or memes circulating in the collective consciousness.\n",
       "\n",
       "Your pipeline could break words into even smaller pieces.\n",
       "Letters, characters, or graphemes footnote:[(https://en.wikipedia.org/wiki/Grapheme)] carry sentiment and meaning too!footnote:[Suzi Park and Hyopil Shin _Grapheme-level Awareness in Word Embeddings for Morphologically Rich Languages_ (https://www.aclweb.org/anthology/L18-1471.pdf)]\n",
       "We haven't yet found the perfect encoding for packets of thought.\n",
       "And machines compute differently than brains.\n",
       "We explain language and concepts to each other in terms of words or terms.\n",
       "But machines can often see patterns in the use of characters that we miss.\n",
       "And for machines to be able to squeeze huge vocabularies into their limited RAM there are more efficient encodings for natural language.\n",
       "\n",
       "The optimal tokens for efficient computation are different from the packets of thought (words) that we humans use.\n",
       "Byte Pair Encoding (BPE), Word Piece Encoding, and Sentence Piece Encoding, each can help machines use natural language more efficiently.\n",
       "BPE finds the optimal groupings of characters (bytes) for your particular set of documents and strings.\n",
       "If you want an *explainable* encoding, use the word tokenizers of the previous sections.\n",
       "If you want more flexible and accurate predictions and generation of text, then BPE, WPE, or SPE may be better for your application.\n",
       "Like the bias variance trade-off, there's often a explainability/accuracy trade-off in NLP.\n",
       "\n",
       "What about invisible or implied words?\n",
       "Can you think of additional words that are implied by the single-word command \"Don't!\"?\n",
       "If you can force yourself to think like a machine and then switch back to thinking like a human, you might realize that there are three invisible words in that command.\n",
       "The single statement \"Don't!\" means \"Don't you do that!\" or \"You, do not do that!\"\n",
       "That's at least three hidden packets of meaning for a total of five tokens you'd like your machine to know about.\n",
       "\n",
       "But don't worry about invisible words for now.\n",
       "All you need for this chapter is a tokenizer that can recognize words that are spelled out.\n",
       "You will worry about implied words and connotation and even meaning itself in chapter 4 and beyond.footnote:[If you want to learn more about exactly what a \"word\" really is, check out the introduction to _The Morphology of Chinese_ by Jerome Packard where he discusses the concept of a \"word\" in detail. The concept of a \"word\" did not exist at all in the Chinese language until the 20th century when it was translated from English grammar into Chinese.]\n",
       "\n",
       "Your NLP pipeline can start with one of these five options as your tokens:\n",
       "\n",
       "1. **Bytes** - ASCII characters\n",
       "2. **Characters** - multi-byte Unicode characters\n",
       "3. **Subwords** (Word pieces) - syllables and common character clusters\n",
       "4. **Words** - dictionary words or their roots (stems, lemmas)\n",
       "5. **Sentence pieces** - short, common word and multi-word pieces\n",
       "\n",
       "As you work your way down this list your vocabulary size increases and your NLP pipeline will need more and more data to train.\n",
       "Character-based NLP pipelines are often used in translation problems or NLG tasks that need to generalize from a modest number of examples.\n",
       "The number of possible words that your pipeline can deal with is called its _vocabulary_.\n",
       "A character-based NLP pipeline typically needs fewer than 200 possible tokens to process many Latin-based languages.\n",
       "That small vocabulary ensures that byte- and character-based NLP pipelines can handle new unseen test examples without too many meaningless OOV (out of vocabulary) tokens.\n",
       "\n",
       "For word-based NLP pipelines your pipeline will need to start paying attention to how often tokens are used before deciding whether to \"count it.\"\n",
       "You don't want you pipeline to do anything meaningful with junk words such `asdf` - the \n",
       "But even if you make sure your pipeline on pays attention to words that occur a lot, you could end up with a vocabulary that's as large as a typical dictionary - 20 to 50 thousand words.\n",
       "\n",
       "Subwords are the optimal token to use for most Deep Learning NLP pipelines.\n",
       "Subword (Word piece) tokenizers are built into many state of the art transformer pipelines.\n",
       "Words are the token of choice for any linguistics project or academic research where your results need to be interpretable and explainable.\n",
       "\n",
       "Sentence pieces take the subword algorithm to the extreme.\n",
       "The sentence piece tokenizer allows your algorithm to combine multiple word pieces together into a single token that can sometimes span multiple words.\n",
       "The only hard limit on sentence pieces is that they do not extend past the end of a sentence.\n",
       "This ensures that the meaning of a token is associated with only a single coherent thought and is useful on single sentences as well as longer documents.W\n",
       "\n",
       "==== _N_-grams\n",
       "\n",
       "No matter which kind of token you use for your pipeline, you will likely extract pairs, triplets, quadruplets, and even quintuplets of tokens.\n",
       "These are called _n_-grams_.footnote:[Pairs of adjacent words are called 2-grams or bigrams. Three words in sequency are called 3-grams or trigrams. Four words in a row are called 4-grams.  5-grams are probably the longest _n_-grams you'll find in an NLP pipeline. Google counts all the 1 to 5-grams in nearly all the books ever written (https://books.google.com/ngrams).]\n",
       "Using _n_-grams enables your machine to know about the token \"ice cream\" as well as the individual tokens \"ice\" and \"cream\" that make it up.\n",
       "Another 2-gram that you'd like to keep together is \"Mr. Smith\".\n",
       "Your tokens and your vector representation of a document will likely want to have a place for \"Mr. Smith\" along with \"Mr.\" and \"Smith.\"\n",
       "\n",
       "You will start with a short list of keywords as your vocabulary.\n",
       "This helps to keep your data structures small and understandable and can make it easier to explain your results.\n",
       "Explainable models create insights that you can use to help your stakeholders, hopefully the users themselves (rather than investors), accomplish their goals.\n",
       "\n",
       "For now, you can just keep track of all the short _n_-grams of words in your vocabulary.\n",
       "But in chapter 3, you will learn how to estimate the importance of words based on their document frequency, or how often they occur.\n",
       "That way you can filter out pairs and triplets of words that rarely occur together.\n",
       "You will find that the approaches we show are not perfect.\n",
       "Feature extraction can rarely retain all the information content of the input data in any machine learning pipeline.\n",
       "That is part of the art of NLP, learning when your tokenizer needs to be adjusted to extract more or different information from your text for your particular applications.\n",
       "\n",
       "In natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\n",
       "Nonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\n",
       "The techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email technology from a flood of spam that almost made it useless.\n",
       "\n",
       "== Challenges (a preview of stemming)\n",
       "\n",
       "As an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\n",
       "Very smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\n",
       "Imagine how difficult that is.\n",
       "Imagine trying to remove verb endings like \"ing\" from \"ending\" so you would have a stem called \"end\" to represent both words.\n",
       "And you would like to stem the word \"running\" to \"run,\" so those two words are treated the same.\n",
       "And that is tricky because you have removed not only the \"ing\" but also the extra \"n.\"\n",
       "But you want the word \"sing\" to stay whole.\n",
       "You would not want to remove the \"ing\" ending from \"sing\" or you would end up with a single-letter \"s.\"\n",
       "\n",
       "Or imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens.\"\n",
       "Do isolated individual letters in a word or parts of a word provide any information at all about that word's meaning?\n",
       "Can the letters be misleading?\n",
       "Yes and yes.\n",
       "\n",
       "In this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\n",
       "Later, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you are interested in.\n",
       "From that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\n",
       "\n",
       "=== Tokenization\n",
       "\n",
       "In NLP, _tokenization_ is a particular kind of document _segmentation_.\n",
       "Segmentation breaks up text into smaller chunks or segments.\n",
       "The segments of text have less information than the whole.\n",
       "Documents can be segmented into paragraphs, paragraphs into sentences, sentences into phrases, and phrases into tokens (usually words and punctuation).\n",
       "In this chapter, we focus on segmenting text into _tokens_ with a _tokenizer_.\n",
       "\n",
       "You may have heard of tokenizers before.\n",
       "If you took a computer science class you likely learned about how programming language compilers work.\n",
       "A tokenizer that is used to compile computer languages is called a _scanner_ or _lexer_.\n",
       "In some cases your computer language parser can work directly on the computer code and doesn't need a tokenizer at all.\n",
       "And for natural language processing, the only parser typically outputs a vector representation, rather than  If the tokenizer functionality is not separated from the compiler, the parser is often called a scannerless _parser_.\n",
       "\n",
       "The set of valid tokens for a particular computer language is called the _vocabulary_ for that language, or more formally its _lexicon_.\n",
       "Linguistics and NLP researchers use the term \"lexicon\" to refer to a set of natural language tokens.\n",
       "The term \"vocabulary\" is the more natural way to refer to a set of natural language words or tokens.\n",
       "So that's what you will use here.\n",
       "\n",
       "The natural language equivalent of a computer language compiler is a natural language parser.\n",
       "A natural language tokenizer is called a _scanner_, or _lexer_, or _lexical analyzer_ in the computer language world.\n",
       "Modern computer language compilers combine the _lexer_ and _parser_ into a single lexer-parser algorithm.\n",
       "The vocabulary of a computer language is usually called a _lexicon_.\n",
       "And computer language compilers sometimes refer to tokens as _symbols_.\n",
       "\n",
       "Here are five important NLP terms.\n",
       "Along side them are some roughly equivalent terms used in computer science when talking about programming language compilers:\n",
       "\n",
       "* _tokenizer_ -- scanner, lexer, lexical analyzer\n",
       "* _vocabulary_ -- lexicon\n",
       "* _parser_ -- compiler\n",
       "* _token_, _term_, _word_, or _n-gram_ -- token or symbol\n",
       "* _statement_ -- statement or expression\n",
       "\n",
       "Tokenization is the first step in an NLP pipeline, so it can have a big impact on the rest of your pipeline.\n",
       "A tokenizer breaks unstructured data, natural language text, into chunks of information which can be counted as discrete elements.\n",
       "These counts of token occurrences in a document can be used directly as a vector representing that document.\n",
       "This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning.\n",
       "These counts can be used directly by a computer to trigger useful actions and responses.\n",
       "Or they might also be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\n",
       "The most common use for bag-of-words vectors created this way is for document retrieval, or search.\n",
       "\n",
       "== Your tokenizer toolbox\n",
       "\n",
       "So each application you encounter you will want to think about which kind of tokenizer is appropriate for your application.\n",
       "And once you decide which kinds of tokens you want to try, you'll need to configure a python package for accomplishing that goal.\n",
       "\n",
       "You can chose from several tokenizer implementations: footnote:[Lysandre explains the various tokenizer options in the Huggingface documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\n",
       "\n",
       ". Python: `str.split`, `re.split`\n",
       ". NLTK: `PennTreebankTokenizer`, `TweetTokenizer`\n",
       ". spaCy: state of the art tokenization is its reason for being\n",
       ". Stanford CoreNLP: linguistically accurate, requires Java interpreter\n",
       ". Huggingface: `BertTokenizer`, a `WordPiece` tokenizer\n",
       "\n",
       "=== The simplest tokenizer\n",
       "\n",
       "The simplest way to tokenize a sentence is to use whitespace within a string as the \"delimiter\" of words. In Python, this can be accomplished with the standard library method `split`, which is available on all `str` object instances as well as on the `str` built-in class itself.\n",
       "\n",
       "Let's say your NLP pipeline needs to parse quotes from WikiQuote.org, and it's having trouble with one titled _The Book Thief_.footnote:[Markus Zusak, _The Book Thief_, p. 85 (https://en.wikiquote.org/wiki/The_Book_Thief)]\n",
       "\n",
       "\n",
       "[[book_thief_sentence_split_py]]\n",
       ".Example quote from _The Book Thief_ split into tokens\n",
       "[source,python]\n",
       "----\n",
       ">>> text = (\"Trust me, though, the words were on their way, and when \"\n",
       "...         \"they arrived, Liesel would hold them in her hands like \"\n",
       "...         \"the clouds, and she would wring them out, like the rain.\")\n",
       ">>> tokens = text.split()\n",
       ">>> tokens[:8]\n",
       "['Trust', 'me,', 'though,', 'the', 'words', 'were', 'on', 'their']\n",
       "----\n",
       "\n",
       "\n",
       ".Tokenized phrase\n",
       "image::../images/ch02/book-thief-split.png[alt=\"Trust|me,|though,|the|words|were|on|their\",align=\"center\",width=100%,link=\"../images/ch02/book-thief-split.png\"]\n",
       "\n",
       "As you can see, this built-in Python method does an OK job of tokenizing this sentence.\n",
       "Its only \"mistake\" is to include commas within the tokens.\n",
       "This would prevent your keyword detector from detecting quite a few important tokens: `['me', 'though', 'way', 'arrived', 'clouds', 'out', \"rain\"]`.\n",
       "Those words \"clouds\" and \"rain\" are pretty important to the meaning of this text.\n",
       "So you'll need to do a bit better with your tokenizer to ensure you can catch all the important words and \"hold\" them like Liesel.\n",
       "\n",
       "=== Rule-based tokenization\n",
       "\n",
       "It turns out there is a simple fix to the challenge of splitting punctuation from words.\n",
       "You can use a regular expression tokenizer to create rules to deal with common punctuation patterns.\n",
       "Here's just one particular regular expression you could use to deal with punctuation \"hanger-ons.\"\n",
       "And while we're at it, this regular expression will be smart about words that have internal punctuation, such as possessive words and contractions that contain apostrophes.\n",
       "\n",
       "You'll use a regular expression to tokenize some text from the book _Blindsight_ by Peter Watts.\n",
       "The text describes how the most _adequate_ humans tend to survive natural selection (and alien invasions).footnote:[Peter Watts, Blindsight, (https://rifters.com/real/Blindsight.htm)]\n",
       "The same goes for your tokenizer.\n",
       "You want to find an _adequate_ tokenizer that solves your problem, not the perfect tokenizer.\n",
       "You probably can't even guess what the _right_ or _fittest_ token is.\n",
       "You will need an accuracy number to evaluate your NLP pipeline with and that will tell you which tokenizer should survive your selection process.\n",
       "The example here should help you start to develop your intuition about applications for regular expression tokenizers.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import re\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
       ">>> texts = [text]\n",
       ">>> texts.append(\"There's no such thing as survival of the fittest. \"\n",
       "...              \"Survival of the most adequate, maybe.\")\n",
       ">>> tokens = list(re.findall(pattern, texts[-1]))\n",
       ">>> tokens[:8]\n",
       "[\"There's\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n",
       ">>> tokens[8:16]\n",
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n",
       ">>> tokens[16:]\n",
       "['maybe', '.']\n",
       "----\n",
       "<1> The _look-ahead_ pattern `(?:\\'\\w+)?` detects whether or not the word contains a single apostrophe followed by 1 or more letters.footnote:[Thank you Wiktor Stribiżew (https://stackoverflow.com/a/43094210/623735).]\n",
       "\n",
       "Much better.\n",
       "Now the tokenizer separates punctuation from the end of a word, but doesn't break up words that contain internal punctuation such as the apostrophe within the token \"There's.\"\n",
       "So all of these words were tokenized the way we wanted: \"There's\", \"fittest\", \"maybe\".\n",
       "And this regular expression tokenizer will work fine on contractions even if they have more than one letter after the apostrophe such as \"can't\", \"she'll\", \"what've\".\n",
       "It will work even typos such as 'can\"t' and \"she,ll\", and \"what`ve\".\n",
       "But this liberal matching of internal punctuation probably isn't what you want if your text contains rare double contractions such as \"couldn't've\", \"ya'll'll\", and \"y'ain't\"\n",
       "\n",
       "[TIP]\n",
       "=====\n",
       "Pro tip: You can accommodate double-contractions with the regular expression `r'\\w+(?:\\'\\w+){0,2}|[^\\w\\s]'`\n",
       "=====\n",
       "\n",
       "This is the main idea to keep in mind.\n",
       "No matter how carefully you craft your tokenizer, it will likely destroy some amount of information in your raw text.\n",
       "As you are cutting up text, you just want to make sure the information you leave on the cutting room floor isn't necessary for your pipeline to do a good job.\n",
       "Also, it helps to think about your downstream NLP algorithms.\n",
       "Later you may configure a case folding, stemming, lemmatizing, synonym substitution, or count vectorizing algorithm.\n",
       "When you do, you'll have to think about what your tokenizer is doing, so your whole pipeline works together to accomplish your desired output.\n",
       "\n",
       "\n",
       "////\n",
       "// too much regex detail?\n",
       "\n",
       "==== How regular expressions work\n",
       "\n",
       "Here is how the regular expression in <<listing_2_7>> works.\n",
       "\n",
       "The square brackets (`[` and `]`) are used to indicate a _character class_, a set of characters.\n",
       "The plus sign after the closing square bracket (`]`) means that a match must contain one or more of the characters inside the square brackets.\n",
       "The `\\s` within the character class is a shortcut to a predefined character class that includes all whitespace characters like those created when you press the `[space]`, `[tab]`, and `[return]` keys.\n",
       "The character class `r'[\\s]'` is equivalent to `r'[ \\t\\r\\n\\f]'`.\n",
       "The six whitespace characters are space (`' '`), tab (`'\\t'`), return (`'\\r'`), newline  (`'\\n'`), and form-feed (`'\\f'`).\n",
       "\n",
       "You did not use any character ranges here, but you may want to later.\n",
       "A character range is a special kind of character class indicated within square brackets and a hyphen like `r'[a-z]'` to match all lowercase letters.\n",
       "The character range `r'[0-9]'` matches any digit 0 through 9 and is equivalent to `r'[0123456789]'`).\n",
       "The regular expression `r'[\\_a-zA-Z]'` would match any underscore character (`r'\\_'`) or letter of the English alphabet (upper or lower case).\n",
       "\n",
       "The hyphen (`-`) right after the opening square bracket is a bit of quirk of regexes.\n",
       "You cannot put a hyphen just anywhere inside your square brackets because the regex parser may think you mean a character range like `r'[0-9]'`.\n",
       "So whenever you want to indicate an actual hyphen (dash) character in your character class, you need to make sure it is the first character after the open square bracket, or you need to escape it with a backslash (`\\`).\n",
       "\n",
       "The `re.split` function goes through each character in the input string (the second argument, `sentence`) left to right looking for any matches based on the \"program\" or \"pattern\" in the regular expression (the first argument, `r'[-\\s.,;!?]+'`).\n",
       "When it finds a match, it breaks the string right before that matched character and right after it, skipping over the matched character or characters.\n",
       "So the `re.split` line will work just like `str.split`, but it will work for any kind of character or multicharacter sequence that matches your regular expression.\n",
       "\n",
       "The parentheses (`(` and `)`) are used to group regular expressions just like they are used to group mathematical, Python, and most other programming language expressions.\n",
       "These parentheses force the regular expression to match the entire expression within the parentheses before moving on to try to match the characters that follow the parentheses.\n",
       "\n",
       "// TODO: TMI?\n",
       "////\n",
       "\n",
       "Take a look at the first few tokens in your lexographically sorted vocabulary for this short text:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import numpy as np  # <1>\n",
       ">>> vocab = sorted(set(tokens))  # <2>\n",
       ">>> ' '.join(vocab[:12])  # <3>\n",
       "\", . Survival There's adequate as fittest maybe most no of such\"\n",
       ">>> num_tokens = len(tokens)\n",
       ">>> num_tokens\n",
       "18\n",
       ">>> vocab_size = len(vocab)\n",
       ">>> vocab_size\n",
       "15\n",
       "----\n",
       "<1> `str.split()` is your quick-and-dirty tokenizer.\n",
       "<2> Coercing the `list` into a `set` ensures that your vocabulary contains only *unique* tokens that you want to keep track of.\n",
       "<3> Sorted lexographically (lexically) so punctuation comes before letters, and capital letters come before lowercase letters.\n",
       "\n",
       "You can see how you may want to consider lowercasing all your tokens so that \"Survival\" is recognized as the same word as \"survival\".\n",
       "And you may want to have a synonym substitution algorithm to replace \"There's\" with \"There is\" for similar reasons.\n",
       "However, this would only work if your tokenizer kept contraction and possessive apostrophes attached to their parent token.\n",
       "\n",
       "[TIP]\n",
       "=====\n",
       "Make sure you take a look at your vocabulary whenever it seems your pipeline isn't working well for a particular text.\n",
       "You may need to revise your tokenizer to make sure it can \"see\" all the tokens it needs to do well for your NLP task.\n",
       "=====\n",
       "\n",
       "\n",
       "=== SpaCy\n",
       "\n",
       "Maybe you don't want your regular expression tokenizer to keep contractions together.\n",
       "Perhaps you'd like to recognize the word \"isn't\" as two separate words, \"is\" and \"n't\".\n",
       "That way you could consolidate the synonyms \"n't\" and \"not\" into a single token.\n",
       "This way your NLP pipeline would understand \"the ice cream isn't bad\" to mean the same thing as \"the ice cream is not bad\".\n",
       "For some applications, such as full text search, intent recognition, and sentiment analysis, you want to be able to *uncontract* or expand contractions like this.\n",
       "By splitting contractions, you can use synonym substitution or contraction expansion to improve the recall of your search engine and the accuracy of your sentiment analysis.\n",
       "\n",
       "[IMPORTANT]\n",
       "====\n",
       "We'll discuss case folding, stemming, lemmatization, and synonym substitution later in this chapter.\n",
       "Be careful about using these techniques for applications such as authorship attribution, style transfer, or text fingerprinting.\n",
       "You want your authorship attribution or style-transfer pipeline to stay true to the author's writing style and the exact spelling of words that they use.\n",
       "====\n",
       "\n",
       "SpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.\n",
       "In fact the name \"spaCy\" is based on the word \"space\", as in the separator used in Western languages to separate words.\n",
       "And spaCy adds a lot of additional _tags_ to tokens at the same time that it is applying rules to split tokens apart.\n",
       "So spaCy is often the first and last tokenizer you'll ever need to use.\n",
       "\n",
       "Let's see how spaCy handles our collection of deep thinker quotes:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy  # <1>\n",
       ">>> nlp = spacy.load('en_core_web_sm')  # <2>\n",
       ">>> doc = nlp(texts[-1])\n",
       ">>> type(doc)\n",
       "<class 'spacy.tokens.doc.Doc'>\n",
       "\n",
       ">>> tokens = [tok.text for tok in doc]\n",
       ">>> tokens[:9]\n",
       "['There', \"'s\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n",
       "\n",
       ">>> tokens[9:17]\n",
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n",
       "----\n",
       "<1> If this is your first time to use spacy you should download the small language model with `spacy.cli.download('en_core_web_sm')`\n",
       "<2> `sm` stands for \"small\" (17 MB), `md` is medium (45 MB), `lg` is \"large\" (780 MB)\n",
       "\n",
       "That tokenization may be more useful to you if you're comparing your results to academic papers or colleagues at work.\n",
       "Spacy is doing a lot more under the hood.\n",
       "That small language model you downloaded is also identifying sentence breaks with some *sentence boundary detection* rules.\n",
       "A language model is a collection of regular expressions and finite state automata (rules).\n",
       "These rules are a lot like the grammar and spelling rules you learned in English class.\n",
       "They are used in the algorithms that tokenize and label your words with useful things like their part of speech and their position in a syntax tree of relationships between words.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from spacy import displacy\n",
       ">>> sentence = list(doc.sents)[0] # <1>\n",
       ">>> displacy.serve(sentence, style=\"dep\")\n",
       ">>> !firefox 127.0.0.1:5000\n",
       "\n",
       "----\n",
       "<1> The first sentence begins with \"There's no such thing...\"\n",
       "\n",
       "If you browse to your `localhost` on port 5000 you should see a sentence diagram that may be even more correct than what you could produce in school:\n",
       "\n",
       "image::../images/ch02/there-such-thing.png[alt=\"NOUN Survival -> ADV maybe. ADJ adequate -> ADV most\",align=\"center\",width=100%,link=\"../images/ch02/there-such-thing.png\"]\n",
       "\n",
       "You can see that spaCy does a lot more than simply separate text into tokens.\n",
       "It identifies sentence boundaries to automatically segment your text into sentences.\n",
       "And it tags tokens with various attributes like their part of speech (PoS) and even their role within the syntax of a sentence.\n",
       "You can see the lemmas displayed by `displacy`  beneath the literal text for each token.footnote:[nlpia2 source code for chapter 2 (https://proai.org/nlpia2-ch2) has additional spaCy and displacy options and examples.]\n",
       "Later in the chapter we'll explain how lemmatization and case folding and other vocabulary *compression* approaches can be helpful for some applications.\n",
       "\n",
       "So spaCy seems pretty great in terms of accuracy and some \"batteries included\" features, such as all those token tags for lemmas and dependencies.\n",
       "What about speed?\n",
       "\n",
       "=== Tokenizer race\n",
       "\n",
       "SpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.\n",
       "First download the AsciiDoc text file for this chapter:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import requests\n",
       ">>> text = requests.get('https://proai.org/nlpia2-ch2.adoc').text\n",
       ">>> f'{round(len(text) / 10_000)}0k'\n",
       "'160k'\n",
       "----\n",
       "<1> I divided by 10,000 and rounded it, so that Doctests would continue to pass as I revise this text.\n",
       "\n",
       "There were about 160 thousand ASCII characters in this AsciiDoc file where I wrote this sentence that you are reading right now.\n",
       "What does that mean in terms of words-per-second, the standard benchmark for tokenizer speed?\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load('en_core_web_sm')\n",
       ">>> %timeit nlp(text)  # <1>\n",
       "4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
       "\n",
       ">>> f'{round(len(text) / 10_000)}0k'\n",
       "'160k'\n",
       ">>> doc = nlp(text)\n",
       ">>> f'{round(len(list(doc)) / 10_000)}0k'\n",
       "'30k'\n",
       ">>> f'{round(len(doc) / 1_000 / 4.67)}kWPS'  # <2> \n",
       "'7kWPS'\n",
       "----\n",
       "<1> `%timeit` is a magic function within `jupyter notebook`, `jupyter console` or `ipython`\n",
       "<2> kWPS is for thousands of words (tokens) per second\n",
       "\n",
       "That's nearly 5 seconds for about 150,000 characters or 34,000 words of English and Python text or about 7000 words per second.\n",
       "\n",
       "That may seem fast enough for you on your personal projects.\n",
       "But on a medical records summarization project we needed to process thousands of large documents with a comparable amount of text as you find in this entire book.\n",
       "And the latency in our medical record summarization pipeline was a critical metric for the project.\n",
       "So this, full-featured spaCy pipeline would require at least 5 days to process 10,000 books such as NLPIA or typical medical records for 10,000 patients.\n",
       "\n",
       "If that's not fast enough for your application you can disable any of the tagging features of the spaCy pipeline that you do not need.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> nlp.pipe_names  # <1>\n",
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
       ">>> nlp = spacy.load('en_core_web_sm', disable=nlp.pipe_names)\n",
       ">>> %timeit nlp(text)\n",
       "199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
       "----\n",
       "<1> The `pipe_names` lists all the currently enabled elements of your spaCy `nlp` pipeline\n",
       "\n",
       "You can disable the pipeline elements you don't need to speed up the tokenizer:\n",
       "\n",
       "- `tok2vec`: word embeddings\n",
       "- `tagger`: part-of-speech (`.pos` and `.pos_`)\n",
       "- `parser`: syntax tree role\n",
       "- `attribute_ruler`: fine-grained POS and other tags\n",
       "- `lemmatizer`: lemma tagger\n",
       "- `ner`: named entity recognition tagger\n",
       "\n",
       "NLTK's `word_tokenize` method is often used as the pace setter in tokenizer benchmark speed comparisons:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import nltk\n",
       ">>> nltk.download('punkt')\n",
       "True\n",
       ">>> from nltk.tokenize import word_tokenize\n",
       ">>> %timeit word_tokenize(text)\n",
       "156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
       ">>> tokens = word_tokenize(text)\n",
       ">>> f'{round(len(tokens) / 10_000)}0k'\n",
       "'30k'\n",
       "----\n",
       "\n",
       "Could it be that you found a winner for the tokenizer race?\n",
       "Not so fast.\n",
       "Your regular expression tokenizer has some pretty simple rules, so it should run pretty fast as well:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n",
       ">>> tokens = re.findall(pattern, text)  # <1>\n",
       ">>> f'{round(len(tokens) / 10_000)}0k'\n",
       "'30k'\n",
       ">>> %timeit re.findall(pattern, text)\n",
       "8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
       "----\n",
       "<1> Try precompiling with `re.compile` to learn something about how smart the core Python developers are\n",
       "\n",
       "Now that's not surprising.\n",
       "Regular expressions can be compiled and run very efficiently within low level C routines in Python.\n",
       "\n",
       "[TIP]\n",
       "================\n",
       "Use a regular expression tokenizer when speed is more import than accuracy.\n",
       "If you do not need the additional linguistic tags that spaCy and other pipelines provide your tokenizer doesn't need to waste time trying to figure out those tags.footnote:[Andrew Long, \"Benchmarking Python NLP Tokenizers\" (https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5)]\n",
       "And each time you use a regular expression in the `re` or `regex` packages, a compiled and optimized version of it is cached in RAM.\n",
       "So there's usually no need to _precompile_ (using `re.compile()`) your regexes.\n",
       "================\n",
       "\n",
       "== Wordpiece tokenizers\n",
       "\n",
       "It probably felt natural to think of words as indivisible atomic chunks of meaning and thought.\n",
       "However, you did find some words that didn't clearly split on spaces or punctuation.\n",
       "And many compound words or named entities that you'd like to keep together have spaces within them.\n",
       "So it can help to dig a little deeper and think about the statistics of what makes a word.\n",
       "Think about how we can build up words from neighboring characters instead of cleaving text at separators such as spaces and punctuation.\n",
       "\n",
       "=== Clumping characters into sentence pieces\n",
       "\n",
       "Instead of thinking about breaking strings up into tokens, your tokenizer can look for characters that are used a lot right next to each other, such as \"i\" before \"e\".\n",
       "You can pair up characters and sequences of characters that belong together.footnote:[In many applications the term \"_n_-gram\" refers to character _n_-grams rather than word n-grams. For example the leading relational database PostgreSQL has a Trigram index which tokenizes your text into character 3-grams not word 3-grams. In this book, we use \"_n_-gram\" to refer to sequences of word grams and \"character _n_-grams\" when talking about sequences of characters.]\n",
       "These clumps of characters can become your tokens.\n",
       "An NLP pipeline only pays attention to the statistics of tokens.\n",
       "And hopefully these statistics will line up with our expectations for what a word is.\n",
       "\n",
       "Many of these character sequences will be whole words, or even compound words, but many will be pieces of words.\n",
       "In fact, all _subword tokenizers_ maintain a token within the vocabulary for every individual character in your vocabulary.\n",
       "This means it never needs to use an OOV (Out-of-Vocabulary) token, as long as any new text doesn't contain any new characters it hasn't seen before.\n",
       "Subword tokenizers attempt to optimally clump characters together to create tokens.\n",
       "Using the statistics of character n-gram counts it's possible for these algorithms to identify wordpieces and even sentence pieces that make good tokens.\n",
       "\n",
       "It may seem odd to identify words by clumping characters.\n",
       "But to a machine, the only obvious, consistent division between elements of meaning in a text is the boundary between bytes or characters.\n",
       "And the frequency with which characters are used together can help the machine identify the meaning associated with subword tokens such as individual syllables or parts of compound words.\n",
       "\n",
       "In English, even individual letters have subtle emotion (sentiment) and meaning (semantics) associated with them.\n",
       "However, there are only 26 unique letters in the English language.\n",
       "That doesn't leave room for individual letters to _specialize_ on any one topic or emotion.\n",
       "Nonetheless savvy marketers know that some letters are cooler than others.\n",
       "Brands will try to portray themselves as technologically advanced by choosing names with exotic letters like \"Q\" and \"X\" or \"Z\".\n",
       "This also helps with SEO (Search Engine Optimization) because rarer letters are more easily found among the sea of possible company and product names.\n",
       "Your NLP pipeline will pick up all these hints of meaning, connotation, and intent.\n",
       "Your token counters will provide the machine with the statistics it needs to infer the meaning of clumps of letters that are used together often.\n",
       "\n",
       "The only disadvantage for subword tokenizers is the fact that they must pass through your corpus of text many times before converging on an optimal vocabulary and tokenizer.\n",
       "A subword tokenizer has to be trained or fit to your text just like a CountVectorizer.\n",
       "In fact you'll use a CountVectorizer in the next section to see how subword tokenizers work.\n",
       "\n",
       "There are two main approaches to subword tokenization: BPE (Byte-Pair Encoding) and Wordpiece tokenization.\n",
       "\n",
       "==== BPE\n",
       "\n",
       "In the previous edition of the book we insisted that words were the smallest unit of meaning in English that you need consider.\n",
       "With the rise of Transformers and other deep learning models that use BPE and similar techniques, we've changed our minds.footnote:[Hannes and Cole are probably screaming \"We told you so!\" as they read this.]\n",
       "Character-based subword tokenizers have proven to be more versatile and robust for most NLP problems.\n",
       "By building up a vocabulary from building blocks of Unicode multi-byte characters you can construct a vocabulary that can handle every possible natural language string you'll ever see, all with a vocabulary of as few as 50,000 tokens.\n",
       "\n",
       "You may think that Unicode characters are the smallest packet of meaning in natural language text.\n",
       "To a human, maybe, but to a machine, no way.\n",
       "Just as the BPE name suggests, characters don't have to be your fundamental atom of meaning for your _base vocabulary_.\n",
       "You can split characters into 8-bit bytes.\n",
       "GPT-2 uses a byte-level BPE tokenizer to naturally compose all the unicode characters you need from the bytes that make them up.\n",
       "Though some special rules are required to handle unicode punctuation within a byte-based vocabulary, no other adjustment to the character-based BPE algorithm is required.\n",
       "A byte-level BPE tokenizer allows you to represent all possible texts with a base (minimum) vocabulary size of 256 tokens.\n",
       "The GPT-2 model can achieve state-of-the-art performance with it's default BPE vocabulary of only 50,000 multibyte _merge tokens_ plus 256 individual byte tokens.\n",
       "\n",
       "You can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker or the hub in a social network.\n",
       "It connects characters together that appear next to each other a lot.\n",
       "It then creates a new token for these character combinations.\n",
       "And it keeps doing this until it has a many frequently used character sequences as you've allowed in your vocabulary size limit.\n",
       "\n",
       "\n",
       "BPE is transforming the way we think about natural language tokens.\n",
       "NLP engineers are finally letting the data do the talking.\n",
       "Statistical thinking is better than human intuition when building an NLP pipeline.\n",
       "A machine can see how _most_ people use language.\n",
       "You are only familiar with what _you_ mean when you use particular words or syllables.\n",
       "Transformers have now surpassed human readers and writers at some natural language understanding and generation tasks, including finding meaning in subword tokens.\n",
       "\n",
       "One complication you have not yet encounter is the dilemma of what to do when you encounter a new word.\n",
       "In the previous examples, we just keep adding new words to our vocabulary.\n",
       "But in the real world your pipeline will have been trained on an initial corpus of documents that may or may not represent all the kinds of tokens it will ever see.\n",
       "If your initial corpus is missing some of the words that you encounter later on, you will not have a slot in your vocabulary to put your counts of that new word.\n",
       "So when you train you initial pipeline, you will always reserve a slot (dimension) to hold the counts of your _out-of-vocabulary_ (OOV) tokens.\n",
       "So if your original set of documents did not contain the girl's name \"Aphra\", all counts of the name Aphra would be lumped into the OOV dimension as counts of Amandine and other rare words.\n",
       "\n",
       "To give Aphra equal representation in your vector space, you can use BPE.\n",
       "BPE breaks down rare words into smaller pieces to create a _periodic table_ of the elements for natural language in your corpus.\n",
       "So, because \"aphr\" is a common english prefix, your BPE tokenizer would probably give Aphra *two* slots for her counts in your vocabulary: one for \"aphr\" and one for \"a\".\n",
       "Actually, you might actually discover that the vobcabulary slots are for \" aphr\" and \"a \", because BPE keeps track of spaces no differently than any other character in your alphabet.footnote:[Actually, the string representation of tokens used for BPE and Wordpiece tokenizer place marker characters at the beginning or end of the token string indicate the absence of a word boundary (typically a space or punctuation). So you may see the \"aphr##\" token in your BPE vocabulary for the prefix \"aphr\" in aphrodesiac (https://stackoverflow.com/a/55416944/623735)]\n",
       "\n",
       "BPE gives you multilingual flexibility to deal with Hebrew names like Aphra.\n",
       "And it give your pipeline robustness against common misspellings and typos, such as \"aphradesiac.\"\n",
       "Every word, including minority 2-grams such as \"African American\", have representation in the voting system of BPE.footnote:[Discriminatory voting restriction laws have recently been passed in US: (https://proai.org/apnews-wisconsin-restricts-blacks)]\n",
       "Gone are the days of using the kluge of OOV (Out-of-Vocabulary) tokens to handle the rare quirks of human communication.\n",
       "Because of this, state of the art deep learning NLP pipelines such as transformers all use word piece tokenization similar to BPE.footnote:[See chapter 12 for information about another similar tokenizer -- sentence piece tokenizer]\n",
       "\n",
       "BPE preserves some of the meaning of new words by using character tokens and word-piece tokens to spell out any unknown words or parts of words.\n",
       "For example, if \"syzygy\" is not in our vocabulary, we could represent it as the six tokens \"s\", \"y\", \"z\", \"y\", \"g\", and \"y\".\n",
       "Perhaps \"smartz\" could be represented as the two tokens \"smart\" and \"z\".\n",
       "\n",
       "That sounds smart.\n",
       "Let's see how it works on our text corpus:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
       ">>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n",
       ">>> vectorizer.fit(texts)\n",
       "CountVectorizer(analyzer='char', ngram_range=(1, 2))\n",
       "\n",
       ">>> bpevocab = vectorizer.get_feature_names()\n",
       ">>> bpevocab[:7]\n",
       "[' ', ' a', ' c', ' f', ' h', ' i', ' l']\n",
       "----\n",
       "\n",
       "We configured the `CountVectorizer` to split the text into all the possible character 1-grams and 2-grams found in the texts.\n",
       "And `CountVectorizer` organizes the vocabulary in lexical order, so n-grams that start with a space character (`' '`) come first.\n",
       "Once the vectorizer knows what tokens it needs to be able to count, it can transform text strings into vectors, with one dimension for every token in your character n-gram vocabulary.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> vectors = vectorizer.transform(texts)\n",
       ">>> df = pd.DataFrame(vectors.todense(), columns=bpevocab)\n",
       ">>> df.index = [t[:8] + '...' for t in texts]\n",
       ">>> df = df.T\n",
       ">>> df['total'] = df.T.sum()\n",
       ">>> df\n",
       "    Trust me...  There's ...  total\n",
       "             31           14     45\n",
       " a            3            2      5\n",
       " c            1            0      1\n",
       " f            0            1      1\n",
       " h            3            0      3\n",
       "..          ...          ...    ...\n",
       "wr            1            0      1\n",
       "y             2            1      3\n",
       "y             1            0      1\n",
       "y,            1            0      1\n",
       "yb            0            1      1\n",
       "<BLANKLINE>\n",
       "[148 rows x 3 columns]\n",
       "----\n",
       "\n",
       "The DataFrame contains a column for each sentence and a row for each character 2-gram.\n",
       "Check out the top four rows where the byte pair (character 2-gram) of \" a\" is seen to occur five times in these two sentences.\n",
       "So even spaces count as \"characters\" when you're building a BPE tokenizer.\n",
       "This is one of the advantages of BPE, it will figure out what your token delimiters are, so it will work even in languages where there is no whitespace between words.\n",
       "And BPE will work on substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13 characters forward.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> df.sort_values('total').tail()\n",
       "        Trust me...  There's ...  total\n",
       "    he           10            3     13\n",
       "    h            14            5     19\n",
       "    t            11            9     20\n",
       "    e            18            8     26\n",
       "                 31           14     45\n",
       "----\n",
       "\n",
       "A BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent vocabulary.\n",
       "Over time it deletes the less frequent character pairs as it gets less and less likely that they won't come up a lot more later in your text.\n",
       "\n",
       "----\n",
       ">>> df['n'] = [len(tok) for tok in bpevocab]\n",
       ">>> df[df['n'] > 1].sort_values('total').tail()\n",
       "    Trust me...  There's ...  total  n\n",
       ",             6            1      7  2\n",
       "e             7            2      9  2\n",
       " t            8            3     11  2\n",
       "th            8            4     12  2\n",
       "he           10            3     13  2\n",
       "----\n",
       "\n",
       "So the next round of preprocessing in the BPE tokenizer would retain the character 2-grams \"he\" and \"th\" and even \" t\" and \"e \".\n",
       "Then the BPE algorithm would make another pass through the text with this smaller character bigram vocabulary.\n",
       "It would look for frequent pairings of these character bigrams with each other and individual characters.\n",
       "This process would continue until the maximum number of tokens is reached and the longest possible character sequences have been incorporated into the vocabulary.\n",
       "\n",
       "[NOTE]\n",
       "====\n",
       "You may see mention of _wordpiece_ tokenizers which are used within some advanced language models such as `BERT` and its derivatives.footnote:[Lysandre Debut explains all the variations on subword tokenizers in the Hugging Face transformers documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\n",
       "It works the same as BPE, but it actually uses the underlying language model to predict the neighboring characters in string.\n",
       "It eliminates the characters from its vocabulary that hurt the accuracy of this language model the least.\n",
       "The math is subtly different and it produces subtly different token vocabularies, but you don't need to select this tokenizer intentionally.\n",
       "The models that use it will come with it built into their pipelines.\n",
       "====\n",
       "\n",
       "One big challenge of BPE-based tokenizers is that they must be trained on your individual corpus.\n",
       "So BPE tokenizers are usually only used for Transformers and Large Language Models (LLMs) which you will learn about in chapter 9.\n",
       "\n",
       "Another challenge of BPE tokenizers is all the book keeping you need to do to keep track of which trained tokenizer goes with each of your trained models.\n",
       "This was one of the big innovations of Huggingface.\n",
       "They made it easy to store and share all the preprocessing data, such as the tokenizer vocabulary, along side the language model.\n",
       "This makes it easier to reuse and share BPE tokenizers. \n",
       "If you want to become an NLP expert, you may want to imitate what they've done at HuggingFace with your own NLP preprocessing pipelines.footnote:[Huggingface documentation on tokenizers (https://huggingface.co/docs/transformers/tokenizer_summary)]\n",
       "\n",
       "== Vectors of tokens\n",
       "\n",
       "Now that you have broken your text into tokens of meaning, what do you do with them?\n",
       "How can you convert them to numbers that will be meaningful to the machine?\n",
       "The simplest most basic thing to do would be to detect whether a particular token you are interested in was present or not.\n",
       "You could hard-code the logic to check for important tokens, called a _keywords_.\n",
       "\n",
       "This might work well for your greeting intent recognizer in chapter 1.\n",
       "Our greeting intent recognizer at the end of chapter 1 looked for words like \"Hi\" and \"Hello\" at the beginning of a text string.\n",
       "Your new tokenized text would help you detect the presence or absence of words such as \"Hi\" and \"Hello\" without getting confused by words like \"Hiking\" and \"Hell.\"\n",
       "With your new tokenizer in place, your NLP pipeline wouldn't misinterpret the word \"Hiking\" as the greeting \"Hi king\":\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> hi_text = 'Hiking home now'\n",
       ">>> hi_text.startswith('Hi')\n",
       "True\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
       ">>> 'Hi' in re.findall(pattern, hi_text)  # <2>\n",
       "False\n",
       ">>> 'Hi' == re.findall(pattern, hi_text)[0]  # <3>\n",
       "False\n",
       "----\n",
       "<1> You can reuse the regular expression pattern from earlier to create a one-line tokenizer\n",
       "<2> 'Hi' is not among the 3 words (tokens) in this phrase\n",
       "<3> 'Hi' is definitely not the first word in this phrase\n",
       "\n",
       "So tokenization can help you reduce the number of false positives in your simple intent recognition pipeline that looks for the presence of greeting words.\n",
       "This is often called keyword detection, because your vocabulary of words is limited to a set of words you think are important.\n",
       "However, it's quite cumbersome to have to think of all the words that might appear in a greeting in order to recognize them all, including slang, misspellngs and typoos.\n",
       "And creating a for loop to iterate through them all would be inefficient.\n",
       "We can use the math of linear algebra and the vectorized operations of `numpy` to speed this process up.\n",
       "\n",
       "In order to detect tokens efficiently you will want to use three new tricks:\n",
       "\n",
       ". matrix and vector representations of documents\n",
       ". vectorized operations in numpy\n",
       ". indexing of discrete vectors\n",
       "\n",
       "You'll first learn the most basic, direct, raw and lossless way to represent words as a matrix, one-hot encoding.\n",
       "\n",
       "=== One-hot Vectors\n",
       "\n",
       "Now that you've successfully split your document into the kinds of words you want, you're ready to create vectors out of them.\n",
       "Vectors of numbers are what we need to do the math or processing of NL*P* on natural language text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> onehot_vectors = np.zeros(\n",
       "...     (len(tokens), vocab_size), int)  # <2>\n",
       ">>> for i, word in enumerate(tokens):\n",
       "...     onehot_vectors[i, vocab.index(word)] = 1  # <3>\n",
       ">>> df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\n",
       ">>> df_onehot.shape\n",
       "(18, 15)\n",
       ">>> df_onehot.iloc[:,:8].replace(0, '')  # <4>\n",
       "    ,  .  Survival  There's  adequate  as  fittest  maybe\n",
       "0                       1\n",
       "1\n",
       "2\n",
       "3\n",
       "4                                   1\n",
       "5\n",
       "6\n",
       "7\n",
       "8                                           1\n",
       "9      1\n",
       "10              1\n",
       "11\n",
       "12\n",
       "13\n",
       "14                               1\n",
       "15  1\n",
       "16                                                1\n",
       "17     1\n",
       "----\n",
       "<2> The table is as wide as your count of unique vocabulary terms and as tall as the length of your document: 18 rows, 15 columns\n",
       "<3> For each token in the sentence, mark the column for it with a `1`.\n",
       "<4> For brevity we're only showing the first 8 columns of the DataFrame and replaced 0's with ''.\n",
       "\n",
       "In this representation of this two-sentence quote, each row is a vector representation of a single word from the text.\n",
       "The table has the 15 columns because this is the number of unique words in your vocabulary.\n",
       "The table has 18 rows, one for each word in the document.\n",
       "A \"1\" in a column indicates a vocabulary word that was present at that position in the document.\n",
       "\n",
       "You can \"read\" a one-hot encoded (vectorized) text from top to bottom.\n",
       "You can tell that the first word in the text was the word \"There's\", because the `1` on the first row is positioned under the column label \"There's\".\n",
       "The next three rows (row indexes 1, 2, and 3) are blank, because we've truncated the table on the right to help it fit on the page.\n",
       "The fifth row of the text, with the 0-offset index number of `4` shows us that the fifth word in the text was the word \"adequate\", because there's a `1` in that column.\n",
       "\n",
       "One-hot vectors are super-sparse, containing only one nonzero value in each row vector.\n",
       "For display, this code replaces the `0`'s with empty strings (`''`), to make it easier to read.\n",
       "But the code did not actually alter the `DataFrame` of data you are processing in your NLP pipeline.\n",
       "The Python code above was just to to make it easier to read, so you can see that it looks a bit like a player piano paper roll, or maybe a music box drum.\n",
       "\n",
       "The Pandas `DataFrame` made this output a little easier to read and interpret.\n",
       "The `DataFrame.columns` keep track of labels for each column.\n",
       "This allows you to label each column in your table with a string, such as the token or word it represents.\n",
       "A `DataFrame` can also keep track of labels for each row in an the `DataFrame.index`, for speedy lookup.\n",
       "\n",
       "[IMPORTANT]\n",
       "====\n",
       "Don't add strings to any `DataFrame` you intend to use in your machine learning pipeline.\n",
       "The purpose of a tokenizer and vectorizer, like this one-hot vectorizer, is to create a numerical array that your NLP pipeline can do math on.\n",
       "You can't do math on strings.\n",
       "====\n",
       "\n",
       "Each row of the table is a binary row vector, and you can see why it's also called a one-hot vector: all but one of the positions (columns) in a row are `0` or blank.\n",
       "Only one column, or position in the vector is \"hot\" (\"1\").\n",
       "A one (`1`) means on, or hot.\n",
       "A zero (`0`) mean off, or absent.\n",
       "\n",
       "One nice feature of this vector representation of words and tabular representation of documents is that no information is lost.\n",
       "The exact sequence of tokens is encoded in the order of the one-hot vectors in the table representing a document.\n",
       "As long as you keep track of which words are indicated by which column, you can reconstruct the original sequence of tokens from this table of one-hot vectors perfectly.\n",
       "And this reconstruction process is 100% accurate even though your tokenizer was only 90% accurate at generating the tokens you thought would be useful.\n",
       "As a result, one-hot word vectors like this are typically used in neural nets, sequence-to-sequence language models, and generative language models.\n",
       "They are a good choice for any model or NLP pipeline that needs to retain all the meaning inherent in the original text.\n",
       "\n",
       "[TIP]\n",
       "====\n",
       "The one-hot encoder (vectorizer) did not discard any information from the text, but our tokenizer did.\n",
       "Our regular expression tokenizer discarded the whitespace characters (`\\s`) that sometimes occur between words.\n",
       "So you could not perfectly reconstruct the original text with a _detokenizer_.\n",
       "Tokenizers like spaCy, however, keep track of these whitespace characters and can in fact detokenize a sequence of tokens perfectly.\n",
       "SpaCy was named for this feature of accurately accounting for white-*space* efficiently and accurately.\n",
       "====\n",
       "\n",
       "This sequence of one-hot vectors is like a digital recording of the original text.\n",
       "If you squint hard enough you might be able to imagine that the matrix of ones and zeros above is a player piano roll.footnote:[See the \"Player piano\" article on Wikipedia (https://en.wikipedia.org/wiki/Player_piano).].\n",
       "Or maybe it's the bumps on the metal drum of a music box.footnote:[See the web page titled \"Music box - Wikipedia\" (https://en.wikipedia.org/wiki/Music_box).]\n",
       "The vocabulary key at the top tells the machine which \"note\" or word to play for each row in the sequence of words or piano music.\n",
       "\n",
       "[[player_piano_roll_jpg]]\n",
       ".Player piano roll\n",
       "image::../images/ch02/piano_roll.jpg[Player piano music roll with parallel sequences of holes running vertically down the paper. The holes meander left and right to indicate the rising and falling of the tones in the melody of a song. Image licensed from Wikimedia CC BY-SA 3.0 (https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg),width=100%,align=\"center\",link=\"https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg\"]\n",
       "\n",
       "Unlike a player-piano or a music box, your mechanical word recorder and player is only allowed to use one \"finger\" at a time.\n",
       "It can only play one \"note\" or word at a time.\n",
       "It's one-hot.\n",
       "And there is no variation in the spacing of the words.\n",
       "\n",
       "The important thing is that you've turned a sentence of natural language words into a sequence of numbers, or vectors.\n",
       "Now you can have the computer read and do math on the vectors just like any other vector or list of numbers.\n",
       "This allows your vectors to be input into any natural language processing pipeline that requires this kind of vector.\n",
       "The Deep Learning pipelines of chapter 5 through 10 typically require this representation, because they can be designed to extract \"features\" of meaning from these raw representations of text.\n",
       "And Deep Learning pipelines can generate text from numerical representations of meaning.\n",
       "So the stream of words emanating from your NLG pipelines in later chapters will often be represented by streams of one-hot encoded vectors, just like a player piano might play a song for a less artificial audience in West World.footnote:[West World is a television series about particularly malevolent humans and human-like robots, including one that plays a piano in the main bar.]\n",
       "\n",
       "Now all you need to do is figure out how to build a \"player piano\" that can _understand_ and combine those word vectors in new ways.\n",
       "Ultimately, you'd like your chatbot or NLP pipeline to play us a song, or say something, you haven't heard before.\n",
       "You'll get to do that in chapters 9 and 10 when you learn about recurrent neural networks that are effective for sequences of one-hot encoded tokens like this.\n",
       "\n",
       "This representation of a sentence in one-hot word vectors retains all the detail, grammar, and order of the original sentence.\n",
       "And you have successfully turned words into numbers that a computer can \"understand.\"\n",
       "They are also a particular kind of number that computers like a lot: binary numbers.\n",
       "But this is a big table for a short sentence.\n",
       "If you think about it, you have expanded the file size that would be required to store your document.\n",
       "For a long document this might not be practical.\n",
       "\n",
       "How big is this *lossless* numerical representation of your collection of documents?\n",
       "Your vocabulary size (the length of the vectors) would get huge.\n",
       "The English language contains at least 20,000 common words, millions if you include names and other proper nouns.\n",
       "And your one-hot vector representation requires a new table (matrix) for every document you want to process.\n",
       "This is almost like a raw \"image\" of your document.\n",
       "If you have done any image processing, you know that you need to do dimension reduction if you want to extract useful information from the data.\n",
       "\n",
       "Let's run through the math to give you an appreciation for just how big and unwieldy these \"piano rolls\" are.\n",
       "In most cases, the vocabulary of tokens you'll use in an NLP pipeline will be much more than 10,000 or 20,000 tokens.\n",
       "Sometimes it can be hundreds of thousands or even millions of tokens.\n",
       "Let's assume you have a million tokens in your NLP pipeline vocabulary.\n",
       "And let's say you have a meager 3000 books with 3500 sentences each and 15 words per sentence -- reasonable averages for short books.\n",
       "That's a whole lot of big tables (matrices), one for each book.\n",
       "That would use 157.5 terabytes.\n",
       "You probably couldn't even store that on disk.\n",
       "\n",
       "That is more than a million million bytes, even if you are super-efficient and use only one byte for each number in your matrix.\n",
       "At one byte per cell, you would need nearly 20 terabytes of storage for a small bookshelf of books processed this way.\n",
       "Fortunately you do not ever use this data structure for storing documents.\n",
       "You only use it temporarily, in RAM, while you are processing documents one word at a time.\n",
       "\n",
       "So storing all those zeros, and recording the order of the words in all your documents does not make much sense.\n",
       "It is not practical.\n",
       "And it's not very useful.\n",
       "Your data structure hasn't abstracted or generalized from the natural language text.\n",
       "An NLP pipeline like this doesn't yet do any real feature extraction or dimension reduction to help your machine learning work well in the real world.\n",
       "\n",
       "What you really want to do is compress the meaning of a document down to its essence.\n",
       "You would like to compress your document down to a single vector rather than a big table.\n",
       "And you are willing to give up perfect \"recall.\"\n",
       "You just want to capture most of the meaning (information) in a document, not all of it.\n",
       "\n",
       "=== BOW (Bag-of-Words) Vectors\n",
       "\n",
       "Is there any way to squeeze all those _player piano music rolls_ into a single vector?\n",
       "Vectors are a great way to represent any object.\n",
       "With vectors we could compare documents to each other just be checking the Euclidian distance between them.\n",
       "Vectors allow us to use all your linear algebra tools on natural language.\n",
       "And that's really the goal of NLP, doing math on text.\n",
       "\n",
       "Let us assume you can ignore the order of the words in our texts.\n",
       "For this first cut at a vector representation of text you can just jumble them all up together into a \"bag,\" one bag for each sentence or short document.\n",
       "It turns out just knowing what words are present in a document can give your NLU pipeline a lot of information about what's in it.\n",
       "This is in fact the representation that power big Internet search engine companies.\n",
       "Even for documents several pages long, a bag-of-words vector is useful for summarizing the essence of a document.\n",
       "\n",
       "Let's see what happens when we jumble and count the words in our text from _The Book Thief_:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> bow = sorted(set(re.findall(pattern, text)))\n",
       ">>> bow[:9]\n",
       "[',', '.', 'Liesel', 'Trust', 'and', 'arrived', 'clouds', 'hands', 'her']\n",
       ">>> bow[9:19]\n",
       "['hold', 'in', 'like', 'me', 'on', 'out', 'rain', 'she', 'the', 'their']\n",
       ">>> bow[19:27]\n",
       "['them', 'they', 'though', 'way', 'were', 'when', 'words', 'would']\n",
       "----\n",
       "\n",
       "Even with this jumbled up bag of words, you can get a general sense that this sentence is about:  \"Trust\", \"words\", \"clouds\", \"rain\", and someone named \"Liesel\".\n",
       "One thing you might notice is that Python's `sorted()` puts punctuation before characters, and capitalized words before lowercase words.\n",
       "This is the ordering of characters in the ASCII and Unicode character sets.\n",
       "However, the order of your vocabulary is unimportant.\n",
       "As long as you are consistent across all the documents you tokenize this way, a machine learning pipeline will work equally well with any vocabulary order.\n",
       "\n",
       "You can use this new bag-of-words vector approach to compress the information content for each document into a data structure that is easier to work with.\n",
       "For keyword search, you could **OR** your one-hot word vectors from the player piano roll representation into a binary bag-of-words vector.\n",
       "In the play piano analogy this is like playing several notes of a melody all at once, to create a \"chord\".\n",
       "Rather than \"replaying\" them one at a time in your NLU pipeline, you would create a single bag-of-words vector for each document.\n",
       "\n",
       "You could use this single vector to represent the whole document in a single vector.\n",
       "Because vectors all need to be the same length, your BOW vector would need to be as long your vocabulary size which is the number of unique tokens in your documents.\n",
       "And you could ignore a lot of words that would not be interesting as search terms or keywords.\n",
       "This is why stop words are often ignored when doing BOW tokenization.\n",
       "This is an extremely efficient representation for a search engine index or the first filter for an information retrieval system.\n",
       "Search indexes only need to know the presence or absence of each word in each document to help you find those documents later.\n",
       "\n",
       "This approach turns out to be critical to helping a machine \"understand\" a collection of words as a single mathematical object.\n",
       "And if you limit your tokens to the 10,000 most important words, you can compress your numerical representation of your imaginary 3500 sentence book down to 10 kilobytes, or about 30 megabytes for your imaginary 3000-book corpus.\n",
       "One-hot vector sequences for such a modest-sized corpus would require hundreds of gigabytes.\n",
       "\n",
       "Another advantage of the BOW representation of text is that it allows you to find similar documents in your corpus in constant time (`O(1)`).\n",
       "You can't get any faster than this.\n",
       "BOW vectors are the precursor to a reverse index which is what makes this speed possible.\n",
       "In computer science and software engineering, you are always on the lookout for data structures that enable this kind of speed.\n",
       "All major full text search tools use BOW vectors to find what you're looking for fast.\n",
       "You can see this numerical representation of natural language in EllasticSearch, Solr,footnote:[Apache Solr home page and Java source code (https://solr.apache.org/)] PostgreSQL, and even state of the art web search engines such as Qwant,footnote:[Qwant web search engine based in Europe (https://www.qwant.com/)], SearX,footnote:[SearX git repository (https://github.com/searx/searx) and web search (https://searx.thegpm.org/)], and Wolfram Alpha footnote:[(https://www.wolframalpha.com/)].\n",
       "\n",
       "Fortunately, the words in your vocabulary are sparsely utilized in any given text.\n",
       "And for most bag-of-words applications, we keep the documents short, sometimes just a sentence will do.\n",
       "So rather than hitting all the notes on a piano at once, your bag-of-words vector is more like a broad and pleasant piano chord, a combination of notes (words) that work well together and contain meaning.\n",
       "Your NLG pipeline or chatbot can handle these chords even if there is a lot of \"dissonance\" from words in the same statement that are not normally used together.\n",
       "Even dissonance (odd word usage) is useful information about a statement that a machine learning pipeline can make use of.\n",
       "\n",
       "Here is how you can put the tokens into a binary vector indicating the presence or absence of a particular word in a particular sentence.\n",
       "This vector representation of a set of sentences could be \"indexed\" to indicate which words were used in which document.\n",
       "This index is equivalent to the index you find at the end of many textbooks, except that instead of keeping track of which page a word occurs on, you can keep track of the sentence (or the associated vector) where it occurred.\n",
       "Whereas a textbook index generally only cares about important words relevant to the subject of the book, you keep track of every single word (at least for now).\n",
       "\n",
       "==== Sparse representations\n",
       "\n",
       "You might be thinking that if you process a huge corpus you'll probably end up with thousands or even millions of unique tokens in your vocabulary.\n",
       "This would mean you would have to store a lot of zeros in your vector representation of our 20-token sentence about Liesel.\n",
       "A `dict` would use much less memory than a vector.\n",
       "Any paired mapping of words to their 0/1 values would be more efficient than a vector.\n",
       "But you can't do math on `dict`'s.\n",
       "So this is why CountVectorizer uses a sparse numpy array to hold the counts of words in a word fequency vector.\n",
       "Using a dictionary or sparse array for your vector ensures that it only has to store a 1 when any one of the millions of possible words in your dictionary appear in a particular document.\n",
       "\n",
       "But if you want to look at an individual vector to make sure everything is working correctly, a Pandas `Series` is the way to go.\n",
       "And you will wrap that up in a Pandas DataFrame so you can add more sentences to your binary vector \"corpus\" of quotes.\n",
       "\n",
       "=== Dot product\n",
       "\n",
       "// TODO: some of this may belong in the discussion of keyword matching and one-hot vectors?\n",
       "You'll use the dot product a lot in NLP, so make sure you understand what it is.\n",
       "Skip this section if you can already do dot products in your head.\n",
       "\n",
       "The dot product is also called the _inner product_ because the \"inner\" dimension of the two vectors (the number of elements in each vector) or matrices (the rows of the first matrix and the columns of the second matrix) must be the same because that is where the products happen.\n",
       "This is analogous to an \"inner join\" on two relational database tables.\n",
       "\n",
       "The dot product is also called the _scalar product_ because it produces a single scalar value as its output.\n",
       "This helps distinguish it from the _cross product_, which produces a vector as its output.\n",
       "Obviously, these names reflect the shape of the symbols used to indicate the dot product (latexmath:[\\cdot]) and cross product (latexmath:[\\times]) in formal mathematical notation.\n",
       "The scalar value output by the scalar product can be calculated by multiplying all the elements of one vector by all the elements of a second vector and then adding up those normal multiplication products.\n",
       "\n",
       "Here is a Python snippet you can run in your Pythonic head to make sure you understand what a dot product is:\n",
       "\n",
       "[[example_dot_product_code]]\n",
       ".Example dot product calculation\n",
       "[source,python]\n",
       "----\n",
       ">>> v1 = pd.np.array([1, 2, 3])\n",
       ">>> v2 = pd.np.array([2, 3, 4])\n",
       ">>> v1.dot(v2)\n",
       "20\n",
       ">>> (v1 * v2).sum()  # <1>\n",
       "20\n",
       ">>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # <2>\n",
       "20\n",
       "----\n",
       "<1> Multiplication of NumPy arrays is a \"vectorized\" operation that is very efficient.\n",
       "<2> You should not iterate through vectors this way unless you want to slow down your pipeline.\n",
       "\n",
       "[TIP]\n",
       "================\n",
       "The dot product is equivalent to the _matrix product_, which can be accomplished in NumPy with the `np.matmul()` function or the `@` operator. Since all vectors can be turned into Nx1 or 1xN matrices, you can use this shorthand operator on two column vectors (Nx1) by transposing the first one so their inner dimensions line up, like this: `v1.reshape((-1, 1)).T @ v2.reshape((-1, 1))`, which outputs your scalar product within a 1x1 matrix: `array([[20]])`\n",
       "================\n",
       "\n",
       "// IDEA: Consider talking about BOW overlap to explain cosine similarity\n",
       "\n",
       "This is your first vector space model of natural language documents (sentences).\n",
       "Not only are dot products possible, but other vector operations are defined for these bag-of-word vectors: addition, subtraction, OR, AND, and so on.\n",
       "You can even compute things such as Euclidean distance or the angle between these vectors. This representation of a document as a binary vector has a lot of power.\n",
       "It was a mainstay for document retrieval and search for many years.\n",
       "All modern CPUs have hardwired memory addressing instructions that can efficiently hash, index, and search a large set of binary vectors like this.\n",
       "Though these instructions were built for another purpose (indexing memory locations to retrieve data from RAM), they are equally efficient at binary vector operations for search and retrieval of text.\n",
       "\n",
       "\n",
       "\n",
       "NLTK and Stanford CoreNLP have been around the longest and are the most widely used for comparison of NLP algorithms in academic papers.\n",
       "Even though the Stanford CoreNLP has a Python API, it relies on the Java 8 CoreNLP backend, which must be installed and configured separately.\n",
       "So if you want to publish the results of your work in an academic paper and compare it to what other researchers are doing, you may need to use NLTK.\n",
       "The most common tokenizer used in academia is the PennTreebank tokenizer:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.tokenize import TreebankWordTokenizer\n",
       ">>> texts.append(\n",
       "...   \"If conscience and empathy were impediments to the advancement of \"\n",
       "...   \"self-interest, then we would have evolved to be amoral sociopaths.\"\n",
       "...   )  # <1>\n",
       ">>> tokenizer = TreebankWordTokenizer()\n",
       ">>> tokens = tokenizer.tokenize(texts[-1])[:6]\n",
       ">>> tokens[:8]\n",
       "['If', 'conscience', 'and', 'empathy', 'were', 'impediments', 'to', 'the']\n",
       ">>> tokens[8:16]\n",
       "['advancement', 'of', 'self-interest', ',', 'then', 'we', 'would', 'have']\n",
       ">>> tokens[16:]\n",
       "['evolved', 'to', 'be', 'amoral', 'sociopaths', '.']\n",
       "----\n",
       "<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield in _SuperCooperators_: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\n",
       "\n",
       "// IDEA: Diagram of Nowak quote with vertical bars breaking up sent into words\n",
       "\n",
       "The spaCy Python library contains a natural language processing pipeline that includes a tokenizer.\n",
       "In fact, the name of the package comes from the words \"space\" and \"Cython\".\n",
       "SpaCy was built using the Cython package to speed the tokenization of text, often using the *space* character (\" \") as the delimmiter.\n",
       "SpaCy has become the *multitool* of NLP, because of its versatility and the elegance of its API.\n",
       "To use spaCy, you can start by creating an callable parser object, typically named `nlp`.\n",
       "You can customize your NLP pipeline by modifying the Pipeline elements within that parser object.\n",
       "\n",
       "And spaCy has \"batteries included.\"\n",
       "So even with the default smallest spaCy language model loaded, you can do tokenization and sentence segementation, plus *part-of-speech* and *abstract-syntax-tree* tagging -- all with a single function call.\n",
       "When you call `nlp()` on a string, spaCy tokenizes the text and returns a `Doc` (document) object.\n",
       "A `Doc` object is a container for the sequence of sentences and tokens that it found in the text.\n",
       "\n",
       "\n",
       "// IDEA: example spacy code for tokenization\n",
       "\n",
       "The spaCy package tags each token with their linguistic function to provide you with information about the text's grammatical structure.\n",
       "Each token object within a `Doc` object has attributes that provide these tags.\n",
       "\n",
       "For example:\n",
       "* `token.text` the original text of the word\n",
       "* `token.pos_` grammatical part of speech tag as a human-readable string\n",
       "* `token.pos`  integer for the grammar part of speech tag\n",
       "* `token.dep_` indicates the tokens role in the syntactic dependency tree\n",
       "* `token.dep`  integer corresponding to the syntactic dependency tree location\n",
       "\n",
       "The `.text` attribute provides the original text for the token.\n",
       "This is what is provided when you request the __str__ representation of a token.\n",
       "A spaCy `Doc` object is allowing you to detokenize a document object to recreate the entire input text. i.e., the relation between tokens\n",
       "You can use these functions to examine the text in more depth.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
       ">>> text = \"Nice guys finish first.\"  # <1>\n",
       ">>> doc = nlp(text)\n",
       ">>> for token in doc:\n",
       ">>>     print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\n",
       "Nice            ADJ       amod\n",
       "guys            NOUN      nsubj\n",
       "finish          VERB      ROOT\n",
       "first           ADV       advmod\n",
       ".               PUNCT     punct\n",
       "----\n",
       "<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\n",
       "\n",
       "== Challenging tokens\n",
       "\n",
       "Chinese, Japanese, and other pictograph languages aren't limited to a small small number letters in alphabets used to compose tokens or words.\n",
       "Characters in these traditional languages look more like drawings and are called \"pictographs.\"\n",
       "There are many thousands of unique characters in the Chinese and Japanese languages.\n",
       "And these characters are used much like we use words in alphabet-based languages such as English.\n",
       "But each Chinese character is usually not a complete word on its own.\n",
       "A character's meaning depends on the characters to either side.\n",
       "And words are not delimited with spaces.\n",
       "This makes it challenging to tokenize Chinese text into words or other packets of thought and meaning. \n",
       "\n",
       "The `jieba` package is a Python package you can use to segment traditional Chinese text into words.\n",
       "It supports three segmentation modes: 1) \"full mode\" for retrieving all possible words from a sentence, 2) \"accurate mode\" for cutting the sentence into the most accurate segments, 3) \"search engine mode\" for splitting long words into shorter ones, sort-of like splitting compound words or finding the roots of words in English.\n",
       "In the example below, the Chinese sentence \"西安是一座举世闻名的文化古城\" translates into \"Xi'an is a city famous world-wide for it's ancient culture.\"\n",
       "Or, a more compact and literal translation might be \"Xi'an is a world-famous city for her ancient culture.\"\n",
       "\n",
       "From a grammatical perspective, you can split the sentence into: 西安 (Xi'an), 是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective suffix), 文化 (culture), 古城 (ancient city).\n",
       "The character \"座\" is the quantifier meaning \"ancient\" that is normally used to modify the word \"city.\"\n",
       "The `accurate mode` in `jieba` causes it to segment the sentence this way so that you can correctly extract a precise interpretation of the text.\n",
       "\n",
       ".Jieba in accurate mode\n",
       "[source,python]\n",
       "----\n",
       ">>> seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\") # <1>\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> the default mode for jieba is accurate or precise mode\n",
       "\n",
       "Jieba's accurate mode minimizes the total number of tokens or words.\n",
       "This gave you 7 tokens for this short\n",
       "Jieba attempts to keep as many possible characters together.\n",
       "This will reduce the false positive rate or type 1 errors for detecting boundaries between words.\n",
       "\n",
       "In full mode, jieba will attempt to split the text into smaller words, and more of them.\n",
       "\n",
       ".Jieba in full mode\n",
       "[source,python]\n",
       "----\n",
       ">>> import jieba\n",
       "... seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # <1>\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世', '举世闻名', '闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> `cut_all==True` means \"full mode\"\n",
       "\n",
       "Now you can try search engine mode to see if it's possible to break up these tokens even further:\n",
       "\n",
       ".Jieba in search engine mode\n",
       "[source,python]\n",
       "----\n",
       ">>> seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世', '闻名', '举世闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> Accurate mode is the default mode.\n",
       "\n",
       "Unfortunately later versions of Python (3.5+) aren't supported by Jieba's part-of-speech tagging model.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import jieba\n",
       ">>> from jieba import posseg\n",
       ">>> words = posseg.cut(\"西安是一座举世闻名的文化古城\")\n",
       ">>> jieba.enable_paddle()  # <1>\n",
       ">>> words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\n",
       ">>> list(words)\n",
       "[pair('西安', 'ns'),\n",
       " pair('是', 'v'),\n",
       " pair('一座', 'm'),\n",
       " pair('举世闻名', 'i'),\n",
       " pair('的', 'uj'),\n",
       " pair('文化', 'n'),\n",
       " pair('古城', 'ns')]\n",
       "----\n",
       "<1> Activate paddle mode\n",
       "\n",
       "You can find more information about jieba at (https://github.com/fxsjy/jieba).\n",
       "SpaCy also contains Chinese language models that do a decent job of segmenting and tagging Chinese text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> spacy.cli.download(\"zh_core_web_sm\")  # <1>\n",
       ">>> nlpzh = spacy.load(\"zh_core_web_sm\")\n",
       ">>> doc = nlpzh(\"西安是一座举世闻名的文化古城\")\n",
       ">>> [(tok.text, tok.pos_) for tok in doc]\n",
       "[('西安', 'PROPN'),\n",
       " ('是', 'VERB'),\n",
       " ('一', 'NUM'),\n",
       " ('座', 'NUM'),\n",
       " ('举世闻名', 'VERB'),\n",
       " ('的', 'PART'),\n",
       " ('文化', 'NOUN'),\n",
       " ('古城', 'NOUN')]\n",
       "----\n",
       "<1> Only need download the Chinese (zh) language model if this is your first time processing Chinese text \n",
       "\n",
       "As you may notice, spaCy provides slightly different tokenization and tagging, which is more attached to the original meaning of each word rather than the context of this sentence.\n",
       "\n",
       "=== A complicated picture\n",
       "\n",
       "Unlike English, there is no concept of stemming or lemmatization in pictographic languages such as Chinese and Japanese (Kanji).\n",
       "However, there’s a related concept.\n",
       "The most essential building blocks of Chinese characters are called _radicals_.\n",
       "To better understand _radicals_, you must first see how Chinese characters are constructed.\n",
       "There are six types of Chinese characters: 1) pictographs, 2) pictophonetic characters, 3) associative compounds, 4) self-explanatory characters, 5) phonetic loan characters, and 6) mutually explanatory characters.\n",
       "The top four categories are the most important and encompass most Chinese characters.\n",
       "\n",
       "1. Pictographs (象形字)\n",
       "2. Pictophonetic characters (形声字)\n",
       "3. Associative compounds (会意字)\n",
       "\n",
       "==== 1. Pictographs (象形字)\n",
       "\n",
       "_Pictographs_ were created from images of real objects, such as the characters for 口 (mouth) and 门 (door).\n",
       "\n",
       "\n",
       "==== 2. Pictophonetic characters (形声字)\n",
       "\n",
       "_Pictophonetic characters_ were created from a radical and a single Chinese character.\n",
       "One part represents its meaning and the other indicates its pronunciation.\n",
       "For example, 妈 (mā, mother) = 女 (female) + 马 (mǎ, horse).\n",
       "Squeezing 女 into 马 gives 妈.\n",
       "The character 女 is the semantic radical that indicates the meaning of the character (female).\n",
       "马 is a single character that has a similar pronunciation (mǎ).\n",
       "You can see that the character for mother (妈) is a combination of the characters for female an\n",
       "This is comparable to the English concept of homophones -- words that sound alike but mean completely different things.\n",
       "But in Chinese use additional characters to disambiguate homophones.\n",
       "The character for female\n",
       "\n",
       "==== 3. Associative compounds (会意字)\n",
       "\n",
       "Associative compounds can be divided into two parts: one symbolizes the image, the other indicates the meaning.\n",
       "\n",
       "For example, 旦 (dawn), the upper part (日) is the sun and the lower part (一) is like the horizon line.\n",
       "\n",
       "\n",
       "==== Self-explanatory characters (指事字)\n",
       "\n",
       "\n",
       "Self-explanatory characters cannot be easily represented by an image, so they are shown by a single abstract symbol.\n",
       "For example, 上 (up), 下 (down).\n",
       "\n",
       "As you can see, procedures like stemming and lemmatization are harder or impossible for many Chinese characters.\n",
       "Separating the parts of a character may radically ;) change its meaning.\n",
       "And there's not prescribed order or rule for combining radicals to create Chinese characters.\n",
       "\n",
       "Nonetheless, some kinds of stemming are harder in English than they are in Chinese\n",
       "For example, automatically removing the pluralization from words like \"we\", \"us\", \"they\" and \"them\" is hard in English but straightforward in Chinese.\n",
       "Chinese uses inflection to construct the plural form of characters, similar to adding s to the end of English words.\n",
       "In Chinese the pluralization suffix character is 们.\n",
       "The character 朋友 (friend) becomes 朋友们 (friends).\n",
       "\n",
       "Even the characters for \"we/us\", \"they/them\", and \"y'all\" use the same pluralization suffix: 我们 (we/us), 他们 (they/them), 你们 (you).\n",
       "But in in English, you can remove the 'ing' or 'ed' from many verbs to get the root word.\n",
       "However, in Chinese, verb conjugation uses an additional character in the front or the end to indicate tense.\n",
       "There's no prescribed rule for verb conjugation.\n",
       "For example, examine the character 学 (learn), 在学 (learning), and 学过 (learned).\n",
       "In Chinese, you can also use a suffix 学 to denote an academic discipline, such as 心理学 (psychology) or 社会学 (sociology).\n",
       "In most cases, you want to keep the integrated Chinese character together rather than reducing it to its components.\n",
       "\n",
       "It turns out this is a good rule of thumb for all languages.\n",
       "Let the data do the talking.\n",
       "Do not stem or lemmatize unless the statistics indicate that it will help your NLP pipeline perform better.\n",
       "Is there not a small amount of meaning that is lost when \"smarter\" and \"smartest\" reduce to \"smart\"?\n",
       "Make sure stemming does not leave your NLP pipeline dumb.\n",
       "\n",
       "Let the statistics of how of how characters and words are used together help you decide how, or if, to decompose any particular word or n-gram.\n",
       "In the next chapter we'll show you some tools like Scikit-Learn's `TfidfVectorizer` that handle all the tedious account required to get this right.\n",
       "\n",
       "\n",
       "==== Contractions\n",
       "\n",
       "// TODO: clean this up\n",
       "You might be wondering why you would want to split the contraction `wasn't` into `was` and `n't`.\n",
       "For some applications, like grammar-based NLP models that use syntax trees, it is important to separate the words `was` and `not` to allow the syntax tree parser to have a consistent, predictable set of tokens with known grammar rules as its input.\n",
       "There are a variety of standard and nonstandard ways to contract words, by reducing contractions to their constituent words, a dependency tree parser or syntax parser only need to be programmed to anticipate the various spellings of individual words rather than all possible contractions.\n",
       "\n",
       "\n",
       "[TIP]\n",
       ".Tokenize informal text from social networks such as Twitter and Facebook\n",
       "====\n",
       "The NLTK library includes a rule-based tokenizer to deal with short, informal, emoji-laced texts from social networks: `casual_tokenize`\n",
       "\n",
       "It handles emojis, emoticons, and usernames.\n",
       "The `reduce_len` option deletes less meaningful character repetitions.\n",
       "The `reduce_len` algorithm retains three repetitions, to approximate the intent and sentiment of the original text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.tokenize.casual import casual_tokenize\n",
       ">>> texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\n",
       ">>> casual_tokenize(texts[-1], reduce_len=True)\n",
       "['@rickrau', 'mind', 'BLOOOWWWN', 'by', 'latest', 'lex', ':*)', '!', '!', '!']\n",
       "----\n",
       "\n",
       "====\n",
       "\n",
       "=== Extending your vocabulary with _n_-grams\n",
       "\n",
       "Let's revisit that \"ice cream\" problem from the beginning of the chapter.\n",
       "Remember we talked about trying to keep \"ice\" and \"cream\" together.\n",
       "\n",
       "____\n",
       "I scream, you scream, we all scream for ice cream.\n",
       "____\n",
       "\n",
       "But I do not know many people that scream for \"cream\".\n",
       "And nobody screams for \"ice\", unless they're about to slip and fall on it.\n",
       "So you need a way for your word-vectors to keep \"ice\" and \"cream\" together.\n",
       "\n",
       "==== We all gram for _n_-grams\n",
       "\n",
       "An _n_-gram is a sequence containing up to _n_ elements that have been extracted from a sequence of those elements, usually a string.\n",
       "In general the \"elements\" of an _n_-gram can be characters, syllables, words, or even symbols like \"A\", \"D\", and \"G\" used to represent the chemical amino acid markers in a DNA or RNA sequence.footnote:[Linguistic and NLP techniques are often used to glean information from DNA and RNA, this site provides a list of amino acid symbols that can help you translate amino acid language into a human-readable language: \"Amino Acid - Wikipedia\" (https://en.wikipedia.org/wiki/Amino_acid#Table_of_standard_amino_acid_abbreviations_and_properties).]\n",
       "\n",
       "In this book, we're only interested in _n_-grams of words, not characters.footnote:[You may have learned about trigram indexes in your database class or the documentation for PostgreSQL (`postgres`). But these are triplets of characters. They help you quickly retrieve fuzzy matches for strings in a massive database of strings using the `%` and `~*` SQL full text search queries.]\n",
       "So in this book, when we say 2-gram, we mean a pair of words, like \"ice cream\".\n",
       "When we say 3-gram, we mean a triplet of words like \"beyond the pale\" or \"Johann Sebastian Bach\" or \"riddle me this\".\n",
       "_n_-grams do not have to mean something special together, like compound words.\n",
       "They have to be frequent enough together to catch the attention of your token counters.\n",
       "\n",
       "Why bother with _n_-grams?\n",
       "As you saw earlier, when a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of those words.\n",
       "By extending your concept of a token to include multiword tokens, _n_-grams, your NLP pipeline can retain much of the meaning inherent in the order of words in your statements.\n",
       "For example, the meaning-inverting word \"not\" will remain attached to its neighboring words, where it belongs.\n",
       "Without _n_-gram tokenization, it would be free floating.\n",
       "Its meaning would be associated with the entire sentence or document rather than its neighboring words.\n",
       "The 2-gram \"was not\" retains much more of the meaning of the individual words \"not\" and \"was\" than those 1-grams alone in a bag-of-words vector.\n",
       "A bit of the context of a word is retained when you tie it to its neighbor(s) in your pipeline.\n",
       "\n",
       "In the next chapter, we show you how to recognize which of these _n_-grams contain the most information relative to the others, which you can use to reduce the number of tokens (_n_-grams) your NLP pipeline has to keep track of.\n",
       "Otherwise it would have to store and maintain a list of every single word sequence it came across.\n",
       "This prioritization of _n_-grams will help it recognize \"Three Body Problem\" and \"ice cream\", without paying particular attention to \"three bodies\" or \"ice shattered\".\n",
       "In chapter 4, we associate word pairs, and even longer sequences, with their actual meaning, independent of the meaning of their individual words.\n",
       "But for now, you need your tokenizer to generate these sequences, these _n_-grams.\n",
       "\n",
       "==== Stop words\n",
       "\n",
       "Stop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase.\n",
       "Examples of some common stop words include footnote:[A more comprehensive list of stop words for various languages can be found in NLTK's corpora (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip).]\n",
       "\n",
       "* a, an\n",
       "* the, this\n",
       "* and, or\n",
       "* of, on\n",
       "\n",
       "Historically stop words have been excluded from NLP pipelines in order to reduce the computational effort to extract information from a text.\n",
       "Even though the words themselves carry little information, the stop words can provide important relational information as part of an _n_-gram.\n",
       "Consider these two examples:\n",
       "\n",
       "* `Mark reported to the CEO`\n",
       "* `Suzanne reported as the CEO to the board`\n",
       "\n",
       "In your NLP pipeline, you might create 4-grams such as `reported to the CEO` and `reported as the CEO`.\n",
       "If you remove the stop words from the 4-grams, both examples would be reduced to `reported CEO`, and you would lack the information about the professional hierarchy.\n",
       "In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board.\n",
       "Unfortunately, retaining the stop words within your pipeline creates another problem: It increases the length of the _n_-grams required to make use of these connections formed by the otherwise meaningless stop words.\n",
       "This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\n",
       "\n",
       "Designing a filter for stop words depends on your particular application.\n",
       "Vocabulary size will drive the computational complexity and memory requirements of all subsequent steps in the NLP pipeline.\n",
       "But stop words are only a small portion of your total vocabulary size.\n",
       "A typical stop word list has only 100 or so frequent and unimportant words listed in it.\n",
       "But a vocabulary size of 20,000 words would be required to keep track of 95% of the words seen in a large corpus of tweets, blog posts, and news articles.footnote:[See the web page titled \"Analysis of text data and Natural Language Processing\" (http://rstudio-pubs-static.s3.amazonaws.com/41251_4c55dff8747c4850a7fb26fb9a969c8f.html).]\n",
       "And that is just for 1-grams or single-word tokens.\n",
       "A 2-gram vocabulary designed to catch 95% of the 2-grams in a large English corpus will generally have more than 1 million unique 2-gram tokens in it.\n",
       "\n",
       "You may be worried that vocabulary size drives the required size of any training set you must acquire to avoid overfitting to any particular word or combination of words.\n",
       "And you know that the size of your training set drives the amount of processing required to process it all.\n",
       "However, getting rid of 100 stop words out of 20,000 is not going to significantly speed up your work.\n",
       "And for a 2-gram vocabulary, the savings you would achieve by removing stop words is minuscule.\n",
       "In addition, for 2-grams you lose a lot more information when you get rid of stop words arbitrarily, without checking for the frequency of the 2-grams that use those stop words in your text.\n",
       "For example, you might miss mentions of \"The Shining\" as a unique title and instead treat texts about that violent, disturbing movie the same as you treat documents that mention \"Shining Light\" or \"shoe shining\".\n",
       "\n",
       "So if you have sufficient memory and processing bandwidth to run all the NLP steps in your pipeline on the larger vocabulary, you probably do not want to worry about ignoring a few unimportant words here and there.\n",
       "And if you are worried about overfitting a small training set with a large vocabulary, there are better ways to select your vocabulary or reduce your dimensionality than ignoring stop words.\n",
       "Including stop words in your vocabulary allows the document frequency filters (discussed in chapter 3) to more accurately identify and ignore the words and _n_-grams with the least information content within your particular domain.\n",
       "\n",
       "The SpaCy and NLTK packages include a variety of predefined sets of stop words for various use cases. footnote:[The spaCy package contains a list of stopwords that you can modify using this Stack Overflow answer (https://stackoverflow.com/a/51627002/623735)]\n",
       "You probably won't need a broad list of stopwords like the one we created for listing <<listing-broad-stop-words>>, but if you do you'll want to check out the SpaCy and NLTK stopwords lists.\n",
       "And if you need an even broader set of stopwords you can `SearX`\n",
       "footnote:[If you want to help others find SearX you can get in the habbit of saying \"SearX\" (pronounced \"see Ricks\") when talking or writing about doing a web search. You can shift the meaning of words in your world to make it a better place!]\n",
       "footnote:[The NLTK package (https://pypi.org/project/nltk) contains the list of stopwords you'll see in most tutorials]  for SEO companies that maintain lists of stopwords in many languages.\n",
       "\n",
       "If your NLP pipeline relies on a fine-tuned list of stop words to achieve high accuracy, it can be a significant maintenance headache.\n",
       "Humans and machines (search engines) are constantly changing which words they ignore.\n",
       " footnote:[Damien Doyle maintains a list of search engine stopwords ranked by popularity and categorized by language (https://www.ranks.nl/stopwords)]\n",
       "// HL: to Karen, Matko & Ivan: how can I use references (anchors) to refer to the correct listing number?\n",
       "Listing <<listing-broad-stop-words>> uses an exhaustive list from all these lists so you can get a feel for the amount of meaning that can be lost if your hand-crafted list of stop words isn't well crafted and kept up to date.\n",
       "\n",
       "////\n",
       "\n",
       "HL to Ivan and Matko:\n",
       "\n",
       "These are the things I tried based on the examples in Manning's ADOC documentation:\n",
       "\n",
       "[[listing-broad-stop-words]] -> \"Listing 2.6\" (capital L)\n",
       "\n",
       "[#listing-broad-stop-words, reftext={chapter}.{counter:listing}] -> \"2.\" (without the counter:listing integer in my Browser plugin)\n",
       "\n",
       "////\n",
       "[#listing-broad-stop-words, reftext={chapter}.{counter:listing}]\n",
       ".Broad list of stop words\n",
       "[source,python]\n",
       "----\n",
       ">>> import requests\n",
       ">>> url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\n",
       "...        \"src/nlpia/data/stopword_lists.json\")\n",
       ">>> response = requests.get(url)\n",
       ">>> stop_words = response.json()['exhaustive']  # <1>\n",
       ">>> tokens = 'the words were just as I remembered them'.split()  # <2>\n",
       ">>> tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
       ">>> print(tokens_without_stopwords)\n",
       "['I', 'remembered']\n",
       "----\n",
       "<1> This exhaustive list of stop words was compiled from various public search engine optimization lists as well as NLP toolkits like spaCy and NLTK.\n",
       "\n",
       "You can see that some words carry more meaning than others.\n",
       "This is a sentence from a short story by Ted Chiang about machines helping us remember our statements so we don't have to rely on flawed memories.footnote:[from Ted Chiang, _Exhalation_, \"Truth of Fact, Truth of Fiction\"]\n",
       "In this phrase you lost two thirds of the words and still retained the bulk of the phrase's meaning.\n",
       "However you can see that an import token \"words\" was discarded by this particular stop words list.\n",
       "You can often get your point across without articles, prepositions, or even forms of the verb \"to be\".\n",
       "Imagine someone doing sign language or in a hurry to write a note to themselves.\n",
       "Which words would they choose to always skip? That is how stop words are chosen.\n",
       "\n",
       "Here's another common stop words list that isn't quite as exhaustive:\n",
       "\n",
       "[[nltk_stop_words_code]]\n",
       ".NLTK list of stop words\n",
       "[source,python]\n",
       "----\n",
       ">>> import nltk\n",
       ">>> nltk.download('stopwords')\n",
       ">>> stop_words = nltk.corpus.stopwords.words('english')\n",
       ">>> len(stop_words)\n",
       "179\n",
       ">>> stop_words[:7]\n",
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
       ">>> [sw for sw in stopwords if len(sw) == 1]\n",
       "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
       "----\n",
       "\n",
       "A document that dwells on the first person is pretty boring, and more importantly for you, has low information content.\n",
       "The NLTK package includes pronouns (not just first person ones) in its list of stop words.\n",
       "And these one-letter stop words are even more curious, but they make sense if you have used the NLTK tokenizer and Porter stemmer a lot.\n",
       "These single-letter tokens pop up a lot when contractions are split and stemmed using NLTK tokenizers and stemmers.\n",
       "\n",
       "[WARNING]\n",
       "====\n",
       "The set of English stop words in `sklearn`, `spacy`, `nltk`, and SEO tools are very different, and they are constantly evolving.\n",
       "At the time of this writing, `sklearn` has 318 stop words, NLTK has 179 stop words, spaCy has 326, and our 'exhaustive' SEO list includes 667 stop words.\n",
       "\n",
       "This is a good reason to consider *not* filtering stop words.\n",
       "If you do, others may not be able to reproduce your results.\n",
       "====\n",
       "\n",
       "Depending on how much natural language information you want to discard ;), you can take the union or the intersection of multiple stop word lists for your pipeline.\n",
       "Here are some stop_words lists we found, though we rarely use any of them in production:\n",
       "\n",
       "[[collection_of_stop_words_lists_code]]\n",
       ".Collection of stop words lists\n",
       "[source,python]\n",
       "----\n",
       ">>> resp = requests.get(url)\n",
       ">>> len(resp.json()['exhaustive'])\n",
       "667\n",
       ">>> len(resp.json()['sklearn'])\n",
       "318\n",
       ">>> len(resp.json()['spacy'])\n",
       "326\n",
       ">>> len(resp.json()['nltk'])\n",
       "179\n",
       ">>> len(resp.json()['reuters'])\n",
       "28\n",
       "----\n",
       "\n",
       "=== Normalizing your vocabulary\n",
       "\n",
       "So you have seen how important vocabulary size is to the performance of an NLP pipeline. Another vocabulary reduction technique is to normalize your vocabulary so that tokens that mean similar things are combined into a single, normalized form. Doing so reduces the number of tokens you need to retain in your vocabulary and also improves the association of meaning across those different \"spellings\" of a token or _n_-gram in your corpus. And as we mentioned before, reducing your vocabulary can reduce the likelihood of overfitting.\n",
       "\n",
       "==== Case folding\n",
       "\n",
       "Case folding is when you consolidate multiple \"spellings\" of a word that differ only in their capitalization.\n",
       "So why would we use case folding at all?\n",
       "Words can become case \"denormalized\" when they are capitalized because of their presence at the beginning of a sentence, or when they're written in `ALL CAPS` for emphasis.\n",
       "// TODO: Discuss _ normalization, Unicode normalization, and asciification, diacritics, accented e in resume', etc\n",
       "Undoing this denormalization is called _case normalization_, or more commonly, _case folding_.\n",
       "Normalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline.\n",
       "It helps you consolidate words that are intended to mean (and be spelled) the same thing under a single token.\n",
       "\n",
       "However, some information is often communicated by capitalization of a word -- for example,  'doctor' and 'Doctor' often have different meanings.\n",
       "Often capitalization is used to indicate that a word is a proper noun, the name of a person, place, or thing.\n",
       "You will want to be able to recognize proper nouns as distinct from other words, if named entity recognition is important to your pipeline.\n",
       "However, if tokens are not case normalized, your vocabulary will be approximately twice as large, consume twice as much memory and processing time, and might increase the amount of training data you need to have labeled for your machine learning pipeline to converge to an accurate, general solution.\n",
       "Just as in any other machine learning pipeline, your labeled dataset used for training must be \"representative\" of the space of all possible feature vectors your model must deal with, including variations in capitalization.\n",
       "For 100000-D bag-of-words vectors, you usually must have 100000 labeled examples, and sometimes even more than that, to train a supervised machine learning pipeline without overfitting.\n",
       "In some situations, cutting your vocabulary size by half can sometimes be worth the loss of information content.\n",
       "\n",
       "In Python, you can easily normalize the capitalization of your tokens with a list comprehension.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> tokens = ['House', 'Visitor', 'Center']\n",
       ">>> normalized_tokens = [x.lower() for x in tokens]\n",
       ">>> print(normalized_tokens)\n",
       "['house', 'visitor', 'center']\n",
       "----\n",
       "\n",
       "And if you are certain that you want to normalize the case for an entire document, you can `lower()` the text string in one operation, before tokenization.\n",
       "But this will prevent advanced tokenizers that can split _camel case_ words like \"WordPerfect\", \"FedEx\", or \"stringVariableName.\"footnote:[See the web page titled \"Camel case case - Wikipedia\" (https://en.wikipedia.org/wiki/Camel_case_case).]]\n",
       "Maybe you want WordPerfect to be its own unique thing (token), or maybe you want to reminisce about a more perfect word processing era.\n",
       "It is up to you to decide when and how to apply case folding.\n",
       "\n",
       "With case normalization, you are attempting to return these tokens to their \"normal\" state before grammar rules and their position in a sentence affected their capitalization.\n",
       "The simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Python's built-in `str.lower()`.footnote:[We're assuming the behavior of str.lower() in Python 3. In Python 2, bytes (strings) could be lowercased by just shifting all alpha characters in the ASCII number (`ord`) space, but in Python 3 `str.lower` properly translates characters so it can handle embellished English characters (like the \"acute accent\" diactric mark over the e in resumé) as well as the particulars of capitalization in non-English languages.]\n",
       "Unfortunately this approach will also \"normalize\" away a lot of meaningful capitalization in addition to the less meaningful first-word-in-sentence capitalization you intended to normalize away.\n",
       "A better approach for case normalization is to lowercase only the first word of a sentence and allow all other words to retain their capitalization.\n",
       "\n",
       "Lowercasing on the first word in a sentence preserves the meaning of a proper nouns in the middle of a sentence, like \"Joe\" and \"Smith\" in \"Joe Smith\".\n",
       "And it properly groups words together that belong together, because they are only capitalized when they are at the beginning of a sentence, since they are not proper nouns.\n",
       "This prevents \"Joe\" from being confused with \"coffee\" (\"joe\")footnote:[The trigram \"cup of joe\" (https://en.wiktionary.org/wiki/cup_of_joe) is slang for \"cup of coffee.\"] during tokenization.\n",
       "And this approach prevents the blacksmith connotation of \"smith\" being confused the the proper name \"Smith\" in a sentence like \"A word smith had a cup of joe.\"\n",
       "Even with this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still need to introduce capitalization errors for the rare proper nouns that start a sentence.\n",
       "\"Joe Smith, the word smith, with a cup of joe.\" will produce a different set of tokens than \"Smith the word with a cup of joe, Joe Smith.\"\n",
       "And you may not  want that.\n",
       "In addition, case normalization is useless for languages that do not have a concept of capitalization, like Arabic or Hindi.\n",
       "\n",
       "To avoid this potential loss of information, many NLP pipelines do not normalize for case at all.\n",
       "For many applications, the efficiency gain (in storage and processing) for reducing one's vocabulary size by about half is outweighed by the loss of information for proper nouns.\n",
       "But some information may be \"lost\" even without case normalization.\n",
       "If you do not identify the word \"The\" at the start of a sentence as a stop word, that can be a problem for some applications.\n",
       "Really sophisticated pipelines will detect proper nouns before selectively normalizing the case for words at the beginning of sentences that are clearly not proper nouns.\n",
       "You should implement whatever case normalization approach makes sense for your application.\n",
       "If you do not have a lot of \"Smith\"s and \"word smiths\" in your corpus, and you do not care if they get assigned to the the same tokens, you can just lowercase everything.\n",
       "The best way to find out what works is to try several different approaches, and see which approach gives you the best performance for the objectives of your NLP project.\n",
       "\n",
       "By generalizing your model to work with text that has odd capitalization, case normalization can reduce overfitting for your machine learning pipeline.\n",
       "Case normalization is particularly useful for a search engine.\n",
       "For search, normalization increases the number of matches found for a particular query.\n",
       "This is often called the \"recall\" performance metric for a search engine (or any other classification model).footnote:[Check our Appendix D to learn more about _precision_ and _recall_. Here's a comparison of the recall of various search engines on the Webology site (http://www.webology.org/2005/v2n2/a12.html).]\n",
       "\n",
       "For a search engine without normalization if you searched for \"Age\" you will get a different set of documents than if you searched for \"age.\"\n",
       "\"Age\" would likely occur in phrases like \"New Age\" or \"Age of Reason\".\n",
       "In contrast, \"age\" would be more likely to occur in phrases like \"at the age of\" in your sentence about Thomas Jefferson.\n",
       "By normalizing the vocabulary in your search index (as well as the query), you can ensure that both kinds of documents about \"age\" are returned regardless of the capitalization in the query from the user.\n",
       "\n",
       "However, this additional recall accuracy comes at the cost of precision, returning many documents that the user may not be interested in. Because of this issue, modern search engines allow users to turn off normalization with each query, typically by quoting those words for which they want only exact matches returned. If you are building such a search engine pipeline, in order to accommodate both types of queries you will have to build two indexes for your documents: one with case-normalized _n_-grams, and another with the original capitalization.\n",
       "\n",
       "==== Stemming\n",
       "\n",
       "Another common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms.\n",
       "This normalization, identifying a common stem among various forms of a word, is called stemming.\n",
       "For example, the words `housing` and `houses` share the same stem, `house`.\n",
       "Stemming removes suffixes from words in an attempt to combine words with similar meanings together under their common stem.\n",
       "A stem is not required to be a properly spelled word, but merely a token, or label, representing several possible spellings of a word.\n",
       "\n",
       "A human can easily see that \"house\" and \"houses\" are the singular and plural forms of the same noun.\n",
       "However, you need some way to provide this information to the machine. One of its main benefits is in the compression of the number of words whose meaning your software or language model needs to keep track of.\n",
       "It reduces the size of your vocabulary while limiting the loss of information and meaning, as much as possible.\n",
       "In machine learning this is referred to as dimension reduction.\n",
       "It helps generalize your language model, enabling the model to behave identically for all the words included in a stem.\n",
       "So, as long as your application does not require your machine to distinguish between \"house\" and \"houses\", this stem will reduce your programming or dataset size by half or even more, depending on the aggressiveness of the stemmer you chose.\n",
       "\n",
       "Stemming is important for keyword search or information retrieval.\n",
       "It allows you to search for \"developing houses in Portland\" and get web pages or documents that use both the word \"house\" and \"houses\" and even the word \"housing\" because these words are all stemmed to the \"hous\" token.\n",
       "Likewise you might receive pages with the words \"developer\" and \"development\" rather than \"developing\" because all these words typically reduce to the stem \"develop\".\n",
       "As you can see, this is a \"broadening\" of your search, ensuring that you are less likely to miss a relevant document or web page.\n",
       "This broadening of your search results would be a big improvement in the \"recall\" score for how well your search engine is doing its job at returning all the relevant documents.footnote:[Review Appendix D if you have forgotten how to measure recall or visit the wikipedia page to learn more (https://en.wikipedia.org/wiki/Precision_and_recall).]\n",
       "\n",
       "But stemming could greatly reduce the \"precision\" score for your search engine because it might return many more irrelevant documents along with the relevant ones.\n",
       "In some applications this \"false-positive rate\" (proportion of the pages returned that you do not find useful) can be a problem.\n",
       "So most search engines allow you to turn off stemming and even case normalization by putting quotes around a word or phrase.\n",
       "Quoting indicates that you only want pages containing the exact spelling of a phrase such as \"'Portland Housing Development software'.\"\n",
       "That would return a different sort of document than one that talks about a \"'a Portland software developer's house'.\"\n",
       "And there are times when you want to search for \"Dr. House's calls\" and not \"dr house call\", which might be the effective query if you used a stemmer on that query.\n",
       "\n",
       "Here's a simple stemmer implementation in pure Python that can handle trailing S's.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> def stem(phrase):\n",
       "...     return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
       "...         word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
       ">>> stem('houses')\n",
       "'house'\n",
       ">>> stem(\"Doctor House's calls\")\n",
       "'doctor house call'\n",
       "----\n",
       "\n",
       "The preceding stemmer function follows a few simple rules within that one short regular expression:\n",
       "\n",
       "* If a word ends with more than one `s`, the stem is the word and the suffix is a blank string.\n",
       "* If a word ends with a single `s`, the stem is the word without the `s` and the suffix is the `s`.\n",
       "* If a word does not end on an `s`, the stem is the word and no suffix is returned.\n",
       "\n",
       "The strip method ensures that some possessive words can be stemmed along with plurals.\n",
       "\n",
       "This function works well for regular cases, but is unable to address more complex cases. For example, the rules would fail with words like `dishes` or `heroes`. For more complex cases like these, the NLTK package provides other stemmers.\n",
       "\n",
       "It also does not handle the \"housing\" example from your \"Portland Housing\" search.\n",
       "\n",
       "Two of the most popular stemming algorithms are the Porter and Snowball stemmers.\n",
       "The Porter stemmer is named for the computer scientist Martin Porter.footnote:[See \"An algorithm for suffix stripping\", 1993 (http://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf) by M.F. Porter.]\n",
       "Porter is also also responsible for enhancing the Porter stemmer to create the Snowball stemmer.footnote:[See the web page titled \"Snowball: A language for stemming algorithms\" (http://snowball.tartarus.org/texts/introduction.html).]\n",
       "Porter dedicated much of his lengthy career to documenting and improving stemmers, due to their value in information retrieval (keyword search).\n",
       "These stemmers implement more complex rules than our simple regular expression.\n",
       "This enables the stemmer to handle the complexities of English spelling and word ending rules.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.stem.porter import PorterStemmer\n",
       ">>> stemmer = PorterStemmer()\n",
       ">>> ' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
       "...   \"dish washer's fairly washed dishes\".split()])\n",
       "'dish washer fairli wash dish'\n",
       "----\n",
       "\n",
       "Notice that the Porter stemmer, like the regular expression stemmer, retains the trailing apostrophe (unless you explicitly strip it), which ensures that possessive words will be distinguishable from nonpossessive words.\n",
       "Possessive words are often proper nouns, so this feature can be important for applications where you want to treat names differently than other nouns.\n",
       "\n",
       ".More on the Porter stemmer\n",
       "****\n",
       "Julia Menchavez has graciously shared her translation of Porter's original stemmer algorithm into pure python (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py). If you are ever tempted to develop your own stemmer, consider these 300 lines of code and the lifetime of refinement that Porter put into them.\n",
       "\n",
       "There are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4, 5a, and 5b.\n",
       "Step 1a is a bit like your regular expression for dealing with trailing \"S\"es:footnote:[This is a trivially abbreviated version of Julia Menchavez's implementation `porter-stemmer` on GitHub (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py).]\n",
       "\n",
       "[source,python]\n",
       "----\n",
       "def step1a(self, word):\n",
       "    if word.endswith('sses'):\n",
       "        word = self.replace(word, 'sses', 'ss')  # <1>\n",
       "    elif word.endswith('ies'):\n",
       "        word = self.replace(word, 'ies', 'i')\n",
       "    elif word.endswith('ss'):\n",
       "        word = self.replace(word, 'ss', 'ss')\n",
       "    elif word.endswith('s'):\n",
       "        word = self.replace(word, 's', '')\n",
       "    return word\n",
       "----\n",
       "<1> This is not at all like `str.replace()`. Julia's `self.replace()` modifies only the ending of a word.\n",
       "\n",
       "The remainining seven steps are much more complicated because they have to deal with the complicated English spelling rules for the following:\n",
       "\n",
       "* *Step 1a*: \"s\" and \"es\" endings\n",
       "* *Step 1b*: \"ed\", \"ing\", and \"at\" endings\n",
       "* *Step 1c*: \"y\" endings\n",
       "* *Step 2*: \"nounifying\" endings such as \"ational\", \"tional\", \"ence\", and \"able\"\n",
       "* *Step 3*: adjective endings such as \"icate\",footnote:[Sorry Chick, Porter doesn't like your `obsfucate` username ;)], \"ful\", and \"alize\"\n",
       "* *Step 4*: adjective and noun endings such as \"ive\", \"ible\", \"ent\", and \"ism\"\n",
       "* *Step 5a*: stubborn \"e\" endings, still hanging around\n",
       "* *Step 5b*: trailing double-consonants for which the stem will end in a single \"l\"\n",
       "****\n",
       "\n",
       "Snowball stemmer is more aggressive than the Porter stemmer.\n",
       "Notice that it stems 'fairly' to 'fair', which is more accurate than the Porter stemmer.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.stem.snowball import SnowballStemmer\n",
       ">>> stemmer = SnowballStemmer(language='english')\n",
       ">>> ' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
       "...   \"dish washer's fairly washed dishes\".split()])\n",
       "'dish washer fair wash dish'\n",
       "----\n",
       "\n",
       "==== Lemmatization\n",
       "\n",
       "If you have access to information about connections between the meanings of various words, you might be able to associate several words together even if their spelling is quite different.\n",
       "This more extensive normalization down to the semantic root of a word -- its lemma -- is called lemmatization.\n",
       "\n",
       "In chapter 12, we show how you can use lemmatization to reduce the complexity of the logic required to respond to a statement with a chatbot.\n",
       "Any NLP pipeline that wants to \"react\" the same for multiple different spellings of the same basic root word can benefit from a lemmatizer.\n",
       "It reduces the number of words you have to respond to, the dimensionality of your language model.\n",
       "Using it can make your model more general, but it can also make your model less precise, because it will treat all spelling variations of a given root word the same.\n",
       "For example \"chat\", \"chatter\", \"chatty\", \"chatting\", and perhaps even \"chatbot\" would all be treated the same in an NLP pipeline with lemmatization, even though they have different meanings.\n",
       "Likewise \"bank\", \"banked\", and \"banking\" would be treated the same by a stemming pipeline despite the river meaning of \"bank\", the motorcycle meaning of \"banked\" and the finance meaning of \"banking.\"\n",
       "\n",
       "As you work through this section, think about words where lemmatization would drastically alter the meaning of a word, perhaps even inverting its meaning and producing the opposite of the intended response from your pipeline.\n",
       "This scenario is called _spoofing_ -- when you try to elicit the wrong response from a machine learning pipeline by cleverly constructing a difficult input.\n",
       "\n",
       "Sometimes lemmatization will be a better way to normalize the words in your vocabulary.\n",
       "You may find that for your application stemming and case folding create stems and tokens that do not take into account a word's meaning.\n",
       "A lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.\n",
       "\n",
       "Some lemmatizers use the word's part of speech (POS) tag in addition to its spelling to help improve accuracy.\n",
       "The POS tag for a word indicates its role in the grammar of a phrase or sentence.\n",
       "For example, the noun POS is for words that refer to \"people, places, or things\" within a phrase.\n",
       "An adjective POS is for a word that modifies or describes a noun.\n",
       "A verb refers to an action.\n",
       "The POS of a word in isolation cannot be determined.\n",
       "The context of a word must be known for its POS to be identified.\n",
       "So some advanced lemmatizers cannot be run on words in isolation.\n",
       "\n",
       "Can you think of ways you can use the part of speech to identify a better \"root\" of a word than stemming could?\n",
       "Consider the word `better`.\n",
       "Stemmers would strip the \"er\" ending from \"better\" and return the stem \"bett\" or \"bet\".\n",
       "However, this would lump the word \"better\" with words like \"betting\", \"bets\", and \"Bet's\", rather than more similar words like \"betterment\", \"best\", or even \"good\" and \"goods\".\n",
       "\n",
       "So lemmatizers are better than stemmers for most applications.\n",
       "Stemmers are only really used in large scale information retrieval applications (keyword search).\n",
       "And if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer.\n",
       "Because the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer.\n",
       "This trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone.footnote:[Thank you Kyle Gorman for pointing this out]\n",
       "\n",
       "How can you identify word lemmas in Python?\n",
       "The NLTK package provides functions for this.\n",
       "Notice that you must tell the WordNetLemmatizer which part of speech you are interested in, if you want to find the most accurate lemma:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> nltk.download('wordnet')\n",
       "True\n",
       ">>> nltk.download('omw-1.4')\n",
       "True\n",
       ">>> from nltk.stem import WordNetLemmatizer\n",
       ">>> lemmatizer = WordNetLemmatizer()\n",
       ">>> lemmatizer.lemmatize(\"better\")  # <1>\n",
       "'better'\n",
       ">>> lemmatizer.lemmatize(\"better\", pos=\"a\")  # <2>\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"good\", pos=\"a\")\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"goods\", pos=\"a\")\n",
       "'goods'\n",
       ">>> lemmatizer.lemmatize(\"goods\", pos=\"n\")\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"goodness\", pos=\"n\")\n",
       "'goodness'\n",
       ">>> lemmatizer.lemmatize(\"best\", pos=\"a\")\n",
       "'best'\n",
       "----\n",
       "<1> The default part of speech is \"n\" for \"noun\"\n",
       "<2> \"a\" indicates the \"adjective\" part of speech\n",
       "\n",
       "You might be surprised that the first attempt to lemmatize the word \"better\" did not change it at all. This is because the part of speech of a word can have a big effect on its meaning. If a POS is not specified for a word, then the NLTK lemmatizer assumes it is a noun. Once you specify the correct POS, 'a' for adjective, the lemmatizer returns the correct lemma. Unfortunately, the NLTK lemmatizer is restricted to the connections within the Princeton WordNet graph of word meanings. So the word \"best\" does not lemmatize to the same root as \"better\". This graph is also missing the connection between \"goodness\" and \"good\". A Porter stemmer, on the other hand, would make this connection by blindly stripping off the \"ness\" ending of all words.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> stemmer.stem('goodness')\n",
       "'good'\n",
       "----\n",
       "\n",
       "You can easily implement lemmatization in spaCy by the following:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
       ">>> doc = nlp(\"better good goods goodness best\")\n",
       ">>> for token in doc:\n",
       ">>> print(token.text, token.lemma_)\n",
       "better well\n",
       "good good\n",
       "goods good\n",
       "goodness goodness\n",
       "best good\n",
       "----\n",
       "Unlike NLTK, spaCy lemmatizes \"better\" to \"well\" by assuming it is an adverb and returns the correct lemma for \"best\" (\"good\").\n",
       "\n",
       "==== Synonym substitution\n",
       "\n",
       "There are five kinds of \"synonyms\" that are sometime helpful in creating a consistent smaller vocabulary to help your NLP pipeline generalize well.\n",
       "\n",
       ". Typo correction\n",
       ". Spelling correction\n",
       ". Synonym substitution\n",
       ". Contraction expansion\n",
       ". Emoji expansion\n",
       "\n",
       "Each of these synonym substitution algorithms can be designed to be more or less agressive.\n",
       "And you will want to think about the language used by your users in your domain.\n",
       "For example, in the legal, technical, or medical fields, it's rarely a good idea to substitute synonyms.\n",
       "A doctor wouldn't want a chatbot telling his patient their \"heart is broken\" because of some synonym substitutions on the heart emoticon (\"<3\").\n",
       "\n",
       "Nonetheless, the use cases for lemmatization and stemming apply to synonym substitution.\n",
       "\n",
       "==== Use cases\n",
       "\n",
       "When should you use a lemmatizer, stemmer, or synonym substitution?\n",
       "Stemmers are generally faster to compute and require less-complex code and datasets.\n",
       "But stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would.\n",
       "Both stemmers and lemmatizers will reduce your vocabulary size and increase the ambiguity of the text.\n",
       "But lemmatizers do a better job retaining as much of the information content as possible based on how the word was used within the text and its intended meaning.\n",
       "As a result, some state of the art NLP packages, such as spaCy, do not provide stemming functions and only offer lemmatization methods.\n",
       "\n",
       "If your application involves search, stemming and lemmatization will improve the recall of your searches by associating more documents with the same query words.\n",
       "However, stemming, lemmatization, and even case folding will usually reduce the precision and accuracy of your search results.\n",
       "These vocabulary compression approaches may cause your information retrieval system (search engine) to return many documents not relevant to the words' original meanings.\n",
       "These are called \"false positives\", a incorrect matches to your search query.\n",
       "Sometimes \"false positives\" are less important than false negatives.\n",
       "A false negative for a search engine is when it fails to list the document you are looking for at all.\n",
       "\n",
       "Because search results can be ranked according to relevance, search engines and document indexes typically use lemmatization when they process your query and index your documents.\n",
       "Because search results can be ranked according to relevance, search engines and document indexes typically use lemmatization in their NLP pipeline.\n",
       "This means a search engine will use lemmatization when they tokenize your search text as well as when they index their collection of documents, such as the web pages they crawl.\n",
       "\n",
       "But they combine search results for unstemmed versions of words to rank the search results that they present to you.footnote:[Additional metadata is also used to adjust the ranking of search results.\n",
       "Duck Duck Go and other popular web search engines combine more than 400 independent algorithms (including user-contributed algorithms) to rank your search results (https://duck.co/help/results/sources).]\n",
       "\n",
       "For a search-based chatbot, precision is usually more important than recall.\n",
       "A false positive match can cause your chatbot says something inappropriate.\n",
       "False negatives just cause your chatbot to have to humbly admit that it cannot find anything appropriate to say.\n",
       "Your chatbot will sound better if your NLP pipeline first searches for matches to your user's questions using unstemmed, unnormalized words.\n",
       "Your search algorithm can fall back to normalized token matches if it cannot find anything else to say.\n",
       "And you can rank these *fallback* matches for normalized tokens lower than the unnormalized token matches.\n",
       "You can even give your bot humility and transparency by introducing lower ranked responses with a caveat, such as \"I haven't heard a phrase like that before, but using my stemmer I found...\"\n",
       "In a modern world crowded with blowhard chatbots, your humbler chatbot can make a name for itself and win out!footnote:[\"Nice guys finish first!\" -- M.A. Nowak author of _SuperCooperators_\"]\n",
       "\n",
       "There are 4 situations when synonym substitution of some sort may make sense.\n",
       "\n",
       ". Search engines\n",
       ". Data augmentation\n",
       ". Scoring the robustness of your NLP\n",
       ". Adversarial NLP\n",
       "\n",
       "Search engines can improve their recall for rare terms by using synonym substitution.\n",
       "When you have limited labeled data, you can often expand your dataset 10 fold (10x) with synonym substitution alone.\n",
       "If you want to find a lower bound on the accuracy of your model you can aggressively substitute synonyms in your test set to see how robust your model is to these changes.\n",
       "And if you are searching for ways to poison or evade detection by an NLP algorithm, synonyms can give you a large number of probing texts to try.\n",
       "You can imagine that substituting the \"currency\" for the word \"cash\", \"dollars\", or \"$$$$\" might help evade a spam detector.\n",
       "\n",
       "[IMPORTANT]\n",
       "Bottom line, try to avoid stemming, lemmatization, case folding, or synonym substitution, unless you have a limited amount of text with contains usages and capitalizations of the words you are interested in.\n",
       "And with the explosion of NLP datasets, this is rarely the case for English documents, unless your documents use a lot of jargon or are from a very small subfield of science, technology, or literature.\n",
       "Nonetheless, for languages other than English, you may still find uses for lemmatization.\n",
       "The Stanford information retrieval course dismisses stemming and lemmatization entirely, due to the negligible recall accuracy improvement and the significant reduction in precision.footnote:[See the Stanford NLP Information Retrieval (IR) book section titled \"Stemming and lemmatization\" (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).]\n",
       "\n",
       "\n",
       "\n",
       "== Sentiment\n",
       "\n",
       "Whether you use raw single-word tokens, _n_-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information.\n",
       "An important part of this information is the word's sentiment -- the overall feeling or emotion that word invokes.\n",
       "This _sentiment analysis_ -- measuring the sentiment of phrases or chunks of text -- is a common application of NLP.\n",
       "In many companies it is the main thing an NLP engineer is asked to do.\n",
       "\n",
       "Companies like to know what users think of their products.\n",
       "So they often will provide some way for you to give feedback.\n",
       "A star rating on Amazon or Rotten Tomatoes is one way to get quantitative data about how people feel about products they've purchased.\n",
       "But a more natural way is to use natural language comments.\n",
       "Giving your user a blank slate (an empty text box) to fill up with comments about your product can produce more detailed feedback.\n",
       "\n",
       "In the past you would have to read all that feedback.\n",
       "Only a human can understand something like emotion and sentiment in natural language text, right?\n",
       "However, if you had to read thousands of reviews you would see how tedious and error-prone a human reader can be.\n",
       "Humans are remarkably bad at reading feedback, especially criticism or negative feedback.\n",
       "And customers are not generally very good at communicating feedback in a way that can get past your natural human triggers and filters.\n",
       "\n",
       "But machines do not have those biases and emotional triggers.\n",
       "And humans are not the only things that can process natural language text and extract information, even meaning from it.\n",
       "An NLP pipeline can process a large quantity of user feedback quickly and objectively, with less chance for bias.\n",
       "And an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\n",
       "\n",
       "Another common application of sentiment analysis is junk mail and troll message filtering.\n",
       "You would like your chatbot to be able to measure the sentiment in the chat messages it processes so it can respond appropriately.\n",
       "And even more importantly, you want your chatbot to measure its own sentiment of the statements it is about to send out, which you can use to steer your bot to be kind and pro-social with the statements it makes.\n",
       "The simplest way to do this might be to do what Moms told us to do: If you cannot say something nice, do not say anything at all.\n",
       "So you need your bot to measure the niceness of everything you are about to say and use that to decide whether to respond.\n",
       "\n",
       "What kind of pipeline would you create to measure the sentiment of a block of text and produce this sentiment positivity number?\n",
       "Say you just want to measure the positivity or favorability of a text -- how much someone likes a product or service that they are writing about.\n",
       "Say you want your NLP pipeline and sentiment analysis algorithm to output a single floating point number between -1 and +1.\n",
       "Your algorithm would output +1 for text with positive sentiment like \"Absolutely perfect! Love it! :-) :-) :-)\".\n",
       "And your algorithm should output -1 for text with negative sentiment like \"Horrible! Completely useless. :(\".\n",
       "Your NLP pipeline could use values near 0, like say +0.1, for a statement like \"It was OK. Some good and some bad things\".\n",
       "\n",
       "There are two approaches to sentiment analysis:\n",
       "\n",
       "* A rule-based algorithm composed by a human\n",
       "* A _machine learning_ model learned from data by a machine\n",
       "\n",
       "The first approach to sentiment analysis uses human-designed rules, sometimes called heuristics, to measure sentiment.\n",
       "A common rule-based approach to sentiment analysis is to find keywords in the text and map each one to numerical scores or weights in a dictionary or \"mapping\" -- a Python `dict`, for example.\n",
       "Now that you know how to do tokenization, you can use stems, lemmas, or _n_-gram tokens in your dictionary, rather than just words.\n",
       "The \"rule\" in your algorithm would be to add up these scores for each keyword in a document that you can find in your dictionary of sentiment scores.\n",
       "Of course you need to hand-compose this dictionary of keywords and their sentiment scores before you can run this algorithm on a body of text.\n",
       "We show you how to do this using the VADER algorithm (in `sklearn`) in the upcoming listing.\n",
       "\n",
       "The second approach, machine learning, relies on a labeled set of statements or documents to train a machine learning model to create those rules.\n",
       "A machine learning sentiment model is trained to process input text and output a numerical value for the sentiment you are trying to measure, like positivity or spamminess or trolliness.\n",
       "For the machine learning approach, you need a lot of data, text labeled with the \"right\" sentiment score.\n",
       "Twitter feeds are often used for this approach because the hash tags, such as `\\#awesome` or `\\#happy` or `\\#sarcasm`, can often be used to create a \"self-labeled\" dataset.\n",
       "Your company may have product reviews with five-star ratings that you could associate with reviewer comments.\n",
       "You can use the star ratings as a numerical score for the positivity of each text.\n",
       "We show you shortly how to process a dataset like this and train a token-based machine learning algorithm called _Naive Bayes_ to measure the positivity of the sentiment in a set of reviews after you are done with VADER.\n",
       "\n",
       "=== VADER -- A rule-based sentiment analyzer\n",
       "\n",
       "Hutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms.\n",
       "They called their algorithm VADER, for **V**alence **A**ware **D**ictionary for\n",
       "s**E**ntiment **R**easoning.footnote:[\"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\" by Hutto and Gilbert (http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).]\n",
       "Many NLP packages implement some form of this algorithm.\n",
       "The NLTK package has an implementation of the VADER algorithm in `nltk.sentiment.vader`.\n",
       "Hutto himself maintains the Python package `vaderSentiment`.\n",
       "You will go straight to the source and use `vaderSentiment` here.\n",
       "\n",
       "You will need to `pip install vaderSentiment` to run the following example.footnote:[You can find more detailed installation instructions with the package source code on github (https://github.com/cjhutto/vaderSentiment).]\n",
       "You have not included it in the `nlpia` package.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
       ">>> sa = SentimentIntensityAnalyzer()\n",
       ">>> sa.lexicon  # <1>\n",
       "{ ...\n",
       "':(': -1.9,  # <2>\n",
       "':)': 2.0,\n",
       "...\n",
       "'pls': 0.3,  # <3>\n",
       "'plz': 0.3,\n",
       "...\n",
       "'great': 3.1,\n",
       "... }\n",
       ">>> [(tok, score) for tok, score in sa.lexicon.items()\n",
       "...   if \" \" in tok]  # <4>\n",
       "[(\"( '}{' )\", 1.6),\n",
       " (\"can't stand\", -2.0),\n",
       " ('fed up', -1.8),\n",
       " ('screwed up', -1.5)]\n",
       ">>> sa.polarity_scores(text=\\\n",
       "...   \"Python is very readable and it's great for NLP.\")\n",
       "{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661,\n",
       "'pos': 0.339}  # <5>\n",
       ">>> sa.polarity_scores(text=\\\n",
       "...   \"Python is not a bad choice for most applications.\")\n",
       "{'compound': 0.431, 'neg': 0.0, 'neu': 0.711,\n",
       "'pos': 0.289}  # <6>\n",
       "----\n",
       "<1> SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores that we talked about.\n",
       "<2> A tokenizer better be good at dealing with punctuation and emoticons (emojis) for VADER to work well. After all, emoticons are designed to convey a lot of sentiment (emotion).\n",
       "<3> If you use a stemmer (or lemmatizer) in your pipeline, you would need to apply that stemmer to the VADER lexicon, too, combining the scores for all the words that go together in a single stem or lemma.\n",
       "<4> Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually _n_-grams; the other is an emoticon for \"kiss\".\n",
       "<5> The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them together into a compound positivity sentiment.\n",
       "<6> Notice that VADER handles negation pretty well -- \"great\" has a slightly more positive sentiment than \"not bad\". VADER's built-in tokenizer ignores any words that aren't in its lexicon, and it does not consider _n_-grams at all.\n",
       "\n",
       "Let us see how well this rule-based approach does for the example statements we mentioned earlier.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
       "...           \"Horrible! Completely useless. :(\",\n",
       "...           \"It was OK. Some good and some bad things.\"]\n",
       ">>> for doc in corpus:\n",
       "...     scores = sa.polarity_scores(doc)\n",
       "...     print('{:+}: {}'.format(scores['compound'], doc))\n",
       "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
       "-0.8768: Horrible! Completely useless. :(\n",
       "+0.3254: It was OK. Some good and some bad things.\n",
       "----\n",
       "\n",
       "This looks a lot like what you wanted.\n",
       "So the only drawback is that VADER does not look at all the words in a document.\n",
       "VADER only \"knows\" about the 7,500 words or so that were hard-coded into its algorithm.\n",
       "What if you want all the words to help add to the sentiment score?\n",
       "And what if you do not want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in `SentimentIntensityAnalyzer.lexicon`?\n",
       "The rule-based approach might be impossible if you do not understand the language because you would not know what scores to put in the dictionary (lexicon)!\n",
       "\n",
       "That is what machine learning sentiment analyzers are for.\n",
       "\n",
       "=== Closeness of vectors\n",
       "\n",
       "Why do we use bags of words rather than bags of characters to represent natural language text?\n",
       "For a cryptographer trying to decrypt an unknown message, frequency analysis of the characters in the text would be a good way to go.\n",
       "But for natural language text in your native language, words turn out to be a better representation.\n",
       "You can see this if you think about what we are using these BOW vectors for.\n",
       "\n",
       "If you think about it, you have a lot of different ways to measure the closeness of things.\n",
       "You probably have a good feel for what a close family relative would be.\n",
       "Or the closeness of the cafes where you can meet your friend to collaborate on writing a book about AI.\n",
       "For cafes your brain probably uses Euclidean distance on the 2D position of the cafes you know about.\n",
       "Or maybe Manhattan or taxi-cab distance.\n",
       "\n",
       "But do you know how to measure the closeness of two pieces of text?\n",
       "In chapter 4 you'll learn about edit distances that check the similarity of two strings of characters.\n",
       "But that doesn't really capture the essence of what you care about.\n",
       "\n",
       "How close are these sentences to each other, in your mind?\n",
       "\n",
       "> I am now coming over to see you.\n",
       "\n",
       "> I am not coming over to see you.\n",
       "\n",
       "Do you see the difference?\n",
       "Which one would you prefer to receive an e-mail from your friend.\n",
       "The words \"now\" and \"not\" are very far apart in meaning.\n",
       "But they are very close in spelling.\n",
       "This is an example about how a single character can change the meaning of an entire sentence.\n",
       "\n",
       "If you just counted up the characters that were different you'd get a distance of 1.\n",
       "And then you could divide by the length of the longest sentence to make sure your distance value is between 0 and 1.\n",
       "So your character difference or distance calculation would be 1 divided by 32 which gives 0.03125, or about 3%.\n",
       "Then, to turn a distance into a closeness you just subtract it from 1.\n",
       "So do you think these two sentences are 0.96875, or about 97% the same?\n",
       "They mean the opposite.\n",
       "So we'd like a better measure than that.\n",
       "\n",
       "What if you compared words instead of characters?\n",
       "In that case you would have one word out of seven that was changed.\n",
       "That is a little better than one character out of 32.\n",
       "The sentences would now have a closeness score of six divided by seven or about 85%.\n",
       "That's a little lower, which is what we want.\n",
       "\n",
       "For natural language you don't want your closeness or distance measure to rely only on a count of the differences in individual characters.\n",
       "This is one reason why you want to use words as your tokens of meaning when processing natural language text.\n",
       "\n",
       "What about these two sentences?\n",
       "\n",
       "> She and I will come over to your place at 3:00.\n",
       "\n",
       "> At 3:00, she and I will stop by your apartment.\n",
       "\n",
       "Are these two sentences close to each other in meaning?\n",
       "They have the exact same length in characters.\n",
       "And they use some of the same words, or at least synonyms.\n",
       "But those words and characters are not in the same order.\n",
       "So we need to make sure that our representation of the sentences does not rely on the precise position of words in a sentence.\n",
       "\n",
       "Bag of words vectors accomplish this by creating a position or slot in a vector for every word you've seen in your vocabulary.\n",
       "You may have learned of a few measures of closeness in geometry and linear algebra.\n",
       "\n",
       "As an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\n",
       "Very smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\n",
       "Imagine how difficult that is.\n",
       "Imagine trying to remove verb endings like \"ing\" from \"ending\" so you'd have a stem called \"end\" to represent both words.\n",
       "And you'd like to stem the word \"running\" to \"run,\" so those two words are treated the same.\n",
       "And that's tricky, because you have remove not only the \"ing\" but also the extra \"n\".\n",
       "But you want the word \"sing\" to stay whole.\n",
       "You wouldn't want to remove the \"ing\" ending from \"sing\" or you'd end up with a single-letter \"s\".\n",
       "\n",
       "Or imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens\".\n",
       "Do isolated individual letters in a word or parts of a word provide any information at all about that word's meaning?\n",
       "Can the letters be misleading?\n",
       "Yes and yes.\n",
       "\n",
       "In this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\n",
       "Later, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you're interested in.\n",
       "From that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\n",
       "\n",
       "=== Count vectorizing\n",
       "\n",
       "In the previous sections you've only been concerned with keyword detection.\n",
       "Your vectors indicated the presence or absence of words.\n",
       "In order to handle longer documents and improve the accuracy of your NLP pipeline, you're going to start counting the occurrences of words in your documents.\n",
       "\n",
       "You can put these counts into a sort-of histogram.\n",
       "Just like before you will create a vector for each document in your pipeline.\n",
       "Only instead of 0's and 1's in your vectors there will be counts.\n",
       "This will improve the accuracy of all the similarity and distance calculations you are doing with these counts.\n",
       "And just like normalizing histograms can improve your ability to compare two histograms, normalizing your word counts is also a good idea.\n",
       "Otherwise a really short wikipedia article that use Barak Obama's name only once along side all the other presidents might get as much \"Barack Obama\" credit as a much longer page about Barack Obama that uses his name many times.\n",
       "Users and Question Answering bots like `qary` trying to answer questions about Obama might get distracted by pages listing all the presidents and might miss the main Barack Obama page entirely.\n",
       "So it's a good idea to normalize your count vectors by dividing the counts by the total length of the document.\n",
       "This more fairly represents the distribution of tokens in the document and will create better similarity scores with other documents, including the text from a search query from `qary`.footnote:[Qary is an open source virtual assistant that actually assists you instead of manipulating and misinforming you (https://docs.qary.ai).]\n",
       "\n",
       "Each position in your vector represents the count for one of your keywords.\n",
       "And having a small vocabulary keeps this vector small, low-dimensional, and easy to reason about.\n",
       "And you can use this _count vectorizing_ approach even for large vocabularies.\n",
       "\n",
       "And you can organize these counts of those keywords into\n",
       "You need to organize the counts into a vector.\n",
       "This opens up a whole range of powerful tools for doing vector algebra.\n",
       "\n",
       "In natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\n",
       "Nonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\n",
       "The techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email from a flood of spam that almost made it useless.\n",
       "\n",
       "\n",
       "=== Naive Bayes\n",
       "\n",
       "A Naive Bayes model tries to find keywords in a set of documents that are predictive of your target (output) variable.\n",
       "When your target variable is the sentiment you are trying to predict, the model will find words that predict that sentiment.\n",
       "The nice thing about a Naive Bayes model is that the internal coefficients will map words or tokens to scores just like VADER does.\n",
       "Only this time you will not have to be limited to just what an individual human decided those scores should be.\n",
       "The machine will find the \"best\" scores for any problem.\n",
       "\n",
       "For any machine learning algorithm, you first need to find a dataset.\n",
       "You need a bunch of text documents that have labels for their positive emotional content (positivity sentiment).\n",
       "Hutto compiled four different sentiment datasets for us when he and his collaborators built VADER.\n",
       "You will load them from the `nlpia` package.footnote:[If you have not already installed `nlpia`, check out the installation instructions at http://gitlab.com/tangibleai/nlpia2.]\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> movies = pd.read_csv('https://proai.org/movie-reviews.csv.gz', \\\n",
       "...     index_col=0)\n",
       ">>> movies.head().round(2)\n",
       "    sentiment                                               text\n",
       "id                                                              \n",
       "1        2.27  The Rock is destined to be the 21st Century's ...\n",
       "2        3.53  The gorgeously elaborate continuation of ''The...\n",
       "3       -0.60                     Effective but too tepid biopic\n",
       "4        1.47  If you sometimes like to go to the movies to h...\n",
       "5        1.73  Emerges as something rare, an issue movie that...\n",
       "\n",
       ">>> movies.describe().round(2)\n",
       "       sentiment\n",
       "count   10605.00\n",
       "mean        0.00  # <1>\n",
       "std         1.92\n",
       "min        -3.88  # <2>\n",
       "...\n",
       "max         3.94  # <3>\n",
       "----\n",
       "<1> Sentiment scores (movie ratings) have been \"centered\" (mean is zero)\n",
       "<2> It looks like the scale starts around -4 for the worst movies\n",
       "<3> Seems like +4 is the maximum rating for the best movies\n",
       "\n",
       "It looks like the movie reviews have been _centered_: normalized by subtracting the mean so that the new mean will be zero and they aren't biased to one side or the other.\n",
       "And it seems the range of movie ratings allowed was -4 to +4.\n",
       "\n",
       "Now you can tokenize all those movie review texts to create a bag of words for each one.\n",
       "If you put them all into a Pandas DataFrame that will make them easier to work with.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> pd.options.display.width = 75  # <1>\n",
       ">>> from nltk.tokenize import casual_tokenize  # <2>\n",
       ">>> bags_of_words = []\n",
       ">>> from collections import Counter  # <3>\n",
       ">>> for text in movies.text:\n",
       "...     bags_of_words.append(Counter(casual_tokenize(text)))\n",
       ">>> df_bows = pd.DataFrame.from_records(bags_of_words)  # <4>\n",
       ">>> df_bows = df_bows.fillna(0).astype(int)  # <5>\n",
       ">>> df_bows.shape  # <6>\n",
       "(10605, 20756)\n",
       "\n",
       ">>> df_bows.head()\n",
       "   !  \"  #  $  %  &  ' ...  zone  zoning  zzzzzzzzz  ½  élan  –  ’\n",
       "0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n",
       "1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n",
       "2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "\n",
       ">>> df_bows.head()[list(bags_of_words[0].keys())]\n",
       "   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  .\n",
       "0    1     1   1         1   2   1 ...    1      1   1       1      1  1\n",
       "1    2     0   1         0   0   0 ...    0      0   0       0      0  4\n",
       "2    0     0   0         0   0   0 ...    0      0   0       0      0  0\n",
       "3    0     0   1         0   4   0 ...    0      0   0       0      0  1\n",
       "4    0     0   0         0   0   0 ...    0      0   0       0      0  1\n",
       "----\n",
       "<1> This prints a wide `DataFrame` in the console so they look prettier.\n",
       "<2> NLTK's `casual_tokenize` can handle emoticons, unusual punctuation, and slang better than `TreebankWordTokenizer`\n",
       "<3> `Counter` takes a list (or iterable) of objects and counts them up, returning a `dict` where the keys are the objects (tokens in your case) and the values are the counts.\n",
       "<4> The `from_records()` DataFrame constructor takes a sequence of dict objects. The `dict` keys become columns, and the values for missing keys are set to `NaN`.\n",
       "<5> NumPy and Pandas can only represent NaNs within a float dtype. So fill NaNs with zeros before converting to integers.\n",
       "<6> A BOW table can get really big if you don't do dimension reduction or feature selection.\n",
       "\n",
       "When you do not use case normalization, stop word filters, stemming, or lemmatization your vocabulary can be quite huge because you are keeping track of every little difference in spelling or capitalization of words.\n",
       "Try inserting some dimension reduction steps into your pipeline to see how they affect your pipeline's accuracy and the amount of memory required to store all these BOWs.\n",
       "\n",
       "Now you have all the data that a Naive Bayes model needs to find the keywords that predict sentiment from natural language text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from sklearn.naive_bayes import MultinomialNB\n",
       ">>> nb = MultinomialNB()\n",
       ">>> nb = nb.fit(df_bows, movies.sentiment > 0)  # <1>\n",
       ">>> movies['pred_senti'] = (\n",
       "...   nb.predict_proba(df_bows))[:, 1] * 8 - 4  # <2>\n",
       ">>> movies['error'] = movies.pred_senti - movies.sentiment\n",
       ">>> mae = movies['error'].abs().mean().round(1)  # <3>\n",
       ">>> mae\n",
       "1.9\n",
       "----\n",
       "\n",
       "To create a binary classification label you can use the fact that the centered movie ratings (sentiment labels) are positive (greater than zero) when the sentiment of the review is positive.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> movies['senti_ispos'] = (movies['sentiment'] > 0).astype(int)\n",
       ">>> movies['pred_ispos'] = (movies['pred_senti'] > 0).astype(int)\n",
       ">>> columns = [c for c in movies.columns if 'senti' in c or 'pred' in c]\n",
       ">>> movies[columns].head(8)\n",
       "    sentiment  pred_senti  senti_ispos  pred_ispos\n",
       "id\n",
       "1    2.266667                    4                     1                     1\n",
       "2    3.533333                    4                     1                     1\n",
       "3   -0.600000                   -4                     0                     0\n",
       "4    1.466667                    4                     1                     1\n",
       "5    1.733333                    4                     1                     1\n",
       "6    2.533333                    4                     1                     1\n",
       "7    2.466667                    4                     1                     1\n",
       "8    1.266667                   -4                     1                     0\n",
       ">>> (movies.pred_ispos ==\n",
       "...   movies.senti_ispos).sum() / len(movies)\n",
       "0.9344648750589345  # <4>\n",
       "----\n",
       "<1> Naive Bayes models are classifiers, so you need to convert your output variable (sentiment float) to a discrete label (integer, string, or bool).\n",
       "<2> Convert your discrete classification variable back to a real value between -4 and +4 so you can compare it to the \"ground truth\" sentiment.\n",
       "<3> Average absolute value of the prediction error or mean absolute error (MAE)\n",
       "<4> You got the \"thumbs up\" rating correct 93% of the time.\n",
       "\n",
       "This is a pretty good start at building a sentiment analyzer with only a few lines of code (and a lot of data).\n",
       "You did not have to guess at the sentiment associated with a list of 7500 words and hard code them into an algorithm such as VADER.\n",
       "Instead you told the machine the sentiment ratings for whole text snippets.\n",
       "And then the machine did all the work to figure out the sentiment associated with each word in those texts.\n",
       "That is the power of machine learning and NLP!\n",
       "\n",
       "How well do you think this model will generalize to a completely different set text examples such as product reviews?\n",
       "Do people use the same words to describe things they like in movie and product reviews such as electronics and household goods? \n",
       "Probably not.\n",
       "But it's a good idea to check the robustness of your language models by running it against challenging text from a different domain.\n",
       "And by testing your model on new domains, you can get ideas for more examples and datasets to use in your training and test sets.\n",
       "\n",
       "First you need to load the product reviews.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> products = pd.read_csv('https://proai.org/product-reviews.csv.gz')\n",
       ">>> for text in products['text']:\n",
       "...     bags_of_words.append(Counter(casual_tokenize(text)))\n",
       ">>> df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
       ">>> df_product_bows = df_product_bows.fillna(0).astype(int)\n",
       ">>> df_all_bows = df_bows.append(df_product_bows)\n",
       ">>> df_all_bows.columns  # <1>\n",
       "Index(['!', '\"', '#', '#38', '$', '%', '&', ''', '(', '(8',\n",
       "       ...\n",
       "       'zoomed', 'zooming', 'zooms', 'zx', 'zzzzzzzzz', '~', '½', 'élan',\n",
       "       '–', '’'],\n",
       "      dtype='object', length=23302)\n",
       ">>> df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]  # <2>\n",
       ">>> df_product_bows.shape\n",
       "(3546, 20756)\n",
       "\n",
       ">>> df_bows.shape  # <3>\n",
       "(10605, 20756)\n",
       "----\n",
       "<1> Your new bags of words have some tokens that were not in the original bags of words DataFrame (23302 columns now instead of 20756 before).\n",
       "<2> You need to make sure your new product DataFrame of bags of words has the exact same columns (tokens) in the exact same order as the original one used to train your Naive Bayes model.\n",
       "<3> The movie bags of words have the same size vocabulary (columns) as for products.\n",
       "\n",
       "Now you need to convert the labels to mimic the binary classification data that you trained your model on.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> products['senti_ispos'] = (products['sentiment'] > 0).astype(int)\n",
       ">>> products['pred_ispos'] = nb.predict(df_product_bows).astype(int)\n",
       ">>> products.head()\n",
       "    id  sentiment                                               text  senti_ispos\n",
       "0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...                     0\n",
       "1  1_2      -0.15  repost from january 13, 2004 with a better fit...                     0\n",
       "2  1_3      -0.20  does your apex dvd player only play dvd audio ...                     0\n",
       "3  1_4      -0.10  or does it play audio and video but scrolling ...                     0\n",
       "4  1_5      -0.50  before you try to return the player or waste h...                     0\n",
       "\n",
       ">>> tp = products['pred_ispos'] == products['senti_ispos']  # <1>\n",
       ">>> tp.sum() / len(products)\n",
       "0.5572476029328821\n",
       "----\n",
       "<1> True positive predictions are when both the predicted and true sentiment are positive\n",
       "\n",
       "\n",
       "So your Naive Bayes model does a  poor job of predicting whether a product review is positive (thumbs up).\n",
       "One reason for this subpar performance is that your vocabulary from the `casual_tokenize` product texts has 2546 tokens that were not in the movie reviews.\n",
       "That is about 10% of the tokens in your original movie review tokenization, which means that all those words will not have any weights or scores in your Naive Bayes model.\n",
       "Also the Naive Bayes model does not deal with negation as well as VADER does.\n",
       "You would need to incorporate _n_-grams into your tokenizer to connect negation words (such as \"not\" or \"never\") to the positive words they might be used to qualify.\n",
       "\n",
       "We leave it to you to continue the NLP action by improving on this machine learning model.\n",
       "And you can check your progress relative to VADER at each step of the way to see if you think machine learning is a better approach than hard-coding algorithms for NLP.\n",
       "\n",
       "== Review\n",
       "\n",
       ". How does a lemmatizer increase the likelihood that your DuckDuckGo search results contain what you are looking for?\n",
       ". Is there a way to optimally decide the _n_ in the _n_-gram range you use to tokenize your documents?\n",
       ". Does lemmatization, case folding, or stopword removal help or hurt your performance on a model to predict misleading news articles with this Kaggle dataset:\n",
       ". How could your find out the best sizes for the word pieces or sentence pieces for your tokenizer?\n",
       ". Is there a website where you can download the token frequencies for most of the words and n-grams ever published?footnote:[Hint: A company that aspired to \"do no evil\", but now does, created this massive NLP corpus.]\n",
       ". What are the risks and possible benefits of pair coding AI assistants built with NLP? What sort of organizations and algorithms do you trust with your mind and your code?\n",
       "\n",
       "=== Summary\n",
       "\n",
       "* You implemented tokenization and configured a tokenizer for your application.\n",
       "* _n_-gram tokenization helps retain some of the \"word order\" information in a document.\n",
       "* Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\n",
       "* Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\n",
       "* Stop words can contain useful information, and discarding them is not always helpful."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65b7f23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35433"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "202d209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7587.366167023555"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc) / 4.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f831578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 ms ± 854 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "nlp.pipe_names\n",
    "nlp = spacy.load('en_core_web_sm', disable=nlp.pipe_names)\n",
    "%timeit nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50b09019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d382138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17dd13a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 761 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d474bbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33109"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcacb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e72f23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('https://proai.org/nlpia2-ch2.adoc').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b93e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= Tokens of thought (natural language words)\\n:chapter: 2\\n:part: 1\\n:imagesdir: .\\n:xrefstyle: short\\n:figure-caption: Figure {chapter}.\\n:listing-caption: Listing {chapter}.\\n:table-caption: Table {chapter}.\\n:stem: latexmath\\n\\nThis chapter covers\\n\\n* Parsing your text into words and _n_-grams (tokens)\\n* Tokenizing punctuation, emoticons, and even Chinese characters\\n* Consolidating your vocabulary with stemming, lemmatization, and case folding\\n* Building a structured numerical representation of natural language text\\n* Scoring text for sentiment and prosocial intent\\n* Using character frequency analysis to optimize your token vocabulary\\n* Dealing with variable length sequences of words and tokens\\n\\n\\nSo you want to help save the world with the power of natural language processing (NLP)?\\nFirst your NLP pipeline will need to compute something about text, and for that you\\'ll need a way to represent text in a numerical data structure.\\nThe part of an NLP pipeline that breaks up your text to create this structured numerical data is called a _parser_.\\nFor many NLP applications, you only need to convert your text to a sequence of words, and that can be enough for searching and classifying text.\\n\\nYou will now learn how to split a document, any string, into discrete tokens of meaning.\\nYou will be able to parse text documents as small as a single word and as large as an entire Encyclopedia.\\nAnd they will all produce a consistent representation that you can use to compare them.\\nFor this chapter your tokens will be words, punctuation marks, and even pictograms such as Chinese characters, emojis and emoticons.\\n\\nLater in the book you will see that you can use these same techniques to find packets of meaning in any discrete sequence.\\nFor example, your tokens could be the ASCII characters represented by a sequence of bytes, perhaps with ASCII emoticons.\\nOr they could be Unicode emojis, mathematical symbols, Egyption, hieroglyphics, pictographs from languages like Kanji  and Cantonese.\\nYou could even define the tokens for DNA and RNA sequences with letters for each of the five base nucleotides: adenine (A), guanine (G), cytosine \\\\(C), thymine (T), and uracil (U).\\nNatural language sequences of tokens are all around you ... and even inside you.\\n\\nIs there something you can do with tokens that doesn\\'t require a lot of complicated deep learning?\\nIf you have a good tokenizer you can use it to identify statistics about the occurrence of tokens in a set of documents, such as your blog posts or a business website.\\nThen you can build a search engine in pure Python with just a dictionary to represent to record links to the set of documents where those words occur.\\nThat Python dictionary that maps words to document links or pages is called a reverse index.\\nIt\\'s just like the index at the back of this book.\\nThis is called _information retrieval_ -- a really powerful tool in your NLP toolbox.\\n\\nStatistics about tokens are often all you need for keyword detection, full text search, and information retrieval.\\nYou can even build customer support chatbots using text search to find answers to customers\\' questions in your documentation or FAQ (frequently asked question) lists.\\nA chatbot can\\'t answer your questions until it knows where to look for the answer.\\nSearch is the foundation of many state of the art applications such as conversational AI and open domain question answering.\\nA tokenizer forms the foundation for almost all NLP pipelines.\\n\\n=== Tokens of emotion\\n\\nAnother practical use for your tokenizer is called _sentiment analysis_, or analysis of text to estimate emotion.\\nYou\\'ll see an example of a sentiment analysis pipeline later in this chapter.\\nFor now you just need to know how to build a tokenizer.\\nAnd your tokenizer will almost certainly need to handle the tokens of emotion called _emoticons_ and _emojis_.\\n\\n_Emoticons_ are a textual representations of a writer\\'s mood or facial expression, such as the _smiley_ emoticon: `:-)`.\\nThey are kind-of like a modern hieroglyph or picture-word for computer users that only have access to an ASCII terminal for communication.\\n_Emojis_ are the graphical representation of these characters.\\nFor example, the smilie emoji has a small yellow circle with two black dots for eyes and a U shaped curve for a mouth.\\nThe smiley emoji is a graphical representation of the `:-)` smiley emoticon.\\n\\nBoth emojis and emoticons have evolved into their own language.\\nThere are hundreds of popular emojis.\\nPeople have created emojis for everything from company logos to memes and innuendo.\\nNoncommercial social media networks such Mastodon even allow you to create your own custom emojis.footnote:[Mastodon servers you can join (https://proai.org/mastoserv)] footnote:[Mastodon custom emoji documentation (https://docs.joinmastodon.org/methods/custom_emojis/)] \\n\\n.Emojis and Emoticons\\n[NOTE]\\n====\\n_Emoticons_ were first typed into an ASCII text message in 1972 when Carnegie Mellon researchers mistakenly understood a text message about a mercury spill to be a joke.\\nThe professor, Dr. Scott E. Fahlman, suggested that `:-)` should be appended to messages that were jokes, and  `:-(` emoticons should be used for serious warning messages. \\nGosh, how far we\\'ve come.\\n====\\n\\nThe plural of \"emoji\" is either \"emoji\" (like \"sushi\") or \"emojis\" (like \"Tsunamis\"), however the the Atlantic and NY Times style editors prefer \"emojis\" to avoid ambiguity.\\nYour NLP pipeline will learn what you mean no matter how you type it.\\n\\nimage::../images/ch02/wikipedia-smiley-icon.svg[alt=\"Smiley icon from wikipedia article en.wikipedia.org/wiki/Smiley\",align=\"center\",width=100%,link=\"../images/ch02/wikipedia-smiley-icon.svg\"]\\n\\n== What is a token?\\n\\nA token can be almost any chunk of text that you want to treat as a packet of thought and emotion.\\nSo you need to break your text into chunks that capture individual thoughts.\\nYou may be thinking that _words_ are the obvious choice for tokens.\\nSo that\\'s what you will start with here.\\nYou\\'ll also learn how to include punctuation marks, emojis, numbers, and other word-like things in your vocabulary of words.\\nLater you\\'ll see that you can use these same techniques to find packets of meaning in any discrete sequence.\\nAnd later you will learn some even more powerful ways to split discrete sequences into meaningful packets.\\nYour tokenizers will be soon able to analyze and structure any text document or string, from a single word, to a sentence, to an entire book.\\n\\nThink about a collection of documents, called a _corpus_, that you want to process with NLP.\\nThink about the _vocabulary_ that would be important to your NLP algorithm -- the set of tokens you will need to keep track of.\\nFor example your tokens could be the characters for ASCII emoticons, if this is what is important in your NLP pipeline for a particular corpus.\\nOr your tokens could be Unicode emojis, mathematical symbols, hieroglyphics, even pictographs like Kanji and Cantonese characters.\\nYour tokenizer and your NLP pipeline would even be useful for the nucleotide sequences of DNA and RNA where your tokens might be A, C, T, G, U, and so on.\\nAnd neuroscientists sometimes create sequences of discrete symbols to represent neurons firing in your brain when you read text like this sentence.\\nNatural language sequences of tokens are inside you, all around you, and flowing through you.\\nSoon you\\'ll be flowing streams of tokens through your machine learning NLP pipeline.\\n\\nRetrieving tokens from a document will require some string manipulation beyond just the `str.split()` method employed in chapter 1.\\nYou\\'ll probably want to split contractions like \"you\\'ll\" into the words that were combined to form them, perhaps \"you\" and \"\\'ll\", or perhaps \"you\" and \"will.\"\\nYou\\'ll want to separate punctuation from words, like quotes at the beginning and end of quoted statements or words, such as those in the previous sentence.\\nAnd you need to treat some punctuation such as dashes (\"-\") as part of singly-hyphenated compound words such as \"singly-hyphenated.\"\\n\\nOnce you have identified the tokens in a document that you would like to include in your vocabulary, you will return to the regular expression toolbox to build a tokenizer.\\nAnd you can use regular expressions combine different forms of a word into a single token in your vocabulary -- a process called _stemming_.\\nThen you will assemble a vector representation of your documents called a _bag of words_.\\nFinally, you will try to use this bag of words vector to see if it can help you improve upon the basic greeting recognizer at the end of chapter 1.\\n\\n=== Alternative tokens\\n\\nWords aren\\'t the only packets of meaning we could use for our tokens.\\nThink for a moment about what a word or token represents to you.\\nDoes it represent a single concept, or some blurry cloud of concepts?\\nCould you always be sure to recognize where a word begins and ends?\\nAre natural language words like programming language keywords that have precise spellings, definitions and grammatical rules for how to use them?\\nCould you write software that reliably recognizes a word?\\n\\nDo you think of \"ice cream\" as one word or two?\\nOr maybe even three?\\nAren\\'t there at least two entries in your mental dictionary for \"ice\" and \"cream\" that are separate from your entry for the compound word \"ice cream\"?\\nWhat about the contraction \"don\\'t\"?\\nShould that string of characters be split into one, or two, or even three packets of meaning?\\n\\nYou might even want to divide words into even smaller meaningful parts.\\nWord pieces such as the prefix \"pre\", the suffix \"fix\", or the interior syllable \"la\" all have meaning.\\nYou can use these word pieces to transfer what you learn about the meaning of one word to another similar word in your vocabulary.\\nYour NLU pipeline can even use these pieces to understand new words.\\nAnd your NLG pipeline can use the pieces to create new words that succinctly capture ideas or memes circulating in the collective consciousness.\\n\\nYour pipeline could break words into even smaller pieces.\\nLetters, characters, or graphemes footnote:[(https://en.wikipedia.org/wiki/Grapheme)] carry sentiment and meaning too!footnote:[Suzi Park and Hyopil Shin _Grapheme-level Awareness in Word Embeddings for Morphologically Rich Languages_ (https://www.aclweb.org/anthology/L18-1471.pdf)]\\nWe haven\\'t yet found the perfect encoding for packets of thought.\\nAnd machines compute differently than brains.\\nWe explain language and concepts to each other in terms of words or terms.\\nBut machines can often see patterns in the use of characters that we miss.\\nAnd for machines to be able to squeeze huge vocabularies into their limited RAM there are more efficient encodings for natural language.\\n\\nThe optimal tokens for efficient computation are different from the packets of thought (words) that we humans use.\\nByte Pair Encoding (BPE), Word Piece Encoding, and Sentence Piece Encoding, each can help machines use natural language more efficiently.\\nBPE finds the optimal groupings of characters (bytes) for your particular set of documents and strings.\\nIf you want an *explainable* encoding, use the word tokenizers of the previous sections.\\nIf you want more flexible and accurate predictions and generation of text, then BPE, WPE, or SPE may be better for your application.\\nLike the bias variance trade-off, there\\'s often a explainability/accuracy trade-off in NLP.\\n\\nWhat about invisible or implied words?\\nCan you think of additional words that are implied by the single-word command \"Don\\'t!\"?\\nIf you can force yourself to think like a machine and then switch back to thinking like a human, you might realize that there are three invisible words in that command.\\nThe single statement \"Don\\'t!\" means \"Don\\'t you do that!\" or \"You, do not do that!\"\\nThat\\'s at least three hidden packets of meaning for a total of five tokens you\\'d like your machine to know about.\\n\\nBut don\\'t worry about invisible words for now.\\nAll you need for this chapter is a tokenizer that can recognize words that are spelled out.\\nYou will worry about implied words and connotation and even meaning itself in chapter 4 and beyond.footnote:[If you want to learn more about exactly what a \"word\" really is, check out the introduction to _The Morphology of Chinese_ by Jerome Packard where he discusses the concept of a \"word\" in detail. The concept of a \"word\" did not exist at all in the Chinese language until the 20th century when it was translated from English grammar into Chinese.]\\n\\nYour NLP pipeline can start with one of these five options as your tokens:\\n\\n1. **Bytes** - ASCII characters\\n2. **Characters** - multi-byte Unicode characters\\n3. **Subwords** (Word pieces) - syllables and common character clusters\\n4. **Words** - dictionary words or their roots (stems, lemmas)\\n5. **Sentence pieces** - short, common word and multi-word pieces\\n\\nAs you work your way down this list your vocabulary size increases and your NLP pipeline will need more and more data to train.\\nCharacter-based NLP pipelines are often used in translation problems or NLG tasks that need to generalize from a modest number of examples.\\nThe number of possible words that your pipeline can deal with is called its _vocabulary_.\\nA character-based NLP pipeline typically needs fewer than 200 possible tokens to process many Latin-based languages.\\nThat small vocabulary ensures that byte- and character-based NLP pipelines can handle new unseen test examples without too many meaningless OOV (out of vocabulary) tokens.\\n\\nFor word-based NLP pipelines your pipeline will need to start paying attention to how often tokens are used before deciding whether to \"count it.\"\\nYou don\\'t want you pipeline to do anything meaningful with junk words such `asdf` - the \\nBut even if you make sure your pipeline on pays attention to words that occur a lot, you could end up with a vocabulary that\\'s as large as a typical dictionary - 20 to 50 thousand words.\\n\\nSubwords are the optimal token to use for most Deep Learning NLP pipelines.\\nSubword (Word piece) tokenizers are built into many state of the art transformer pipelines.\\nWords are the token of choice for any linguistics project or academic research where your results need to be interpretable and explainable.\\n\\nSentence pieces take the subword algorithm to the extreme.\\nThe sentence piece tokenizer allows your algorithm to combine multiple word pieces together into a single token that can sometimes span multiple words.\\nThe only hard limit on sentence pieces is that they do not extend past the end of a sentence.\\nThis ensures that the meaning of a token is associated with only a single coherent thought and is useful on single sentences as well as longer documents.W\\n\\n==== _N_-grams\\n\\nNo matter which kind of token you use for your pipeline, you will likely extract pairs, triplets, quadruplets, and even quintuplets of tokens.\\nThese are called _n_-grams_.footnote:[Pairs of adjacent words are called 2-grams or bigrams. Three words in sequency are called 3-grams or trigrams. Four words in a row are called 4-grams.  5-grams are probably the longest _n_-grams you\\'ll find in an NLP pipeline. Google counts all the 1 to 5-grams in nearly all the books ever written (https://books.google.com/ngrams).]\\nUsing _n_-grams enables your machine to know about the token \"ice cream\" as well as the individual tokens \"ice\" and \"cream\" that make it up.\\nAnother 2-gram that you\\'d like to keep together is \"Mr. Smith\".\\nYour tokens and your vector representation of a document will likely want to have a place for \"Mr. Smith\" along with \"Mr.\" and \"Smith.\"\\n\\nYou will start with a short list of keywords as your vocabulary.\\nThis helps to keep your data structures small and understandable and can make it easier to explain your results.\\nExplainable models create insights that you can use to help your stakeholders, hopefully the users themselves (rather than investors), accomplish their goals.\\n\\nFor now, you can just keep track of all the short _n_-grams of words in your vocabulary.\\nBut in chapter 3, you will learn how to estimate the importance of words based on their document frequency, or how often they occur.\\nThat way you can filter out pairs and triplets of words that rarely occur together.\\nYou will find that the approaches we show are not perfect.\\nFeature extraction can rarely retain all the information content of the input data in any machine learning pipeline.\\nThat is part of the art of NLP, learning when your tokenizer needs to be adjusted to extract more or different information from your text for your particular applications.\\n\\nIn natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\\nNonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\\nThe techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email technology from a flood of spam that almost made it useless.\\n\\n== Challenges (a preview of stemming)\\n\\nAs an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\\nVery smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\\nImagine how difficult that is.\\nImagine trying to remove verb endings like \"ing\" from \"ending\" so you would have a stem called \"end\" to represent both words.\\nAnd you would like to stem the word \"running\" to \"run,\" so those two words are treated the same.\\nAnd that is tricky because you have removed not only the \"ing\" but also the extra \"n.\"\\nBut you want the word \"sing\" to stay whole.\\nYou would not want to remove the \"ing\" ending from \"sing\" or you would end up with a single-letter \"s.\"\\n\\nOr imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens.\"\\nDo isolated individual letters in a word or parts of a word provide any information at all about that word\\'s meaning?\\nCan the letters be misleading?\\nYes and yes.\\n\\nIn this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\\nLater, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you are interested in.\\nFrom that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\\n\\n=== Tokenization\\n\\nIn NLP, _tokenization_ is a particular kind of document _segmentation_.\\nSegmentation breaks up text into smaller chunks or segments.\\nThe segments of text have less information than the whole.\\nDocuments can be segmented into paragraphs, paragraphs into sentences, sentences into phrases, and phrases into tokens (usually words and punctuation).\\nIn this chapter, we focus on segmenting text into _tokens_ with a _tokenizer_.\\n\\nYou may have heard of tokenizers before.\\nIf you took a computer science class you likely learned about how programming language compilers work.\\nA tokenizer that is used to compile computer languages is called a _scanner_ or _lexer_.\\nIn some cases your computer language parser can work directly on the computer code and doesn\\'t need a tokenizer at all.\\nAnd for natural language processing, the only parser typically outputs a vector representation, rather than  If the tokenizer functionality is not separated from the compiler, the parser is often called a scannerless _parser_.\\n\\nThe set of valid tokens for a particular computer language is called the _vocabulary_ for that language, or more formally its _lexicon_.\\nLinguistics and NLP researchers use the term \"lexicon\" to refer to a set of natural language tokens.\\nThe term \"vocabulary\" is the more natural way to refer to a set of natural language words or tokens.\\nSo that\\'s what you will use here.\\n\\nThe natural language equivalent of a computer language compiler is a natural language parser.\\nA natural language tokenizer is called a _scanner_, or _lexer_, or _lexical analyzer_ in the computer language world.\\nModern computer language compilers combine the _lexer_ and _parser_ into a single lexer-parser algorithm.\\nThe vocabulary of a computer language is usually called a _lexicon_.\\nAnd computer language compilers sometimes refer to tokens as _symbols_.\\n\\nHere are five important NLP terms.\\nAlong side them are some roughly equivalent terms used in computer science when talking about programming language compilers:\\n\\n* _tokenizer_ -- scanner, lexer, lexical analyzer\\n* _vocabulary_ -- lexicon\\n* _parser_ -- compiler\\n* _token_, _term_, _word_, or _n-gram_ -- token or symbol\\n* _statement_ -- statement or expression\\n\\nTokenization is the first step in an NLP pipeline, so it can have a big impact on the rest of your pipeline.\\nA tokenizer breaks unstructured data, natural language text, into chunks of information which can be counted as discrete elements.\\nThese counts of token occurrences in a document can be used directly as a vector representing that document.\\nThis immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning.\\nThese counts can be used directly by a computer to trigger useful actions and responses.\\nOr they might also be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\\nThe most common use for bag-of-words vectors created this way is for document retrieval, or search.\\n\\n== Your tokenizer toolbox\\n\\nSo each application you encounter you will want to think about which kind of tokenizer is appropriate for your application.\\nAnd once you decide which kinds of tokens you want to try, you\\'ll need to configure a python package for accomplishing that goal.\\n\\nYou can chose from several tokenizer implementations: footnote:[Lysandre explains the various tokenizer options in the Huggingface documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\\n\\n. Python: `str.split`, `re.split`\\n. NLTK: `PennTreebankTokenizer`, `TweetTokenizer`\\n. spaCy: state of the art tokenization is its reason for being\\n. Stanford CoreNLP: linguistically accurate, requires Java interpreter\\n. Huggingface: `BertTokenizer`, a `WordPiece` tokenizer\\n\\n=== The simplest tokenizer\\n\\nThe simplest way to tokenize a sentence is to use whitespace within a string as the \"delimiter\" of words. In Python, this can be accomplished with the standard library method `split`, which is available on all `str` object instances as well as on the `str` built-in class itself.\\n\\nLet\\'s say your NLP pipeline needs to parse quotes from WikiQuote.org, and it\\'s having trouble with one titled _The Book Thief_.footnote:[Markus Zusak, _The Book Thief_, p. 85 (https://en.wikiquote.org/wiki/The_Book_Thief)]\\n\\n\\n[[book_thief_sentence_split_py]]\\n.Example quote from _The Book Thief_ split into tokens\\n[source,python]\\n----\\n>>> text = (\"Trust me, though, the words were on their way, and when \"\\n...         \"they arrived, Liesel would hold them in her hands like \"\\n...         \"the clouds, and she would wring them out, like the rain.\")\\n>>> tokens = text.split()\\n>>> tokens[:8]\\n[\\'Trust\\', \\'me,\\', \\'though,\\', \\'the\\', \\'words\\', \\'were\\', \\'on\\', \\'their\\']\\n----\\n\\n\\n.Tokenized phrase\\nimage::../images/ch02/book-thief-split.png[alt=\"Trust|me,|though,|the|words|were|on|their\",align=\"center\",width=100%,link=\"../images/ch02/book-thief-split.png\"]\\n\\nAs you can see, this built-in Python method does an OK job of tokenizing this sentence.\\nIts only \"mistake\" is to include commas within the tokens.\\nThis would prevent your keyword detector from detecting quite a few important tokens: `[\\'me\\', \\'though\\', \\'way\\', \\'arrived\\', \\'clouds\\', \\'out\\', \"rain\"]`.\\nThose words \"clouds\" and \"rain\" are pretty important to the meaning of this text.\\nSo you\\'ll need to do a bit better with your tokenizer to ensure you can catch all the important words and \"hold\" them like Liesel.\\n\\n=== Rule-based tokenization\\n\\nIt turns out there is a simple fix to the challenge of splitting punctuation from words.\\nYou can use a regular expression tokenizer to create rules to deal with common punctuation patterns.\\nHere\\'s just one particular regular expression you could use to deal with punctuation \"hanger-ons.\"\\nAnd while we\\'re at it, this regular expression will be smart about words that have internal punctuation, such as possessive words and contractions that contain apostrophes.\\n\\nYou\\'ll use a regular expression to tokenize some text from the book _Blindsight_ by Peter Watts.\\nThe text describes how the most _adequate_ humans tend to survive natural selection (and alien invasions).footnote:[Peter Watts, Blindsight, (https://rifters.com/real/Blindsight.htm)]\\nThe same goes for your tokenizer.\\nYou want to find an _adequate_ tokenizer that solves your problem, not the perfect tokenizer.\\nYou probably can\\'t even guess what the _right_ or _fittest_ token is.\\nYou will need an accuracy number to evaluate your NLP pipeline with and that will tell you which tokenizer should survive your selection process.\\nThe example here should help you start to develop your intuition about applications for regular expression tokenizers.\\n\\n[source,python]\\n----\\n>>> import re\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'  # <1>\\n>>> texts = [text]\\n>>> texts.append(\"There\\'s no such thing as survival of the fittest. \"\\n...              \"Survival of the most adequate, maybe.\")\\n>>> tokens = list(re.findall(pattern, texts[-1]))\\n>>> tokens[:8]\\n[\"There\\'s\", \\'no\\', \\'such\\', \\'thing\\', \\'as\\', \\'survival\\', \\'of\\', \\'the\\']\\n>>> tokens[8:16]\\n[\\'fittest\\', \\'.\\', \\'Survival\\', \\'of\\', \\'the\\', \\'most\\', \\'adequate\\', \\',\\']\\n>>> tokens[16:]\\n[\\'maybe\\', \\'.\\']\\n----\\n<1> The _look-ahead_ pattern `(?:\\\\\\'\\\\w+)?` detects whether or not the word contains a single apostrophe followed by 1 or more letters.footnote:[Thank you Wiktor Stribiżew (https://stackoverflow.com/a/43094210/623735).]\\n\\nMuch better.\\nNow the tokenizer separates punctuation from the end of a word, but doesn\\'t break up words that contain internal punctuation such as the apostrophe within the token \"There\\'s.\"\\nSo all of these words were tokenized the way we wanted: \"There\\'s\", \"fittest\", \"maybe\".\\nAnd this regular expression tokenizer will work fine on contractions even if they have more than one letter after the apostrophe such as \"can\\'t\", \"she\\'ll\", \"what\\'ve\".\\nIt will work even typos such as \\'can\"t\\' and \"she,ll\", and \"what`ve\".\\nBut this liberal matching of internal punctuation probably isn\\'t what you want if your text contains rare double contractions such as \"couldn\\'t\\'ve\", \"ya\\'ll\\'ll\", and \"y\\'ain\\'t\"\\n\\n[TIP]\\n=====\\nPro tip: You can accommodate double-contractions with the regular expression `r\\'\\\\w+(?:\\\\\\'\\\\w+){0,2}|[^\\\\w\\\\s]\\'`\\n=====\\n\\nThis is the main idea to keep in mind.\\nNo matter how carefully you craft your tokenizer, it will likely destroy some amount of information in your raw text.\\nAs you are cutting up text, you just want to make sure the information you leave on the cutting room floor isn\\'t necessary for your pipeline to do a good job.\\nAlso, it helps to think about your downstream NLP algorithms.\\nLater you may configure a case folding, stemming, lemmatizing, synonym substitution, or count vectorizing algorithm.\\nWhen you do, you\\'ll have to think about what your tokenizer is doing, so your whole pipeline works together to accomplish your desired output.\\n\\n\\n////\\n// too much regex detail?\\n\\n==== How regular expressions work\\n\\nHere is how the regular expression in <<listing_2_7>> works.\\n\\nThe square brackets (`[` and `]`) are used to indicate a _character class_, a set of characters.\\nThe plus sign after the closing square bracket (`]`) means that a match must contain one or more of the characters inside the square brackets.\\nThe `\\\\s` within the character class is a shortcut to a predefined character class that includes all whitespace characters like those created when you press the `[space]`, `[tab]`, and `[return]` keys.\\nThe character class `r\\'[\\\\s]\\'` is equivalent to `r\\'[ \\\\t\\\\r\\\\n\\\\f]\\'`.\\nThe six whitespace characters are space (`\\' \\'`), tab (`\\'\\\\t\\'`), return (`\\'\\\\r\\'`), newline  (`\\'\\\\n\\'`), and form-feed (`\\'\\\\f\\'`).\\n\\nYou did not use any character ranges here, but you may want to later.\\nA character range is a special kind of character class indicated within square brackets and a hyphen like `r\\'[a-z]\\'` to match all lowercase letters.\\nThe character range `r\\'[0-9]\\'` matches any digit 0 through 9 and is equivalent to `r\\'[0123456789]\\'`).\\nThe regular expression `r\\'[\\\\_a-zA-Z]\\'` would match any underscore character (`r\\'\\\\_\\'`) or letter of the English alphabet (upper or lower case).\\n\\nThe hyphen (`-`) right after the opening square bracket is a bit of quirk of regexes.\\nYou cannot put a hyphen just anywhere inside your square brackets because the regex parser may think you mean a character range like `r\\'[0-9]\\'`.\\nSo whenever you want to indicate an actual hyphen (dash) character in your character class, you need to make sure it is the first character after the open square bracket, or you need to escape it with a backslash (`\\\\`).\\n\\nThe `re.split` function goes through each character in the input string (the second argument, `sentence`) left to right looking for any matches based on the \"program\" or \"pattern\" in the regular expression (the first argument, `r\\'[-\\\\s.,;!?]+\\'`).\\nWhen it finds a match, it breaks the string right before that matched character and right after it, skipping over the matched character or characters.\\nSo the `re.split` line will work just like `str.split`, but it will work for any kind of character or multicharacter sequence that matches your regular expression.\\n\\nThe parentheses (`(` and `)`) are used to group regular expressions just like they are used to group mathematical, Python, and most other programming language expressions.\\nThese parentheses force the regular expression to match the entire expression within the parentheses before moving on to try to match the characters that follow the parentheses.\\n\\n// TODO: TMI?\\n////\\n\\nTake a look at the first few tokens in your lexographically sorted vocabulary for this short text:\\n\\n[source,python]\\n----\\n>>> import numpy as np  # <1>\\n>>> vocab = sorted(set(tokens))  # <2>\\n>>> \\' \\'.join(vocab[:12])  # <3>\\n\", . Survival There\\'s adequate as fittest maybe most no of such\"\\n>>> num_tokens = len(tokens)\\n>>> num_tokens\\n18\\n>>> vocab_size = len(vocab)\\n>>> vocab_size\\n15\\n----\\n<1> `str.split()` is your quick-and-dirty tokenizer.\\n<2> Coercing the `list` into a `set` ensures that your vocabulary contains only *unique* tokens that you want to keep track of.\\n<3> Sorted lexographically (lexically) so punctuation comes before letters, and capital letters come before lowercase letters.\\n\\nYou can see how you may want to consider lowercasing all your tokens so that \"Survival\" is recognized as the same word as \"survival\".\\nAnd you may want to have a synonym substitution algorithm to replace \"There\\'s\" with \"There is\" for similar reasons.\\nHowever, this would only work if your tokenizer kept contraction and possessive apostrophes attached to their parent token.\\n\\n[TIP]\\n=====\\nMake sure you take a look at your vocabulary whenever it seems your pipeline isn\\'t working well for a particular text.\\nYou may need to revise your tokenizer to make sure it can \"see\" all the tokens it needs to do well for your NLP task.\\n=====\\n\\n\\n=== SpaCy\\n\\nMaybe you don\\'t want your regular expression tokenizer to keep contractions together.\\nPerhaps you\\'d like to recognize the word \"isn\\'t\" as two separate words, \"is\" and \"n\\'t\".\\nThat way you could consolidate the synonyms \"n\\'t\" and \"not\" into a single token.\\nThis way your NLP pipeline would understand \"the ice cream isn\\'t bad\" to mean the same thing as \"the ice cream is not bad\".\\nFor some applications, such as full text search, intent recognition, and sentiment analysis, you want to be able to *uncontract* or expand contractions like this.\\nBy splitting contractions, you can use synonym substitution or contraction expansion to improve the recall of your search engine and the accuracy of your sentiment analysis.\\n\\n[IMPORTANT]\\n====\\nWe\\'ll discuss case folding, stemming, lemmatization, and synonym substitution later in this chapter.\\nBe careful about using these techniques for applications such as authorship attribution, style transfer, or text fingerprinting.\\nYou want your authorship attribution or style-transfer pipeline to stay true to the author\\'s writing style and the exact spelling of words that they use.\\n====\\n\\nSpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.\\nIn fact the name \"spaCy\" is based on the word \"space\", as in the separator used in Western languages to separate words.\\nAnd spaCy adds a lot of additional _tags_ to tokens at the same time that it is applying rules to split tokens apart.\\nSo spaCy is often the first and last tokenizer you\\'ll ever need to use.\\n\\nLet\\'s see how spaCy handles our collection of deep thinker quotes:\\n\\n[source,python]\\n----\\n>>> import spacy  # <1>\\n>>> nlp = spacy.load(\\'en_core_web_sm\\')  # <2>\\n>>> doc = nlp(texts[-1])\\n>>> type(doc)\\n<class \\'spacy.tokens.doc.Doc\\'>\\n\\n>>> tokens = [tok.text for tok in doc]\\n>>> tokens[:9]\\n[\\'There\\', \"\\'s\", \\'no\\', \\'such\\', \\'thing\\', \\'as\\', \\'survival\\', \\'of\\', \\'the\\']\\n\\n>>> tokens[9:17]\\n[\\'fittest\\', \\'.\\', \\'Survival\\', \\'of\\', \\'the\\', \\'most\\', \\'adequate\\', \\',\\']\\n----\\n<1> If this is your first time to use spacy you should download the small language model with `spacy.cli.download(\\'en_core_web_sm\\')`\\n<2> `sm` stands for \"small\" (17 MB), `md` is medium (45 MB), `lg` is \"large\" (780 MB)\\n\\nThat tokenization may be more useful to you if you\\'re comparing your results to academic papers or colleagues at work.\\nSpacy is doing a lot more under the hood.\\nThat small language model you downloaded is also identifying sentence breaks with some *sentence boundary detection* rules.\\nA language model is a collection of regular expressions and finite state automata (rules).\\nThese rules are a lot like the grammar and spelling rules you learned in English class.\\nThey are used in the algorithms that tokenize and label your words with useful things like their part of speech and their position in a syntax tree of relationships between words.\\n\\n[source,python]\\n----\\n>>> from spacy import displacy\\n>>> sentence = list(doc.sents)[0] # <1>\\n>>> displacy.serve(sentence, style=\"dep\")\\n>>> !firefox 127.0.0.1:5000\\n\\n----\\n<1> The first sentence begins with \"There\\'s no such thing...\"\\n\\nIf you browse to your `localhost` on port 5000 you should see a sentence diagram that may be even more correct than what you could produce in school:\\n\\nimage::../images/ch02/there-such-thing.png[alt=\"NOUN Survival -> ADV maybe. ADJ adequate -> ADV most\",align=\"center\",width=100%,link=\"../images/ch02/there-such-thing.png\"]\\n\\nYou can see that spaCy does a lot more than simply separate text into tokens.\\nIt identifies sentence boundaries to automatically segment your text into sentences.\\nAnd it tags tokens with various attributes like their part of speech (PoS) and even their role within the syntax of a sentence.\\nYou can see the lemmas displayed by `displacy`  beneath the literal text for each token.footnote:[nlpia2 source code for chapter 2 (https://proai.org/nlpia2-ch2) has additional spaCy and displacy options and examples.]\\nLater in the chapter we\\'ll explain how lemmatization and case folding and other vocabulary *compression* approaches can be helpful for some applications.\\n\\nSo spaCy seems pretty great in terms of accuracy and some \"batteries included\" features, such as all those token tags for lemmas and dependencies.\\nWhat about speed?\\n\\n=== Tokenizer race\\n\\nSpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.\\nFirst download the AsciiDoc text file for this chapter:\\n\\n[source,python]\\n----\\n>>> import requests\\n>>> text = requests.get(\\'https://proai.org/nlpia2-ch2.adoc\\').text\\n>>> f\\'{round(len(text) / 10_000)}0k\\'\\n\\'160k\\'\\n----\\n<1> I divided by 10,000 and rounded it, so that Doctests would continue to pass as I revise this text.\\n\\nThere were about 160 thousand ASCII characters in this AsciiDoc file where I wrote this sentence that you are reading right now.\\nWhat does that mean in terms of words-per-second, the standard benchmark for tokenizer speed?\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\\'en_core_web_sm\\')\\n>>> %timeit nlp(text)  # <1>\\n4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\\n\\n>>> f\\'{round(len(text) / 10_000)}0k\\'\\n\\'160k\\'\\n>>> doc = nlp(text)\\n>>> f\\'{round(len(list(doc)) / 10_000)}0k\\'\\n\\'30k\\'\\n>>> f\\'{round(len(doc) / 1_000 / 4.67)}kWPS\\'  # <2> \\n\\'7kWPS\\'\\n----\\n<1> `%timeit` is a magic function within `jupyter notebook`, `jupyter console` or `ipython`\\n<2> kWPS is for thousands of words (tokens) per second\\n\\nThat\\'s nearly 5 seconds for about 150,000 characters or 34,000 words of English and Python text or about 7000 words per second.\\n\\nThat may seem fast enough for you on your personal projects.\\nBut on a medical records summarization project we needed to process thousands of large documents with a comparable amount of text as you find in this entire book.\\nAnd the latency in our medical record summarization pipeline was a critical metric for the project.\\nSo this, full-featured spaCy pipeline would require at least 5 days to process 10,000 books such as NLPIA or typical medical records for 10,000 patients.\\n\\nIf that\\'s not fast enough for your application you can disable any of the tagging features of the spaCy pipeline that you do not need.\\n\\n[source,python]\\n----\\n>>> nlp.pipe_names  # <1>\\n[\\'tok2vec\\', \\'tagger\\', \\'parser\\', \\'attribute_ruler\\', \\'lemmatizer\\', \\'ner\\']\\n>>> nlp = spacy.load(\\'en_core_web_sm\\', disable=nlp.pipe_names)\\n>>> %timeit nlp(text)\\n199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\\n----\\n<1> The `pipe_names` lists all the currently enabled elements of your spaCy `nlp` pipeline\\n\\nYou can disable the pipeline elements you don\\'t need to speed up the tokenizer:\\n\\n- `tok2vec`: word embeddings\\n- `tagger`: part-of-speech (`.pos` and `.pos_`)\\n- `parser`: syntax tree role\\n- `attribute_ruler`: fine-grained POS and other tags\\n- `lemmatizer`: lemma tagger\\n- `ner`: named entity recognition tagger\\n\\nNLTK\\'s `word_tokenize` method is often used as the pace setter in tokenizer benchmark speed comparisons:\\n\\n[source,python]\\n----\\n>>> import nltk\\n>>> nltk.download(\\'punkt\\')\\nTrue\\n>>> from nltk.tokenize import word_tokenize\\n>>> %timeit word_tokenize(text)\\n156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\\n>>> tokens = word_tokenize(text)\\n>>> f\\'{round(len(tokens) / 10_000)}0k\\'\\n\\'30k\\'\\n----\\n\\nCould it be that you found a winner for the tokenizer race?\\nNot so fast.\\nYour regular expression tokenizer has some pretty simple rules, so it should run pretty fast as well:\\n\\n[source,python]\\n----\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'\\n>>> tokens = re.findall(pattern, text)  # <1>\\n>>> f\\'{round(len(tokens) / 10_000)}0k\\'\\n\\'30k\\'\\n>>> %timeit re.findall(pattern, text)\\n8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\\n----\\n<1> Try precompiling with `re.compile` to learn something about how smart the core Python developers are\\n\\nNow that\\'s not surprising.\\nRegular expressions can be compiled and run very efficiently within low level C routines in Python.\\n\\n[TIP]\\n================\\nUse a regular expression tokenizer when speed is more import than accuracy.\\nIf you do not need the additional linguistic tags that spaCy and other pipelines provide your tokenizer doesn\\'t need to waste time trying to figure out those tags.footnote:[Andrew Long, \"Benchmarking Python NLP Tokenizers\" (https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5)]\\nAnd each time you use a regular expression in the `re` or `regex` packages, a compiled and optimized version of it is cached in RAM.\\nSo there\\'s usually no need to _precompile_ (using `re.compile()`) your regexes.\\n================\\n\\n== Wordpiece tokenizers\\n\\nIt probably felt natural to think of words as indivisible atomic chunks of meaning and thought.\\nHowever, you did find some words that didn\\'t clearly split on spaces or punctuation.\\nAnd many compound words or named entities that you\\'d like to keep together have spaces within them.\\nSo it can help to dig a little deeper and think about the statistics of what makes a word.\\nThink about how we can build up words from neighboring characters instead of cleaving text at separators such as spaces and punctuation.\\n\\n=== Clumping characters into sentence pieces\\n\\nInstead of thinking about breaking strings up into tokens, your tokenizer can look for characters that are used a lot right next to each other, such as \"i\" before \"e\".\\nYou can pair up characters and sequences of characters that belong together.footnote:[In many applications the term \"_n_-gram\" refers to character _n_-grams rather than word n-grams. For example the leading relational database PostgreSQL has a Trigram index which tokenizes your text into character 3-grams not word 3-grams. In this book, we use \"_n_-gram\" to refer to sequences of word grams and \"character _n_-grams\" when talking about sequences of characters.]\\nThese clumps of characters can become your tokens.\\nAn NLP pipeline only pays attention to the statistics of tokens.\\nAnd hopefully these statistics will line up with our expectations for what a word is.\\n\\nMany of these character sequences will be whole words, or even compound words, but many will be pieces of words.\\nIn fact, all _subword tokenizers_ maintain a token within the vocabulary for every individual character in your vocabulary.\\nThis means it never needs to use an OOV (Out-of-Vocabulary) token, as long as any new text doesn\\'t contain any new characters it hasn\\'t seen before.\\nSubword tokenizers attempt to optimally clump characters together to create tokens.\\nUsing the statistics of character n-gram counts it\\'s possible for these algorithms to identify wordpieces and even sentence pieces that make good tokens.\\n\\nIt may seem odd to identify words by clumping characters.\\nBut to a machine, the only obvious, consistent division between elements of meaning in a text is the boundary between bytes or characters.\\nAnd the frequency with which characters are used together can help the machine identify the meaning associated with subword tokens such as individual syllables or parts of compound words.\\n\\nIn English, even individual letters have subtle emotion (sentiment) and meaning (semantics) associated with them.\\nHowever, there are only 26 unique letters in the English language.\\nThat doesn\\'t leave room for individual letters to _specialize_ on any one topic or emotion.\\nNonetheless savvy marketers know that some letters are cooler than others.\\nBrands will try to portray themselves as technologically advanced by choosing names with exotic letters like \"Q\" and \"X\" or \"Z\".\\nThis also helps with SEO (Search Engine Optimization) because rarer letters are more easily found among the sea of possible company and product names.\\nYour NLP pipeline will pick up all these hints of meaning, connotation, and intent.\\nYour token counters will provide the machine with the statistics it needs to infer the meaning of clumps of letters that are used together often.\\n\\nThe only disadvantage for subword tokenizers is the fact that they must pass through your corpus of text many times before converging on an optimal vocabulary and tokenizer.\\nA subword tokenizer has to be trained or fit to your text just like a CountVectorizer.\\nIn fact you\\'ll use a CountVectorizer in the next section to see how subword tokenizers work.\\n\\nThere are two main approaches to subword tokenization: BPE (Byte-Pair Encoding) and Wordpiece tokenization.\\n\\n==== BPE\\n\\nIn the previous edition of the book we insisted that words were the smallest unit of meaning in English that you need consider.\\nWith the rise of Transformers and other deep learning models that use BPE and similar techniques, we\\'ve changed our minds.footnote:[Hannes and Cole are probably screaming \"We told you so!\" as they read this.]\\nCharacter-based subword tokenizers have proven to be more versatile and robust for most NLP problems.\\nBy building up a vocabulary from building blocks of Unicode multi-byte characters you can construct a vocabulary that can handle every possible natural language string you\\'ll ever see, all with a vocabulary of as few as 50,000 tokens.\\n\\nYou may think that Unicode characters are the smallest packet of meaning in natural language text.\\nTo a human, maybe, but to a machine, no way.\\nJust as the BPE name suggests, characters don\\'t have to be your fundamental atom of meaning for your _base vocabulary_.\\nYou can split characters into 8-bit bytes.\\nGPT-2 uses a byte-level BPE tokenizer to naturally compose all the unicode characters you need from the bytes that make them up.\\nThough some special rules are required to handle unicode punctuation within a byte-based vocabulary, no other adjustment to the character-based BPE algorithm is required.\\nA byte-level BPE tokenizer allows you to represent all possible texts with a base (minimum) vocabulary size of 256 tokens.\\nThe GPT-2 model can achieve state-of-the-art performance with it\\'s default BPE vocabulary of only 50,000 multibyte _merge tokens_ plus 256 individual byte tokens.\\n\\nYou can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker or the hub in a social network.\\nIt connects characters together that appear next to each other a lot.\\nIt then creates a new token for these character combinations.\\nAnd it keeps doing this until it has a many frequently used character sequences as you\\'ve allowed in your vocabulary size limit.\\n\\n\\nBPE is transforming the way we think about natural language tokens.\\nNLP engineers are finally letting the data do the talking.\\nStatistical thinking is better than human intuition when building an NLP pipeline.\\nA machine can see how _most_ people use language.\\nYou are only familiar with what _you_ mean when you use particular words or syllables.\\nTransformers have now surpassed human readers and writers at some natural language understanding and generation tasks, including finding meaning in subword tokens.\\n\\nOne complication you have not yet encounter is the dilemma of what to do when you encounter a new word.\\nIn the previous examples, we just keep adding new words to our vocabulary.\\nBut in the real world your pipeline will have been trained on an initial corpus of documents that may or may not represent all the kinds of tokens it will ever see.\\nIf your initial corpus is missing some of the words that you encounter later on, you will not have a slot in your vocabulary to put your counts of that new word.\\nSo when you train you initial pipeline, you will always reserve a slot (dimension) to hold the counts of your _out-of-vocabulary_ (OOV) tokens.\\nSo if your original set of documents did not contain the girl\\'s name \"Aphra\", all counts of the name Aphra would be lumped into the OOV dimension as counts of Amandine and other rare words.\\n\\nTo give Aphra equal representation in your vector space, you can use BPE.\\nBPE breaks down rare words into smaller pieces to create a _periodic table_ of the elements for natural language in your corpus.\\nSo, because \"aphr\" is a common english prefix, your BPE tokenizer would probably give Aphra *two* slots for her counts in your vocabulary: one for \"aphr\" and one for \"a\".\\nActually, you might actually discover that the vobcabulary slots are for \" aphr\" and \"a \", because BPE keeps track of spaces no differently than any other character in your alphabet.footnote:[Actually, the string representation of tokens used for BPE and Wordpiece tokenizer place marker characters at the beginning or end of the token string indicate the absence of a word boundary (typically a space or punctuation). So you may see the \"aphr##\" token in your BPE vocabulary for the prefix \"aphr\" in aphrodesiac (https://stackoverflow.com/a/55416944/623735)]\\n\\nBPE gives you multilingual flexibility to deal with Hebrew names like Aphra.\\nAnd it give your pipeline robustness against common misspellings and typos, such as \"aphradesiac.\"\\nEvery word, including minority 2-grams such as \"African American\", have representation in the voting system of BPE.footnote:[Discriminatory voting restriction laws have recently been passed in US: (https://proai.org/apnews-wisconsin-restricts-blacks)]\\nGone are the days of using the kluge of OOV (Out-of-Vocabulary) tokens to handle the rare quirks of human communication.\\nBecause of this, state of the art deep learning NLP pipelines such as transformers all use word piece tokenization similar to BPE.footnote:[See chapter 12 for information about another similar tokenizer -- sentence piece tokenizer]\\n\\nBPE preserves some of the meaning of new words by using character tokens and word-piece tokens to spell out any unknown words or parts of words.\\nFor example, if \"syzygy\" is not in our vocabulary, we could represent it as the six tokens \"s\", \"y\", \"z\", \"y\", \"g\", and \"y\".\\nPerhaps \"smartz\" could be represented as the two tokens \"smart\" and \"z\".\\n\\nThat sounds smart.\\nLet\\'s see how it works on our text corpus:\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> from sklearn.feature_extraction.text import CountVectorizer\\n>>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer=\\'char\\')\\n>>> vectorizer.fit(texts)\\nCountVectorizer(analyzer=\\'char\\', ngram_range=(1, 2))\\n\\n>>> bpevocab = vectorizer.get_feature_names()\\n>>> bpevocab[:7]\\n[\\' \\', \\' a\\', \\' c\\', \\' f\\', \\' h\\', \\' i\\', \\' l\\']\\n----\\n\\nWe configured the `CountVectorizer` to split the text into all the possible character 1-grams and 2-grams found in the texts.\\nAnd `CountVectorizer` organizes the vocabulary in lexical order, so n-grams that start with a space character (`\\' \\'`) come first.\\nOnce the vectorizer knows what tokens it needs to be able to count, it can transform text strings into vectors, with one dimension for every token in your character n-gram vocabulary.\\n\\n[source,python]\\n----\\n>>> vectors = vectorizer.transform(texts)\\n>>> df = pd.DataFrame(vectors.todense(), columns=bpevocab)\\n>>> df.index = [t[:8] + \\'...\\' for t in texts]\\n>>> df = df.T\\n>>> df[\\'total\\'] = df.T.sum()\\n>>> df\\n    Trust me...  There\\'s ...  total\\n             31           14     45\\n a            3            2      5\\n c            1            0      1\\n f            0            1      1\\n h            3            0      3\\n..          ...          ...    ...\\nwr            1            0      1\\ny             2            1      3\\ny             1            0      1\\ny,            1            0      1\\nyb            0            1      1\\n<BLANKLINE>\\n[148 rows x 3 columns]\\n----\\n\\nThe DataFrame contains a column for each sentence and a row for each character 2-gram.\\nCheck out the top four rows where the byte pair (character 2-gram) of \" a\" is seen to occur five times in these two sentences.\\nSo even spaces count as \"characters\" when you\\'re building a BPE tokenizer.\\nThis is one of the advantages of BPE, it will figure out what your token delimiters are, so it will work even in languages where there is no whitespace between words.\\nAnd BPE will work on substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13 characters forward.\\n\\n[source,python]\\n----\\n>>> df.sort_values(\\'total\\').tail()\\n        Trust me...  There\\'s ...  total\\n    he           10            3     13\\n    h            14            5     19\\n    t            11            9     20\\n    e            18            8     26\\n                 31           14     45\\n----\\n\\nA BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent vocabulary.\\nOver time it deletes the less frequent character pairs as it gets less and less likely that they won\\'t come up a lot more later in your text.\\n\\n----\\n>>> df[\\'n\\'] = [len(tok) for tok in bpevocab]\\n>>> df[df[\\'n\\'] > 1].sort_values(\\'total\\').tail()\\n    Trust me...  There\\'s ...  total  n\\n,             6            1      7  2\\ne             7            2      9  2\\n t            8            3     11  2\\nth            8            4     12  2\\nhe           10            3     13  2\\n----\\n\\nSo the next round of preprocessing in the BPE tokenizer would retain the character 2-grams \"he\" and \"th\" and even \" t\" and \"e \".\\nThen the BPE algorithm would make another pass through the text with this smaller character bigram vocabulary.\\nIt would look for frequent pairings of these character bigrams with each other and individual characters.\\nThis process would continue until the maximum number of tokens is reached and the longest possible character sequences have been incorporated into the vocabulary.\\n\\n[NOTE]\\n====\\nYou may see mention of _wordpiece_ tokenizers which are used within some advanced language models such as `BERT` and its derivatives.footnote:[Lysandre Debut explains all the variations on subword tokenizers in the Hugging Face transformers documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\\nIt works the same as BPE, but it actually uses the underlying language model to predict the neighboring characters in string.\\nIt eliminates the characters from its vocabulary that hurt the accuracy of this language model the least.\\nThe math is subtly different and it produces subtly different token vocabularies, but you don\\'t need to select this tokenizer intentionally.\\nThe models that use it will come with it built into their pipelines.\\n====\\n\\nOne big challenge of BPE-based tokenizers is that they must be trained on your individual corpus.\\nSo BPE tokenizers are usually only used for Transformers and Large Language Models (LLMs) which you will learn about in chapter 9.\\n\\nAnother challenge of BPE tokenizers is all the book keeping you need to do to keep track of which trained tokenizer goes with each of your trained models.\\nThis was one of the big innovations of Huggingface.\\nThey made it easy to store and share all the preprocessing data, such as the tokenizer vocabulary, along side the language model.\\nThis makes it easier to reuse and share BPE tokenizers. \\nIf you want to become an NLP expert, you may want to imitate what they\\'ve done at HuggingFace with your own NLP preprocessing pipelines.footnote:[Huggingface documentation on tokenizers (https://huggingface.co/docs/transformers/tokenizer_summary)]\\n\\n== Vectors of tokens\\n\\nNow that you have broken your text into tokens of meaning, what do you do with them?\\nHow can you convert them to numbers that will be meaningful to the machine?\\nThe simplest most basic thing to do would be to detect whether a particular token you are interested in was present or not.\\nYou could hard-code the logic to check for important tokens, called a _keywords_.\\n\\nThis might work well for your greeting intent recognizer in chapter 1.\\nOur greeting intent recognizer at the end of chapter 1 looked for words like \"Hi\" and \"Hello\" at the beginning of a text string.\\nYour new tokenized text would help you detect the presence or absence of words such as \"Hi\" and \"Hello\" without getting confused by words like \"Hiking\" and \"Hell.\"\\nWith your new tokenizer in place, your NLP pipeline wouldn\\'t misinterpret the word \"Hiking\" as the greeting \"Hi king\":\\n\\n[source,python]\\n----\\n>>> hi_text = \\'Hiking home now\\'\\n>>> hi_text.startswith(\\'Hi\\')\\nTrue\\n>>> pattern = r\\'\\\\w+(?:\\\\\\'\\\\w+)?|[^\\\\w\\\\s]\\'  # <1>\\n>>> \\'Hi\\' in re.findall(pattern, hi_text)  # <2>\\nFalse\\n>>> \\'Hi\\' == re.findall(pattern, hi_text)[0]  # <3>\\nFalse\\n----\\n<1> You can reuse the regular expression pattern from earlier to create a one-line tokenizer\\n<2> \\'Hi\\' is not among the 3 words (tokens) in this phrase\\n<3> \\'Hi\\' is definitely not the first word in this phrase\\n\\nSo tokenization can help you reduce the number of false positives in your simple intent recognition pipeline that looks for the presence of greeting words.\\nThis is often called keyword detection, because your vocabulary of words is limited to a set of words you think are important.\\nHowever, it\\'s quite cumbersome to have to think of all the words that might appear in a greeting in order to recognize them all, including slang, misspellngs and typoos.\\nAnd creating a for loop to iterate through them all would be inefficient.\\nWe can use the math of linear algebra and the vectorized operations of `numpy` to speed this process up.\\n\\nIn order to detect tokens efficiently you will want to use three new tricks:\\n\\n. matrix and vector representations of documents\\n. vectorized operations in numpy\\n. indexing of discrete vectors\\n\\nYou\\'ll first learn the most basic, direct, raw and lossless way to represent words as a matrix, one-hot encoding.\\n\\n=== One-hot Vectors\\n\\nNow that you\\'ve successfully split your document into the kinds of words you want, you\\'re ready to create vectors out of them.\\nVectors of numbers are what we need to do the math or processing of NL*P* on natural language text.\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> onehot_vectors = np.zeros(\\n...     (len(tokens), vocab_size), int)  # <2>\\n>>> for i, word in enumerate(tokens):\\n...     onehot_vectors[i, vocab.index(word)] = 1  # <3>\\n>>> df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\\n>>> df_onehot.shape\\n(18, 15)\\n>>> df_onehot.iloc[:,:8].replace(0, \\'\\')  # <4>\\n    ,  .  Survival  There\\'s  adequate  as  fittest  maybe\\n0                       1\\n1\\n2\\n3\\n4                                   1\\n5\\n6\\n7\\n8                                           1\\n9      1\\n10              1\\n11\\n12\\n13\\n14                               1\\n15  1\\n16                                                1\\n17     1\\n----\\n<2> The table is as wide as your count of unique vocabulary terms and as tall as the length of your document: 18 rows, 15 columns\\n<3> For each token in the sentence, mark the column for it with a `1`.\\n<4> For brevity we\\'re only showing the first 8 columns of the DataFrame and replaced 0\\'s with \\'\\'.\\n\\nIn this representation of this two-sentence quote, each row is a vector representation of a single word from the text.\\nThe table has the 15 columns because this is the number of unique words in your vocabulary.\\nThe table has 18 rows, one for each word in the document.\\nA \"1\" in a column indicates a vocabulary word that was present at that position in the document.\\n\\nYou can \"read\" a one-hot encoded (vectorized) text from top to bottom.\\nYou can tell that the first word in the text was the word \"There\\'s\", because the `1` on the first row is positioned under the column label \"There\\'s\".\\nThe next three rows (row indexes 1, 2, and 3) are blank, because we\\'ve truncated the table on the right to help it fit on the page.\\nThe fifth row of the text, with the 0-offset index number of `4` shows us that the fifth word in the text was the word \"adequate\", because there\\'s a `1` in that column.\\n\\nOne-hot vectors are super-sparse, containing only one nonzero value in each row vector.\\nFor display, this code replaces the `0`\\'s with empty strings (`\\'\\'`), to make it easier to read.\\nBut the code did not actually alter the `DataFrame` of data you are processing in your NLP pipeline.\\nThe Python code above was just to to make it easier to read, so you can see that it looks a bit like a player piano paper roll, or maybe a music box drum.\\n\\nThe Pandas `DataFrame` made this output a little easier to read and interpret.\\nThe `DataFrame.columns` keep track of labels for each column.\\nThis allows you to label each column in your table with a string, such as the token or word it represents.\\nA `DataFrame` can also keep track of labels for each row in an the `DataFrame.index`, for speedy lookup.\\n\\n[IMPORTANT]\\n====\\nDon\\'t add strings to any `DataFrame` you intend to use in your machine learning pipeline.\\nThe purpose of a tokenizer and vectorizer, like this one-hot vectorizer, is to create a numerical array that your NLP pipeline can do math on.\\nYou can\\'t do math on strings.\\n====\\n\\nEach row of the table is a binary row vector, and you can see why it\\'s also called a one-hot vector: all but one of the positions (columns) in a row are `0` or blank.\\nOnly one column, or position in the vector is \"hot\" (\"1\").\\nA one (`1`) means on, or hot.\\nA zero (`0`) mean off, or absent.\\n\\nOne nice feature of this vector representation of words and tabular representation of documents is that no information is lost.\\nThe exact sequence of tokens is encoded in the order of the one-hot vectors in the table representing a document.\\nAs long as you keep track of which words are indicated by which column, you can reconstruct the original sequence of tokens from this table of one-hot vectors perfectly.\\nAnd this reconstruction process is 100% accurate even though your tokenizer was only 90% accurate at generating the tokens you thought would be useful.\\nAs a result, one-hot word vectors like this are typically used in neural nets, sequence-to-sequence language models, and generative language models.\\nThey are a good choice for any model or NLP pipeline that needs to retain all the meaning inherent in the original text.\\n\\n[TIP]\\n====\\nThe one-hot encoder (vectorizer) did not discard any information from the text, but our tokenizer did.\\nOur regular expression tokenizer discarded the whitespace characters (`\\\\s`) that sometimes occur between words.\\nSo you could not perfectly reconstruct the original text with a _detokenizer_.\\nTokenizers like spaCy, however, keep track of these whitespace characters and can in fact detokenize a sequence of tokens perfectly.\\nSpaCy was named for this feature of accurately accounting for white-*space* efficiently and accurately.\\n====\\n\\nThis sequence of one-hot vectors is like a digital recording of the original text.\\nIf you squint hard enough you might be able to imagine that the matrix of ones and zeros above is a player piano roll.footnote:[See the \"Player piano\" article on Wikipedia (https://en.wikipedia.org/wiki/Player_piano).].\\nOr maybe it\\'s the bumps on the metal drum of a music box.footnote:[See the web page titled \"Music box - Wikipedia\" (https://en.wikipedia.org/wiki/Music_box).]\\nThe vocabulary key at the top tells the machine which \"note\" or word to play for each row in the sequence of words or piano music.\\n\\n[[player_piano_roll_jpg]]\\n.Player piano roll\\nimage::../images/ch02/piano_roll.jpg[Player piano music roll with parallel sequences of holes running vertically down the paper. The holes meander left and right to indicate the rising and falling of the tones in the melody of a song. Image licensed from Wikimedia CC BY-SA 3.0 (https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg),width=100%,align=\"center\",link=\"https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg\"]\\n\\nUnlike a player-piano or a music box, your mechanical word recorder and player is only allowed to use one \"finger\" at a time.\\nIt can only play one \"note\" or word at a time.\\nIt\\'s one-hot.\\nAnd there is no variation in the spacing of the words.\\n\\nThe important thing is that you\\'ve turned a sentence of natural language words into a sequence of numbers, or vectors.\\nNow you can have the computer read and do math on the vectors just like any other vector or list of numbers.\\nThis allows your vectors to be input into any natural language processing pipeline that requires this kind of vector.\\nThe Deep Learning pipelines of chapter 5 through 10 typically require this representation, because they can be designed to extract \"features\" of meaning from these raw representations of text.\\nAnd Deep Learning pipelines can generate text from numerical representations of meaning.\\nSo the stream of words emanating from your NLG pipelines in later chapters will often be represented by streams of one-hot encoded vectors, just like a player piano might play a song for a less artificial audience in West World.footnote:[West World is a television series about particularly malevolent humans and human-like robots, including one that plays a piano in the main bar.]\\n\\nNow all you need to do is figure out how to build a \"player piano\" that can _understand_ and combine those word vectors in new ways.\\nUltimately, you\\'d like your chatbot or NLP pipeline to play us a song, or say something, you haven\\'t heard before.\\nYou\\'ll get to do that in chapters 9 and 10 when you learn about recurrent neural networks that are effective for sequences of one-hot encoded tokens like this.\\n\\nThis representation of a sentence in one-hot word vectors retains all the detail, grammar, and order of the original sentence.\\nAnd you have successfully turned words into numbers that a computer can \"understand.\"\\nThey are also a particular kind of number that computers like a lot: binary numbers.\\nBut this is a big table for a short sentence.\\nIf you think about it, you have expanded the file size that would be required to store your document.\\nFor a long document this might not be practical.\\n\\nHow big is this *lossless* numerical representation of your collection of documents?\\nYour vocabulary size (the length of the vectors) would get huge.\\nThe English language contains at least 20,000 common words, millions if you include names and other proper nouns.\\nAnd your one-hot vector representation requires a new table (matrix) for every document you want to process.\\nThis is almost like a raw \"image\" of your document.\\nIf you have done any image processing, you know that you need to do dimension reduction if you want to extract useful information from the data.\\n\\nLet\\'s run through the math to give you an appreciation for just how big and unwieldy these \"piano rolls\" are.\\nIn most cases, the vocabulary of tokens you\\'ll use in an NLP pipeline will be much more than 10,000 or 20,000 tokens.\\nSometimes it can be hundreds of thousands or even millions of tokens.\\nLet\\'s assume you have a million tokens in your NLP pipeline vocabulary.\\nAnd let\\'s say you have a meager 3000 books with 3500 sentences each and 15 words per sentence -- reasonable averages for short books.\\nThat\\'s a whole lot of big tables (matrices), one for each book.\\nThat would use 157.5 terabytes.\\nYou probably couldn\\'t even store that on disk.\\n\\nThat is more than a million million bytes, even if you are super-efficient and use only one byte for each number in your matrix.\\nAt one byte per cell, you would need nearly 20 terabytes of storage for a small bookshelf of books processed this way.\\nFortunately you do not ever use this data structure for storing documents.\\nYou only use it temporarily, in RAM, while you are processing documents one word at a time.\\n\\nSo storing all those zeros, and recording the order of the words in all your documents does not make much sense.\\nIt is not practical.\\nAnd it\\'s not very useful.\\nYour data structure hasn\\'t abstracted or generalized from the natural language text.\\nAn NLP pipeline like this doesn\\'t yet do any real feature extraction or dimension reduction to help your machine learning work well in the real world.\\n\\nWhat you really want to do is compress the meaning of a document down to its essence.\\nYou would like to compress your document down to a single vector rather than a big table.\\nAnd you are willing to give up perfect \"recall.\"\\nYou just want to capture most of the meaning (information) in a document, not all of it.\\n\\n=== BOW (Bag-of-Words) Vectors\\n\\nIs there any way to squeeze all those _player piano music rolls_ into a single vector?\\nVectors are a great way to represent any object.\\nWith vectors we could compare documents to each other just be checking the Euclidian distance between them.\\nVectors allow us to use all your linear algebra tools on natural language.\\nAnd that\\'s really the goal of NLP, doing math on text.\\n\\nLet us assume you can ignore the order of the words in our texts.\\nFor this first cut at a vector representation of text you can just jumble them all up together into a \"bag,\" one bag for each sentence or short document.\\nIt turns out just knowing what words are present in a document can give your NLU pipeline a lot of information about what\\'s in it.\\nThis is in fact the representation that power big Internet search engine companies.\\nEven for documents several pages long, a bag-of-words vector is useful for summarizing the essence of a document.\\n\\nLet\\'s see what happens when we jumble and count the words in our text from _The Book Thief_:\\n\\n[source,python]\\n----\\n>>> bow = sorted(set(re.findall(pattern, text)))\\n>>> bow[:9]\\n[\\',\\', \\'.\\', \\'Liesel\\', \\'Trust\\', \\'and\\', \\'arrived\\', \\'clouds\\', \\'hands\\', \\'her\\']\\n>>> bow[9:19]\\n[\\'hold\\', \\'in\\', \\'like\\', \\'me\\', \\'on\\', \\'out\\', \\'rain\\', \\'she\\', \\'the\\', \\'their\\']\\n>>> bow[19:27]\\n[\\'them\\', \\'they\\', \\'though\\', \\'way\\', \\'were\\', \\'when\\', \\'words\\', \\'would\\']\\n----\\n\\nEven with this jumbled up bag of words, you can get a general sense that this sentence is about:  \"Trust\", \"words\", \"clouds\", \"rain\", and someone named \"Liesel\".\\nOne thing you might notice is that Python\\'s `sorted()` puts punctuation before characters, and capitalized words before lowercase words.\\nThis is the ordering of characters in the ASCII and Unicode character sets.\\nHowever, the order of your vocabulary is unimportant.\\nAs long as you are consistent across all the documents you tokenize this way, a machine learning pipeline will work equally well with any vocabulary order.\\n\\nYou can use this new bag-of-words vector approach to compress the information content for each document into a data structure that is easier to work with.\\nFor keyword search, you could **OR** your one-hot word vectors from the player piano roll representation into a binary bag-of-words vector.\\nIn the play piano analogy this is like playing several notes of a melody all at once, to create a \"chord\".\\nRather than \"replaying\" them one at a time in your NLU pipeline, you would create a single bag-of-words vector for each document.\\n\\nYou could use this single vector to represent the whole document in a single vector.\\nBecause vectors all need to be the same length, your BOW vector would need to be as long your vocabulary size which is the number of unique tokens in your documents.\\nAnd you could ignore a lot of words that would not be interesting as search terms or keywords.\\nThis is why stop words are often ignored when doing BOW tokenization.\\nThis is an extremely efficient representation for a search engine index or the first filter for an information retrieval system.\\nSearch indexes only need to know the presence or absence of each word in each document to help you find those documents later.\\n\\nThis approach turns out to be critical to helping a machine \"understand\" a collection of words as a single mathematical object.\\nAnd if you limit your tokens to the 10,000 most important words, you can compress your numerical representation of your imaginary 3500 sentence book down to 10 kilobytes, or about 30 megabytes for your imaginary 3000-book corpus.\\nOne-hot vector sequences for such a modest-sized corpus would require hundreds of gigabytes.\\n\\nAnother advantage of the BOW representation of text is that it allows you to find similar documents in your corpus in constant time (`O(1)`).\\nYou can\\'t get any faster than this.\\nBOW vectors are the precursor to a reverse index which is what makes this speed possible.\\nIn computer science and software engineering, you are always on the lookout for data structures that enable this kind of speed.\\nAll major full text search tools use BOW vectors to find what you\\'re looking for fast.\\nYou can see this numerical representation of natural language in EllasticSearch, Solr,footnote:[Apache Solr home page and Java source code (https://solr.apache.org/)] PostgreSQL, and even state of the art web search engines such as Qwant,footnote:[Qwant web search engine based in Europe (https://www.qwant.com/)], SearX,footnote:[SearX git repository (https://github.com/searx/searx) and web search (https://searx.thegpm.org/)], and Wolfram Alpha footnote:[(https://www.wolframalpha.com/)].\\n\\nFortunately, the words in your vocabulary are sparsely utilized in any given text.\\nAnd for most bag-of-words applications, we keep the documents short, sometimes just a sentence will do.\\nSo rather than hitting all the notes on a piano at once, your bag-of-words vector is more like a broad and pleasant piano chord, a combination of notes (words) that work well together and contain meaning.\\nYour NLG pipeline or chatbot can handle these chords even if there is a lot of \"dissonance\" from words in the same statement that are not normally used together.\\nEven dissonance (odd word usage) is useful information about a statement that a machine learning pipeline can make use of.\\n\\nHere is how you can put the tokens into a binary vector indicating the presence or absence of a particular word in a particular sentence.\\nThis vector representation of a set of sentences could be \"indexed\" to indicate which words were used in which document.\\nThis index is equivalent to the index you find at the end of many textbooks, except that instead of keeping track of which page a word occurs on, you can keep track of the sentence (or the associated vector) where it occurred.\\nWhereas a textbook index generally only cares about important words relevant to the subject of the book, you keep track of every single word (at least for now).\\n\\n==== Sparse representations\\n\\nYou might be thinking that if you process a huge corpus you\\'ll probably end up with thousands or even millions of unique tokens in your vocabulary.\\nThis would mean you would have to store a lot of zeros in your vector representation of our 20-token sentence about Liesel.\\nA `dict` would use much less memory than a vector.\\nAny paired mapping of words to their 0/1 values would be more efficient than a vector.\\nBut you can\\'t do math on `dict`\\'s.\\nSo this is why CountVectorizer uses a sparse numpy array to hold the counts of words in a word fequency vector.\\nUsing a dictionary or sparse array for your vector ensures that it only has to store a 1 when any one of the millions of possible words in your dictionary appear in a particular document.\\n\\nBut if you want to look at an individual vector to make sure everything is working correctly, a Pandas `Series` is the way to go.\\nAnd you will wrap that up in a Pandas DataFrame so you can add more sentences to your binary vector \"corpus\" of quotes.\\n\\n=== Dot product\\n\\n// TODO: some of this may belong in the discussion of keyword matching and one-hot vectors?\\nYou\\'ll use the dot product a lot in NLP, so make sure you understand what it is.\\nSkip this section if you can already do dot products in your head.\\n\\nThe dot product is also called the _inner product_ because the \"inner\" dimension of the two vectors (the number of elements in each vector) or matrices (the rows of the first matrix and the columns of the second matrix) must be the same because that is where the products happen.\\nThis is analogous to an \"inner join\" on two relational database tables.\\n\\nThe dot product is also called the _scalar product_ because it produces a single scalar value as its output.\\nThis helps distinguish it from the _cross product_, which produces a vector as its output.\\nObviously, these names reflect the shape of the symbols used to indicate the dot product (latexmath:[\\\\cdot]) and cross product (latexmath:[\\\\times]) in formal mathematical notation.\\nThe scalar value output by the scalar product can be calculated by multiplying all the elements of one vector by all the elements of a second vector and then adding up those normal multiplication products.\\n\\nHere is a Python snippet you can run in your Pythonic head to make sure you understand what a dot product is:\\n\\n[[example_dot_product_code]]\\n.Example dot product calculation\\n[source,python]\\n----\\n>>> v1 = pd.np.array([1, 2, 3])\\n>>> v2 = pd.np.array([2, 3, 4])\\n>>> v1.dot(v2)\\n20\\n>>> (v1 * v2).sum()  # <1>\\n20\\n>>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # <2>\\n20\\n----\\n<1> Multiplication of NumPy arrays is a \"vectorized\" operation that is very efficient.\\n<2> You should not iterate through vectors this way unless you want to slow down your pipeline.\\n\\n[TIP]\\n================\\nThe dot product is equivalent to the _matrix product_, which can be accomplished in NumPy with the `np.matmul()` function or the `@` operator. Since all vectors can be turned into Nx1 or 1xN matrices, you can use this shorthand operator on two column vectors (Nx1) by transposing the first one so their inner dimensions line up, like this: `v1.reshape((-1, 1)).T @ v2.reshape((-1, 1))`, which outputs your scalar product within a 1x1 matrix: `array([[20]])`\\n================\\n\\n// IDEA: Consider talking about BOW overlap to explain cosine similarity\\n\\nThis is your first vector space model of natural language documents (sentences).\\nNot only are dot products possible, but other vector operations are defined for these bag-of-word vectors: addition, subtraction, OR, AND, and so on.\\nYou can even compute things such as Euclidean distance or the angle between these vectors. This representation of a document as a binary vector has a lot of power.\\nIt was a mainstay for document retrieval and search for many years.\\nAll modern CPUs have hardwired memory addressing instructions that can efficiently hash, index, and search a large set of binary vectors like this.\\nThough these instructions were built for another purpose (indexing memory locations to retrieve data from RAM), they are equally efficient at binary vector operations for search and retrieval of text.\\n\\n\\n\\nNLTK and Stanford CoreNLP have been around the longest and are the most widely used for comparison of NLP algorithms in academic papers.\\nEven though the Stanford CoreNLP has a Python API, it relies on the Java 8 CoreNLP backend, which must be installed and configured separately.\\nSo if you want to publish the results of your work in an academic paper and compare it to what other researchers are doing, you may need to use NLTK.\\nThe most common tokenizer used in academia is the PennTreebank tokenizer:\\n\\n[source,python]\\n----\\n>>> from nltk.tokenize import TreebankWordTokenizer\\n>>> texts.append(\\n...   \"If conscience and empathy were impediments to the advancement of \"\\n...   \"self-interest, then we would have evolved to be amoral sociopaths.\"\\n...   )  # <1>\\n>>> tokenizer = TreebankWordTokenizer()\\n>>> tokens = tokenizer.tokenize(texts[-1])[:6]\\n>>> tokens[:8]\\n[\\'If\\', \\'conscience\\', \\'and\\', \\'empathy\\', \\'were\\', \\'impediments\\', \\'to\\', \\'the\\']\\n>>> tokens[8:16]\\n[\\'advancement\\', \\'of\\', \\'self-interest\\', \\',\\', \\'then\\', \\'we\\', \\'would\\', \\'have\\']\\n>>> tokens[16:]\\n[\\'evolved\\', \\'to\\', \\'be\\', \\'amoral\\', \\'sociopaths\\', \\'.\\']\\n----\\n<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield in _SuperCooperators_: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\\n\\n// IDEA: Diagram of Nowak quote with vertical bars breaking up sent into words\\n\\nThe spaCy Python library contains a natural language processing pipeline that includes a tokenizer.\\nIn fact, the name of the package comes from the words \"space\" and \"Cython\".\\nSpaCy was built using the Cython package to speed the tokenization of text, often using the *space* character (\" \") as the delimmiter.\\nSpaCy has become the *multitool* of NLP, because of its versatility and the elegance of its API.\\nTo use spaCy, you can start by creating an callable parser object, typically named `nlp`.\\nYou can customize your NLP pipeline by modifying the Pipeline elements within that parser object.\\n\\nAnd spaCy has \"batteries included.\"\\nSo even with the default smallest spaCy language model loaded, you can do tokenization and sentence segementation, plus *part-of-speech* and *abstract-syntax-tree* tagging -- all with a single function call.\\nWhen you call `nlp()` on a string, spaCy tokenizes the text and returns a `Doc` (document) object.\\nA `Doc` object is a container for the sequence of sentences and tokens that it found in the text.\\n\\n\\n// IDEA: example spacy code for tokenization\\n\\nThe spaCy package tags each token with their linguistic function to provide you with information about the text\\'s grammatical structure.\\nEach token object within a `Doc` object has attributes that provide these tags.\\n\\nFor example:\\n* `token.text` the original text of the word\\n* `token.pos_` grammatical part of speech tag as a human-readable string\\n* `token.pos`  integer for the grammar part of speech tag\\n* `token.dep_` indicates the tokens role in the syntactic dependency tree\\n* `token.dep`  integer corresponding to the syntactic dependency tree location\\n\\nThe `.text` attribute provides the original text for the token.\\nThis is what is provided when you request the __str__ representation of a token.\\nA spaCy `Doc` object is allowing you to detokenize a document object to recreate the entire input text. i.e., the relation between tokens\\nYou can use these functions to examine the text in more depth.\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\"en_core_web_sm\")\\n>>> text = \"Nice guys finish first.\"  # <1>\\n>>> doc = nlp(text)\\n>>> for token in doc:\\n>>>     print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\\nNice            ADJ       amod\\nguys            NOUN      nsubj\\nfinish          VERB      ROOT\\nfirst           ADV       advmod\\n.               PUNCT     punct\\n----\\n<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\\n\\n== Challenging tokens\\n\\nChinese, Japanese, and other pictograph languages aren\\'t limited to a small small number letters in alphabets used to compose tokens or words.\\nCharacters in these traditional languages look more like drawings and are called \"pictographs.\"\\nThere are many thousands of unique characters in the Chinese and Japanese languages.\\nAnd these characters are used much like we use words in alphabet-based languages such as English.\\nBut each Chinese character is usually not a complete word on its own.\\nA character\\'s meaning depends on the characters to either side.\\nAnd words are not delimited with spaces.\\nThis makes it challenging to tokenize Chinese text into words or other packets of thought and meaning. \\n\\nThe `jieba` package is a Python package you can use to segment traditional Chinese text into words.\\nIt supports three segmentation modes: 1) \"full mode\" for retrieving all possible words from a sentence, 2) \"accurate mode\" for cutting the sentence into the most accurate segments, 3) \"search engine mode\" for splitting long words into shorter ones, sort-of like splitting compound words or finding the roots of words in English.\\nIn the example below, the Chinese sentence \"西安是一座举世闻名的文化古城\" translates into \"Xi\\'an is a city famous world-wide for it\\'s ancient culture.\"\\nOr, a more compact and literal translation might be \"Xi\\'an is a world-famous city for her ancient culture.\"\\n\\nFrom a grammatical perspective, you can split the sentence into: 西安 (Xi\\'an), 是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective suffix), 文化 (culture), 古城 (ancient city).\\nThe character \"座\" is the quantifier meaning \"ancient\" that is normally used to modify the word \"city.\"\\nThe `accurate mode` in `jieba` causes it to segment the sentence this way so that you can correctly extract a precise interpretation of the text.\\n\\n.Jieba in accurate mode\\n[source,python]\\n----\\n>>> seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\") # <1>\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> the default mode for jieba is accurate or precise mode\\n\\nJieba\\'s accurate mode minimizes the total number of tokens or words.\\nThis gave you 7 tokens for this short\\nJieba attempts to keep as many possible characters together.\\nThis will reduce the false positive rate or type 1 errors for detecting boundaries between words.\\n\\nIn full mode, jieba will attempt to split the text into smaller words, and more of them.\\n\\n.Jieba in full mode\\n[source,python]\\n----\\n>>> import jieba\\n... seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # <1>\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世\\', \\'举世闻名\\', \\'闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> `cut_all==True` means \"full mode\"\\n\\nNow you can try search engine mode to see if it\\'s possible to break up these tokens even further:\\n\\n.Jieba in search engine mode\\n[source,python]\\n----\\n>>> seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")\\n>>> list(seg_list)\\n[\\'西安\\', \\'是\\', \\'一座\\', \\'举世\\', \\'闻名\\', \\'举世闻名\\', \\'的\\', \\'文化\\', \\'古城\\']\\n----\\n<1> Accurate mode is the default mode.\\n\\nUnfortunately later versions of Python (3.5+) aren\\'t supported by Jieba\\'s part-of-speech tagging model.\\n\\n[source,python]\\n----\\n>>> import jieba\\n>>> from jieba import posseg\\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\")\\n>>> jieba.enable_paddle()  # <1>\\n>>> words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\\n>>> list(words)\\n[pair(\\'西安\\', \\'ns\\'),\\n pair(\\'是\\', \\'v\\'),\\n pair(\\'一座\\', \\'m\\'),\\n pair(\\'举世闻名\\', \\'i\\'),\\n pair(\\'的\\', \\'uj\\'),\\n pair(\\'文化\\', \\'n\\'),\\n pair(\\'古城\\', \\'ns\\')]\\n----\\n<1> Activate paddle mode\\n\\nYou can find more information about jieba at (https://github.com/fxsjy/jieba).\\nSpaCy also contains Chinese language models that do a decent job of segmenting and tagging Chinese text.\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> spacy.cli.download(\"zh_core_web_sm\")  # <1>\\n>>> nlpzh = spacy.load(\"zh_core_web_sm\")\\n>>> doc = nlpzh(\"西安是一座举世闻名的文化古城\")\\n>>> [(tok.text, tok.pos_) for tok in doc]\\n[(\\'西安\\', \\'PROPN\\'),\\n (\\'是\\', \\'VERB\\'),\\n (\\'一\\', \\'NUM\\'),\\n (\\'座\\', \\'NUM\\'),\\n (\\'举世闻名\\', \\'VERB\\'),\\n (\\'的\\', \\'PART\\'),\\n (\\'文化\\', \\'NOUN\\'),\\n (\\'古城\\', \\'NOUN\\')]\\n----\\n<1> Only need download the Chinese (zh) language model if this is your first time processing Chinese text \\n\\nAs you may notice, spaCy provides slightly different tokenization and tagging, which is more attached to the original meaning of each word rather than the context of this sentence.\\n\\n=== A complicated picture\\n\\nUnlike English, there is no concept of stemming or lemmatization in pictographic languages such as Chinese and Japanese (Kanji).\\nHowever, there’s a related concept.\\nThe most essential building blocks of Chinese characters are called _radicals_.\\nTo better understand _radicals_, you must first see how Chinese characters are constructed.\\nThere are six types of Chinese characters: 1) pictographs, 2) pictophonetic characters, 3) associative compounds, 4) self-explanatory characters, 5) phonetic loan characters, and 6) mutually explanatory characters.\\nThe top four categories are the most important and encompass most Chinese characters.\\n\\n1. Pictographs (象形字)\\n2. Pictophonetic characters (形声字)\\n3. Associative compounds (会意字)\\n\\n==== 1. Pictographs (象形字)\\n\\n_Pictographs_ were created from images of real objects, such as the characters for 口 (mouth) and 门 (door).\\n\\n\\n==== 2. Pictophonetic characters (形声字)\\n\\n_Pictophonetic characters_ were created from a radical and a single Chinese character.\\nOne part represents its meaning and the other indicates its pronunciation.\\nFor example, 妈 (mā, mother) = 女 (female) + 马 (mǎ, horse).\\nSqueezing 女 into 马 gives 妈.\\nThe character 女 is the semantic radical that indicates the meaning of the character (female).\\n马 is a single character that has a similar pronunciation (mǎ).\\nYou can see that the character for mother (妈) is a combination of the characters for female an\\nThis is comparable to the English concept of homophones -- words that sound alike but mean completely different things.\\nBut in Chinese use additional characters to disambiguate homophones.\\nThe character for female\\n\\n==== 3. Associative compounds (会意字)\\n\\nAssociative compounds can be divided into two parts: one symbolizes the image, the other indicates the meaning.\\n\\nFor example, 旦 (dawn), the upper part (日) is the sun and the lower part (一) is like the horizon line.\\n\\n\\n==== Self-explanatory characters (指事字)\\n\\n\\nSelf-explanatory characters cannot be easily represented by an image, so they are shown by a single abstract symbol.\\nFor example, 上 (up), 下 (down).\\n\\nAs you can see, procedures like stemming and lemmatization are harder or impossible for many Chinese characters.\\nSeparating the parts of a character may radically ;) change its meaning.\\nAnd there\\'s not prescribed order or rule for combining radicals to create Chinese characters.\\n\\nNonetheless, some kinds of stemming are harder in English than they are in Chinese\\nFor example, automatically removing the pluralization from words like \"we\", \"us\", \"they\" and \"them\" is hard in English but straightforward in Chinese.\\nChinese uses inflection to construct the plural form of characters, similar to adding s to the end of English words.\\nIn Chinese the pluralization suffix character is 们.\\nThe character 朋友 (friend) becomes 朋友们 (friends).\\n\\nEven the characters for \"we/us\", \"they/them\", and \"y\\'all\" use the same pluralization suffix: 我们 (we/us), 他们 (they/them), 你们 (you).\\nBut in in English, you can remove the \\'ing\\' or \\'ed\\' from many verbs to get the root word.\\nHowever, in Chinese, verb conjugation uses an additional character in the front or the end to indicate tense.\\nThere\\'s no prescribed rule for verb conjugation.\\nFor example, examine the character 学 (learn), 在学 (learning), and 学过 (learned).\\nIn Chinese, you can also use a suffix 学 to denote an academic discipline, such as 心理学 (psychology) or 社会学 (sociology).\\nIn most cases, you want to keep the integrated Chinese character together rather than reducing it to its components.\\n\\nIt turns out this is a good rule of thumb for all languages.\\nLet the data do the talking.\\nDo not stem or lemmatize unless the statistics indicate that it will help your NLP pipeline perform better.\\nIs there not a small amount of meaning that is lost when \"smarter\" and \"smartest\" reduce to \"smart\"?\\nMake sure stemming does not leave your NLP pipeline dumb.\\n\\nLet the statistics of how of how characters and words are used together help you decide how, or if, to decompose any particular word or n-gram.\\nIn the next chapter we\\'ll show you some tools like Scikit-Learn\\'s `TfidfVectorizer` that handle all the tedious account required to get this right.\\n\\n\\n==== Contractions\\n\\n// TODO: clean this up\\nYou might be wondering why you would want to split the contraction `wasn\\'t` into `was` and `n\\'t`.\\nFor some applications, like grammar-based NLP models that use syntax trees, it is important to separate the words `was` and `not` to allow the syntax tree parser to have a consistent, predictable set of tokens with known grammar rules as its input.\\nThere are a variety of standard and nonstandard ways to contract words, by reducing contractions to their constituent words, a dependency tree parser or syntax parser only need to be programmed to anticipate the various spellings of individual words rather than all possible contractions.\\n\\n\\n[TIP]\\n.Tokenize informal text from social networks such as Twitter and Facebook\\n====\\nThe NLTK library includes a rule-based tokenizer to deal with short, informal, emoji-laced texts from social networks: `casual_tokenize`\\n\\nIt handles emojis, emoticons, and usernames.\\nThe `reduce_len` option deletes less meaningful character repetitions.\\nThe `reduce_len` algorithm retains three repetitions, to approximate the intent and sentiment of the original text.\\n\\n[source,python]\\n----\\n>>> from nltk.tokenize.casual import casual_tokenize\\n>>> texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\\n>>> casual_tokenize(texts[-1], reduce_len=True)\\n[\\'@rickrau\\', \\'mind\\', \\'BLOOOWWWN\\', \\'by\\', \\'latest\\', \\'lex\\', \\':*)\\', \\'!\\', \\'!\\', \\'!\\']\\n----\\n\\n====\\n\\n=== Extending your vocabulary with _n_-grams\\n\\nLet\\'s revisit that \"ice cream\" problem from the beginning of the chapter.\\nRemember we talked about trying to keep \"ice\" and \"cream\" together.\\n\\n____\\nI scream, you scream, we all scream for ice cream.\\n____\\n\\nBut I do not know many people that scream for \"cream\".\\nAnd nobody screams for \"ice\", unless they\\'re about to slip and fall on it.\\nSo you need a way for your word-vectors to keep \"ice\" and \"cream\" together.\\n\\n==== We all gram for _n_-grams\\n\\nAn _n_-gram is a sequence containing up to _n_ elements that have been extracted from a sequence of those elements, usually a string.\\nIn general the \"elements\" of an _n_-gram can be characters, syllables, words, or even symbols like \"A\", \"D\", and \"G\" used to represent the chemical amino acid markers in a DNA or RNA sequence.footnote:[Linguistic and NLP techniques are often used to glean information from DNA and RNA, this site provides a list of amino acid symbols that can help you translate amino acid language into a human-readable language: \"Amino Acid - Wikipedia\" (https://en.wikipedia.org/wiki/Amino_acid#Table_of_standard_amino_acid_abbreviations_and_properties).]\\n\\nIn this book, we\\'re only interested in _n_-grams of words, not characters.footnote:[You may have learned about trigram indexes in your database class or the documentation for PostgreSQL (`postgres`). But these are triplets of characters. They help you quickly retrieve fuzzy matches for strings in a massive database of strings using the `%` and `~*` SQL full text search queries.]\\nSo in this book, when we say 2-gram, we mean a pair of words, like \"ice cream\".\\nWhen we say 3-gram, we mean a triplet of words like \"beyond the pale\" or \"Johann Sebastian Bach\" or \"riddle me this\".\\n_n_-grams do not have to mean something special together, like compound words.\\nThey have to be frequent enough together to catch the attention of your token counters.\\n\\nWhy bother with _n_-grams?\\nAs you saw earlier, when a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of those words.\\nBy extending your concept of a token to include multiword tokens, _n_-grams, your NLP pipeline can retain much of the meaning inherent in the order of words in your statements.\\nFor example, the meaning-inverting word \"not\" will remain attached to its neighboring words, where it belongs.\\nWithout _n_-gram tokenization, it would be free floating.\\nIts meaning would be associated with the entire sentence or document rather than its neighboring words.\\nThe 2-gram \"was not\" retains much more of the meaning of the individual words \"not\" and \"was\" than those 1-grams alone in a bag-of-words vector.\\nA bit of the context of a word is retained when you tie it to its neighbor(s) in your pipeline.\\n\\nIn the next chapter, we show you how to recognize which of these _n_-grams contain the most information relative to the others, which you can use to reduce the number of tokens (_n_-grams) your NLP pipeline has to keep track of.\\nOtherwise it would have to store and maintain a list of every single word sequence it came across.\\nThis prioritization of _n_-grams will help it recognize \"Three Body Problem\" and \"ice cream\", without paying particular attention to \"three bodies\" or \"ice shattered\".\\nIn chapter 4, we associate word pairs, and even longer sequences, with their actual meaning, independent of the meaning of their individual words.\\nBut for now, you need your tokenizer to generate these sequences, these _n_-grams.\\n\\n==== Stop words\\n\\nStop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase.\\nExamples of some common stop words include footnote:[A more comprehensive list of stop words for various languages can be found in NLTK\\'s corpora (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip).]\\n\\n* a, an\\n* the, this\\n* and, or\\n* of, on\\n\\nHistorically stop words have been excluded from NLP pipelines in order to reduce the computational effort to extract information from a text.\\nEven though the words themselves carry little information, the stop words can provide important relational information as part of an _n_-gram.\\nConsider these two examples:\\n\\n* `Mark reported to the CEO`\\n* `Suzanne reported as the CEO to the board`\\n\\nIn your NLP pipeline, you might create 4-grams such as `reported to the CEO` and `reported as the CEO`.\\nIf you remove the stop words from the 4-grams, both examples would be reduced to `reported CEO`, and you would lack the information about the professional hierarchy.\\nIn the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board.\\nUnfortunately, retaining the stop words within your pipeline creates another problem: It increases the length of the _n_-grams required to make use of these connections formed by the otherwise meaningless stop words.\\nThis issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\\n\\nDesigning a filter for stop words depends on your particular application.\\nVocabulary size will drive the computational complexity and memory requirements of all subsequent steps in the NLP pipeline.\\nBut stop words are only a small portion of your total vocabulary size.\\nA typical stop word list has only 100 or so frequent and unimportant words listed in it.\\nBut a vocabulary size of 20,000 words would be required to keep track of 95% of the words seen in a large corpus of tweets, blog posts, and news articles.footnote:[See the web page titled \"Analysis of text data and Natural Language Processing\" (http://rstudio-pubs-static.s3.amazonaws.com/41251_4c55dff8747c4850a7fb26fb9a969c8f.html).]\\nAnd that is just for 1-grams or single-word tokens.\\nA 2-gram vocabulary designed to catch 95% of the 2-grams in a large English corpus will generally have more than 1 million unique 2-gram tokens in it.\\n\\nYou may be worried that vocabulary size drives the required size of any training set you must acquire to avoid overfitting to any particular word or combination of words.\\nAnd you know that the size of your training set drives the amount of processing required to process it all.\\nHowever, getting rid of 100 stop words out of 20,000 is not going to significantly speed up your work.\\nAnd for a 2-gram vocabulary, the savings you would achieve by removing stop words is minuscule.\\nIn addition, for 2-grams you lose a lot more information when you get rid of stop words arbitrarily, without checking for the frequency of the 2-grams that use those stop words in your text.\\nFor example, you might miss mentions of \"The Shining\" as a unique title and instead treat texts about that violent, disturbing movie the same as you treat documents that mention \"Shining Light\" or \"shoe shining\".\\n\\nSo if you have sufficient memory and processing bandwidth to run all the NLP steps in your pipeline on the larger vocabulary, you probably do not want to worry about ignoring a few unimportant words here and there.\\nAnd if you are worried about overfitting a small training set with a large vocabulary, there are better ways to select your vocabulary or reduce your dimensionality than ignoring stop words.\\nIncluding stop words in your vocabulary allows the document frequency filters (discussed in chapter 3) to more accurately identify and ignore the words and _n_-grams with the least information content within your particular domain.\\n\\nThe SpaCy and NLTK packages include a variety of predefined sets of stop words for various use cases. footnote:[The spaCy package contains a list of stopwords that you can modify using this Stack Overflow answer (https://stackoverflow.com/a/51627002/623735)]\\nYou probably won\\'t need a broad list of stopwords like the one we created for listing <<listing-broad-stop-words>>, but if you do you\\'ll want to check out the SpaCy and NLTK stopwords lists.\\nAnd if you need an even broader set of stopwords you can `SearX`\\nfootnote:[If you want to help others find SearX you can get in the habbit of saying \"SearX\" (pronounced \"see Ricks\") when talking or writing about doing a web search. You can shift the meaning of words in your world to make it a better place!]\\nfootnote:[The NLTK package (https://pypi.org/project/nltk) contains the list of stopwords you\\'ll see in most tutorials]  for SEO companies that maintain lists of stopwords in many languages.\\n\\nIf your NLP pipeline relies on a fine-tuned list of stop words to achieve high accuracy, it can be a significant maintenance headache.\\nHumans and machines (search engines) are constantly changing which words they ignore.\\n footnote:[Damien Doyle maintains a list of search engine stopwords ranked by popularity and categorized by language (https://www.ranks.nl/stopwords)]\\n// HL: to Karen, Matko & Ivan: how can I use references (anchors) to refer to the correct listing number?\\nListing <<listing-broad-stop-words>> uses an exhaustive list from all these lists so you can get a feel for the amount of meaning that can be lost if your hand-crafted list of stop words isn\\'t well crafted and kept up to date.\\n\\n////\\n\\nHL to Ivan and Matko:\\n\\nThese are the things I tried based on the examples in Manning\\'s ADOC documentation:\\n\\n[[listing-broad-stop-words]] -> \"Listing 2.6\" (capital L)\\n\\n[#listing-broad-stop-words, reftext={chapter}.{counter:listing}] -> \"2.\" (without the counter:listing integer in my Browser plugin)\\n\\n////\\n[#listing-broad-stop-words, reftext={chapter}.{counter:listing}]\\n.Broad list of stop words\\n[source,python]\\n----\\n>>> import requests\\n>>> url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\\n...        \"src/nlpia/data/stopword_lists.json\")\\n>>> response = requests.get(url)\\n>>> stop_words = response.json()[\\'exhaustive\\']  # <1>\\n>>> tokens = \\'the words were just as I remembered them\\'.split()  # <2>\\n>>> tokens_without_stopwords = [x for x in tokens if x not in stop_words]\\n>>> print(tokens_without_stopwords)\\n[\\'I\\', \\'remembered\\']\\n----\\n<1> This exhaustive list of stop words was compiled from various public search engine optimization lists as well as NLP toolkits like spaCy and NLTK.\\n\\nYou can see that some words carry more meaning than others.\\nThis is a sentence from a short story by Ted Chiang about machines helping us remember our statements so we don\\'t have to rely on flawed memories.footnote:[from Ted Chiang, _Exhalation_, \"Truth of Fact, Truth of Fiction\"]\\nIn this phrase you lost two thirds of the words and still retained the bulk of the phrase\\'s meaning.\\nHowever you can see that an import token \"words\" was discarded by this particular stop words list.\\nYou can often get your point across without articles, prepositions, or even forms of the verb \"to be\".\\nImagine someone doing sign language or in a hurry to write a note to themselves.\\nWhich words would they choose to always skip? That is how stop words are chosen.\\n\\nHere\\'s another common stop words list that isn\\'t quite as exhaustive:\\n\\n[[nltk_stop_words_code]]\\n.NLTK list of stop words\\n[source,python]\\n----\\n>>> import nltk\\n>>> nltk.download(\\'stopwords\\')\\n>>> stop_words = nltk.corpus.stopwords.words(\\'english\\')\\n>>> len(stop_words)\\n179\\n>>> stop_words[:7]\\n[\\'i\\', \\'me\\', \\'my\\', \\'myself\\', \\'we\\', \\'our\\', \\'ours\\']\\n>>> [sw for sw in stopwords if len(sw) == 1]\\n[\\'i\\', \\'a\\', \\'s\\', \\'t\\', \\'d\\', \\'m\\', \\'o\\', \\'y\\']\\n----\\n\\nA document that dwells on the first person is pretty boring, and more importantly for you, has low information content.\\nThe NLTK package includes pronouns (not just first person ones) in its list of stop words.\\nAnd these one-letter stop words are even more curious, but they make sense if you have used the NLTK tokenizer and Porter stemmer a lot.\\nThese single-letter tokens pop up a lot when contractions are split and stemmed using NLTK tokenizers and stemmers.\\n\\n[WARNING]\\n====\\nThe set of English stop words in `sklearn`, `spacy`, `nltk`, and SEO tools are very different, and they are constantly evolving.\\nAt the time of this writing, `sklearn` has 318 stop words, NLTK has 179 stop words, spaCy has 326, and our \\'exhaustive\\' SEO list includes 667 stop words.\\n\\nThis is a good reason to consider *not* filtering stop words.\\nIf you do, others may not be able to reproduce your results.\\n====\\n\\nDepending on how much natural language information you want to discard ;), you can take the union or the intersection of multiple stop word lists for your pipeline.\\nHere are some stop_words lists we found, though we rarely use any of them in production:\\n\\n[[collection_of_stop_words_lists_code]]\\n.Collection of stop words lists\\n[source,python]\\n----\\n>>> resp = requests.get(url)\\n>>> len(resp.json()[\\'exhaustive\\'])\\n667\\n>>> len(resp.json()[\\'sklearn\\'])\\n318\\n>>> len(resp.json()[\\'spacy\\'])\\n326\\n>>> len(resp.json()[\\'nltk\\'])\\n179\\n>>> len(resp.json()[\\'reuters\\'])\\n28\\n----\\n\\n=== Normalizing your vocabulary\\n\\nSo you have seen how important vocabulary size is to the performance of an NLP pipeline. Another vocabulary reduction technique is to normalize your vocabulary so that tokens that mean similar things are combined into a single, normalized form. Doing so reduces the number of tokens you need to retain in your vocabulary and also improves the association of meaning across those different \"spellings\" of a token or _n_-gram in your corpus. And as we mentioned before, reducing your vocabulary can reduce the likelihood of overfitting.\\n\\n==== Case folding\\n\\nCase folding is when you consolidate multiple \"spellings\" of a word that differ only in their capitalization.\\nSo why would we use case folding at all?\\nWords can become case \"denormalized\" when they are capitalized because of their presence at the beginning of a sentence, or when they\\'re written in `ALL CAPS` for emphasis.\\n// TODO: Discuss _ normalization, Unicode normalization, and asciification, diacritics, accented e in resume\\', etc\\nUndoing this denormalization is called _case normalization_, or more commonly, _case folding_.\\nNormalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline.\\nIt helps you consolidate words that are intended to mean (and be spelled) the same thing under a single token.\\n\\nHowever, some information is often communicated by capitalization of a word -- for example,  \\'doctor\\' and \\'Doctor\\' often have different meanings.\\nOften capitalization is used to indicate that a word is a proper noun, the name of a person, place, or thing.\\nYou will want to be able to recognize proper nouns as distinct from other words, if named entity recognition is important to your pipeline.\\nHowever, if tokens are not case normalized, your vocabulary will be approximately twice as large, consume twice as much memory and processing time, and might increase the amount of training data you need to have labeled for your machine learning pipeline to converge to an accurate, general solution.\\nJust as in any other machine learning pipeline, your labeled dataset used for training must be \"representative\" of the space of all possible feature vectors your model must deal with, including variations in capitalization.\\nFor 100000-D bag-of-words vectors, you usually must have 100000 labeled examples, and sometimes even more than that, to train a supervised machine learning pipeline without overfitting.\\nIn some situations, cutting your vocabulary size by half can sometimes be worth the loss of information content.\\n\\nIn Python, you can easily normalize the capitalization of your tokens with a list comprehension.\\n\\n[source,python]\\n----\\n>>> tokens = [\\'House\\', \\'Visitor\\', \\'Center\\']\\n>>> normalized_tokens = [x.lower() for x in tokens]\\n>>> print(normalized_tokens)\\n[\\'house\\', \\'visitor\\', \\'center\\']\\n----\\n\\nAnd if you are certain that you want to normalize the case for an entire document, you can `lower()` the text string in one operation, before tokenization.\\nBut this will prevent advanced tokenizers that can split _camel case_ words like \"WordPerfect\", \"FedEx\", or \"stringVariableName.\"footnote:[See the web page titled \"Camel case case - Wikipedia\" (https://en.wikipedia.org/wiki/Camel_case_case).]]\\nMaybe you want WordPerfect to be its own unique thing (token), or maybe you want to reminisce about a more perfect word processing era.\\nIt is up to you to decide when and how to apply case folding.\\n\\nWith case normalization, you are attempting to return these tokens to their \"normal\" state before grammar rules and their position in a sentence affected their capitalization.\\nThe simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Python\\'s built-in `str.lower()`.footnote:[We\\'re assuming the behavior of str.lower() in Python 3. In Python 2, bytes (strings) could be lowercased by just shifting all alpha characters in the ASCII number (`ord`) space, but in Python 3 `str.lower` properly translates characters so it can handle embellished English characters (like the \"acute accent\" diactric mark over the e in resumé) as well as the particulars of capitalization in non-English languages.]\\nUnfortunately this approach will also \"normalize\" away a lot of meaningful capitalization in addition to the less meaningful first-word-in-sentence capitalization you intended to normalize away.\\nA better approach for case normalization is to lowercase only the first word of a sentence and allow all other words to retain their capitalization.\\n\\nLowercasing on the first word in a sentence preserves the meaning of a proper nouns in the middle of a sentence, like \"Joe\" and \"Smith\" in \"Joe Smith\".\\nAnd it properly groups words together that belong together, because they are only capitalized when they are at the beginning of a sentence, since they are not proper nouns.\\nThis prevents \"Joe\" from being confused with \"coffee\" (\"joe\")footnote:[The trigram \"cup of joe\" (https://en.wiktionary.org/wiki/cup_of_joe) is slang for \"cup of coffee.\"] during tokenization.\\nAnd this approach prevents the blacksmith connotation of \"smith\" being confused the the proper name \"Smith\" in a sentence like \"A word smith had a cup of joe.\"\\nEven with this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still need to introduce capitalization errors for the rare proper nouns that start a sentence.\\n\"Joe Smith, the word smith, with a cup of joe.\" will produce a different set of tokens than \"Smith the word with a cup of joe, Joe Smith.\"\\nAnd you may not  want that.\\nIn addition, case normalization is useless for languages that do not have a concept of capitalization, like Arabic or Hindi.\\n\\nTo avoid this potential loss of information, many NLP pipelines do not normalize for case at all.\\nFor many applications, the efficiency gain (in storage and processing) for reducing one\\'s vocabulary size by about half is outweighed by the loss of information for proper nouns.\\nBut some information may be \"lost\" even without case normalization.\\nIf you do not identify the word \"The\" at the start of a sentence as a stop word, that can be a problem for some applications.\\nReally sophisticated pipelines will detect proper nouns before selectively normalizing the case for words at the beginning of sentences that are clearly not proper nouns.\\nYou should implement whatever case normalization approach makes sense for your application.\\nIf you do not have a lot of \"Smith\"s and \"word smiths\" in your corpus, and you do not care if they get assigned to the the same tokens, you can just lowercase everything.\\nThe best way to find out what works is to try several different approaches, and see which approach gives you the best performance for the objectives of your NLP project.\\n\\nBy generalizing your model to work with text that has odd capitalization, case normalization can reduce overfitting for your machine learning pipeline.\\nCase normalization is particularly useful for a search engine.\\nFor search, normalization increases the number of matches found for a particular query.\\nThis is often called the \"recall\" performance metric for a search engine (or any other classification model).footnote:[Check our Appendix D to learn more about _precision_ and _recall_. Here\\'s a comparison of the recall of various search engines on the Webology site (http://www.webology.org/2005/v2n2/a12.html).]\\n\\nFor a search engine without normalization if you searched for \"Age\" you will get a different set of documents than if you searched for \"age.\"\\n\"Age\" would likely occur in phrases like \"New Age\" or \"Age of Reason\".\\nIn contrast, \"age\" would be more likely to occur in phrases like \"at the age of\" in your sentence about Thomas Jefferson.\\nBy normalizing the vocabulary in your search index (as well as the query), you can ensure that both kinds of documents about \"age\" are returned regardless of the capitalization in the query from the user.\\n\\nHowever, this additional recall accuracy comes at the cost of precision, returning many documents that the user may not be interested in. Because of this issue, modern search engines allow users to turn off normalization with each query, typically by quoting those words for which they want only exact matches returned. If you are building such a search engine pipeline, in order to accommodate both types of queries you will have to build two indexes for your documents: one with case-normalized _n_-grams, and another with the original capitalization.\\n\\n==== Stemming\\n\\nAnother common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms.\\nThis normalization, identifying a common stem among various forms of a word, is called stemming.\\nFor example, the words `housing` and `houses` share the same stem, `house`.\\nStemming removes suffixes from words in an attempt to combine words with similar meanings together under their common stem.\\nA stem is not required to be a properly spelled word, but merely a token, or label, representing several possible spellings of a word.\\n\\nA human can easily see that \"house\" and \"houses\" are the singular and plural forms of the same noun.\\nHowever, you need some way to provide this information to the machine. One of its main benefits is in the compression of the number of words whose meaning your software or language model needs to keep track of.\\nIt reduces the size of your vocabulary while limiting the loss of information and meaning, as much as possible.\\nIn machine learning this is referred to as dimension reduction.\\nIt helps generalize your language model, enabling the model to behave identically for all the words included in a stem.\\nSo, as long as your application does not require your machine to distinguish between \"house\" and \"houses\", this stem will reduce your programming or dataset size by half or even more, depending on the aggressiveness of the stemmer you chose.\\n\\nStemming is important for keyword search or information retrieval.\\nIt allows you to search for \"developing houses in Portland\" and get web pages or documents that use both the word \"house\" and \"houses\" and even the word \"housing\" because these words are all stemmed to the \"hous\" token.\\nLikewise you might receive pages with the words \"developer\" and \"development\" rather than \"developing\" because all these words typically reduce to the stem \"develop\".\\nAs you can see, this is a \"broadening\" of your search, ensuring that you are less likely to miss a relevant document or web page.\\nThis broadening of your search results would be a big improvement in the \"recall\" score for how well your search engine is doing its job at returning all the relevant documents.footnote:[Review Appendix D if you have forgotten how to measure recall or visit the wikipedia page to learn more (https://en.wikipedia.org/wiki/Precision_and_recall).]\\n\\nBut stemming could greatly reduce the \"precision\" score for your search engine because it might return many more irrelevant documents along with the relevant ones.\\nIn some applications this \"false-positive rate\" (proportion of the pages returned that you do not find useful) can be a problem.\\nSo most search engines allow you to turn off stemming and even case normalization by putting quotes around a word or phrase.\\nQuoting indicates that you only want pages containing the exact spelling of a phrase such as \"\\'Portland Housing Development software\\'.\"\\nThat would return a different sort of document than one that talks about a \"\\'a Portland software developer\\'s house\\'.\"\\nAnd there are times when you want to search for \"Dr. House\\'s calls\" and not \"dr house call\", which might be the effective query if you used a stemmer on that query.\\n\\nHere\\'s a simple stemmer implementation in pure Python that can handle trailing S\\'s.\\n\\n[source,python]\\n----\\n>>> def stem(phrase):\\n...     return \\' \\'.join([re.findall(\\'^(.*ss|.*?)(s)?$\\',\\n...         word)[0][0].strip(\"\\'\") for word in phrase.lower().split()])\\n>>> stem(\\'houses\\')\\n\\'house\\'\\n>>> stem(\"Doctor House\\'s calls\")\\n\\'doctor house call\\'\\n----\\n\\nThe preceding stemmer function follows a few simple rules within that one short regular expression:\\n\\n* If a word ends with more than one `s`, the stem is the word and the suffix is a blank string.\\n* If a word ends with a single `s`, the stem is the word without the `s` and the suffix is the `s`.\\n* If a word does not end on an `s`, the stem is the word and no suffix is returned.\\n\\nThe strip method ensures that some possessive words can be stemmed along with plurals.\\n\\nThis function works well for regular cases, but is unable to address more complex cases. For example, the rules would fail with words like `dishes` or `heroes`. For more complex cases like these, the NLTK package provides other stemmers.\\n\\nIt also does not handle the \"housing\" example from your \"Portland Housing\" search.\\n\\nTwo of the most popular stemming algorithms are the Porter and Snowball stemmers.\\nThe Porter stemmer is named for the computer scientist Martin Porter.footnote:[See \"An algorithm for suffix stripping\", 1993 (http://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf) by M.F. Porter.]\\nPorter is also also responsible for enhancing the Porter stemmer to create the Snowball stemmer.footnote:[See the web page titled \"Snowball: A language for stemming algorithms\" (http://snowball.tartarus.org/texts/introduction.html).]\\nPorter dedicated much of his lengthy career to documenting and improving stemmers, due to their value in information retrieval (keyword search).\\nThese stemmers implement more complex rules than our simple regular expression.\\nThis enables the stemmer to handle the complexities of English spelling and word ending rules.\\n\\n[source,python]\\n----\\n>>> from nltk.stem.porter import PorterStemmer\\n>>> stemmer = PorterStemmer()\\n>>> \\' \\'.join([stemmer.stem(w).strip(\"\\'\") for w in\\n...   \"dish washer\\'s fairly washed dishes\".split()])\\n\\'dish washer fairli wash dish\\'\\n----\\n\\nNotice that the Porter stemmer, like the regular expression stemmer, retains the trailing apostrophe (unless you explicitly strip it), which ensures that possessive words will be distinguishable from nonpossessive words.\\nPossessive words are often proper nouns, so this feature can be important for applications where you want to treat names differently than other nouns.\\n\\n.More on the Porter stemmer\\n****\\nJulia Menchavez has graciously shared her translation of Porter\\'s original stemmer algorithm into pure python (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py). If you are ever tempted to develop your own stemmer, consider these 300 lines of code and the lifetime of refinement that Porter put into them.\\n\\nThere are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4, 5a, and 5b.\\nStep 1a is a bit like your regular expression for dealing with trailing \"S\"es:footnote:[This is a trivially abbreviated version of Julia Menchavez\\'s implementation `porter-stemmer` on GitHub (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py).]\\n\\n[source,python]\\n----\\ndef step1a(self, word):\\n    if word.endswith(\\'sses\\'):\\n        word = self.replace(word, \\'sses\\', \\'ss\\')  # <1>\\n    elif word.endswith(\\'ies\\'):\\n        word = self.replace(word, \\'ies\\', \\'i\\')\\n    elif word.endswith(\\'ss\\'):\\n        word = self.replace(word, \\'ss\\', \\'ss\\')\\n    elif word.endswith(\\'s\\'):\\n        word = self.replace(word, \\'s\\', \\'\\')\\n    return word\\n----\\n<1> This is not at all like `str.replace()`. Julia\\'s `self.replace()` modifies only the ending of a word.\\n\\nThe remainining seven steps are much more complicated because they have to deal with the complicated English spelling rules for the following:\\n\\n* *Step 1a*: \"s\" and \"es\" endings\\n* *Step 1b*: \"ed\", \"ing\", and \"at\" endings\\n* *Step 1c*: \"y\" endings\\n* *Step 2*: \"nounifying\" endings such as \"ational\", \"tional\", \"ence\", and \"able\"\\n* *Step 3*: adjective endings such as \"icate\",footnote:[Sorry Chick, Porter doesn\\'t like your `obsfucate` username ;)], \"ful\", and \"alize\"\\n* *Step 4*: adjective and noun endings such as \"ive\", \"ible\", \"ent\", and \"ism\"\\n* *Step 5a*: stubborn \"e\" endings, still hanging around\\n* *Step 5b*: trailing double-consonants for which the stem will end in a single \"l\"\\n****\\n\\nSnowball stemmer is more aggressive than the Porter stemmer.\\nNotice that it stems \\'fairly\\' to \\'fair\\', which is more accurate than the Porter stemmer.\\n\\n[source,python]\\n----\\n>>> from nltk.stem.snowball import SnowballStemmer\\n>>> stemmer = SnowballStemmer(language=\\'english\\')\\n>>> \\' \\'.join([stemmer.stem(w).strip(\"\\'\") for w in\\n...   \"dish washer\\'s fairly washed dishes\".split()])\\n\\'dish washer fair wash dish\\'\\n----\\n\\n==== Lemmatization\\n\\nIf you have access to information about connections between the meanings of various words, you might be able to associate several words together even if their spelling is quite different.\\nThis more extensive normalization down to the semantic root of a word -- its lemma -- is called lemmatization.\\n\\nIn chapter 12, we show how you can use lemmatization to reduce the complexity of the logic required to respond to a statement with a chatbot.\\nAny NLP pipeline that wants to \"react\" the same for multiple different spellings of the same basic root word can benefit from a lemmatizer.\\nIt reduces the number of words you have to respond to, the dimensionality of your language model.\\nUsing it can make your model more general, but it can also make your model less precise, because it will treat all spelling variations of a given root word the same.\\nFor example \"chat\", \"chatter\", \"chatty\", \"chatting\", and perhaps even \"chatbot\" would all be treated the same in an NLP pipeline with lemmatization, even though they have different meanings.\\nLikewise \"bank\", \"banked\", and \"banking\" would be treated the same by a stemming pipeline despite the river meaning of \"bank\", the motorcycle meaning of \"banked\" and the finance meaning of \"banking.\"\\n\\nAs you work through this section, think about words where lemmatization would drastically alter the meaning of a word, perhaps even inverting its meaning and producing the opposite of the intended response from your pipeline.\\nThis scenario is called _spoofing_ -- when you try to elicit the wrong response from a machine learning pipeline by cleverly constructing a difficult input.\\n\\nSometimes lemmatization will be a better way to normalize the words in your vocabulary.\\nYou may find that for your application stemming and case folding create stems and tokens that do not take into account a word\\'s meaning.\\nA lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.\\n\\nSome lemmatizers use the word\\'s part of speech (POS) tag in addition to its spelling to help improve accuracy.\\nThe POS tag for a word indicates its role in the grammar of a phrase or sentence.\\nFor example, the noun POS is for words that refer to \"people, places, or things\" within a phrase.\\nAn adjective POS is for a word that modifies or describes a noun.\\nA verb refers to an action.\\nThe POS of a word in isolation cannot be determined.\\nThe context of a word must be known for its POS to be identified.\\nSo some advanced lemmatizers cannot be run on words in isolation.\\n\\nCan you think of ways you can use the part of speech to identify a better \"root\" of a word than stemming could?\\nConsider the word `better`.\\nStemmers would strip the \"er\" ending from \"better\" and return the stem \"bett\" or \"bet\".\\nHowever, this would lump the word \"better\" with words like \"betting\", \"bets\", and \"Bet\\'s\", rather than more similar words like \"betterment\", \"best\", or even \"good\" and \"goods\".\\n\\nSo lemmatizers are better than stemmers for most applications.\\nStemmers are only really used in large scale information retrieval applications (keyword search).\\nAnd if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer.\\nBecause the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer.\\nThis trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone.footnote:[Thank you Kyle Gorman for pointing this out]\\n\\nHow can you identify word lemmas in Python?\\nThe NLTK package provides functions for this.\\nNotice that you must tell the WordNetLemmatizer which part of speech you are interested in, if you want to find the most accurate lemma:\\n\\n[source,python]\\n----\\n>>> nltk.download(\\'wordnet\\')\\nTrue\\n>>> nltk.download(\\'omw-1.4\\')\\nTrue\\n>>> from nltk.stem import WordNetLemmatizer\\n>>> lemmatizer = WordNetLemmatizer()\\n>>> lemmatizer.lemmatize(\"better\")  # <1>\\n\\'better\\'\\n>>> lemmatizer.lemmatize(\"better\", pos=\"a\")  # <2>\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"good\", pos=\"a\")\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"goods\", pos=\"a\")\\n\\'goods\\'\\n>>> lemmatizer.lemmatize(\"goods\", pos=\"n\")\\n\\'good\\'\\n>>> lemmatizer.lemmatize(\"goodness\", pos=\"n\")\\n\\'goodness\\'\\n>>> lemmatizer.lemmatize(\"best\", pos=\"a\")\\n\\'best\\'\\n----\\n<1> The default part of speech is \"n\" for \"noun\"\\n<2> \"a\" indicates the \"adjective\" part of speech\\n\\nYou might be surprised that the first attempt to lemmatize the word \"better\" did not change it at all. This is because the part of speech of a word can have a big effect on its meaning. If a POS is not specified for a word, then the NLTK lemmatizer assumes it is a noun. Once you specify the correct POS, \\'a\\' for adjective, the lemmatizer returns the correct lemma. Unfortunately, the NLTK lemmatizer is restricted to the connections within the Princeton WordNet graph of word meanings. So the word \"best\" does not lemmatize to the same root as \"better\". This graph is also missing the connection between \"goodness\" and \"good\". A Porter stemmer, on the other hand, would make this connection by blindly stripping off the \"ness\" ending of all words.\\n\\n[source,python]\\n----\\n>>> stemmer.stem(\\'goodness\\')\\n\\'good\\'\\n----\\n\\nYou can easily implement lemmatization in spaCy by the following:\\n\\n[source,python]\\n----\\n>>> import spacy\\n>>> nlp = spacy.load(\"en_core_web_sm\")\\n>>> doc = nlp(\"better good goods goodness best\")\\n>>> for token in doc:\\n>>> print(token.text, token.lemma_)\\nbetter well\\ngood good\\ngoods good\\ngoodness goodness\\nbest good\\n----\\nUnlike NLTK, spaCy lemmatizes \"better\" to \"well\" by assuming it is an adverb and returns the correct lemma for \"best\" (\"good\").\\n\\n==== Synonym substitution\\n\\nThere are five kinds of \"synonyms\" that are sometime helpful in creating a consistent smaller vocabulary to help your NLP pipeline generalize well.\\n\\n. Typo correction\\n. Spelling correction\\n. Synonym substitution\\n. Contraction expansion\\n. Emoji expansion\\n\\nEach of these synonym substitution algorithms can be designed to be more or less agressive.\\nAnd you will want to think about the language used by your users in your domain.\\nFor example, in the legal, technical, or medical fields, it\\'s rarely a good idea to substitute synonyms.\\nA doctor wouldn\\'t want a chatbot telling his patient their \"heart is broken\" because of some synonym substitutions on the heart emoticon (\"<3\").\\n\\nNonetheless, the use cases for lemmatization and stemming apply to synonym substitution.\\n\\n==== Use cases\\n\\nWhen should you use a lemmatizer, stemmer, or synonym substitution?\\nStemmers are generally faster to compute and require less-complex code and datasets.\\nBut stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would.\\nBoth stemmers and lemmatizers will reduce your vocabulary size and increase the ambiguity of the text.\\nBut lemmatizers do a better job retaining as much of the information content as possible based on how the word was used within the text and its intended meaning.\\nAs a result, some state of the art NLP packages, such as spaCy, do not provide stemming functions and only offer lemmatization methods.\\n\\nIf your application involves search, stemming and lemmatization will improve the recall of your searches by associating more documents with the same query words.\\nHowever, stemming, lemmatization, and even case folding will usually reduce the precision and accuracy of your search results.\\nThese vocabulary compression approaches may cause your information retrieval system (search engine) to return many documents not relevant to the words\\' original meanings.\\nThese are called \"false positives\", a incorrect matches to your search query.\\nSometimes \"false positives\" are less important than false negatives.\\nA false negative for a search engine is when it fails to list the document you are looking for at all.\\n\\nBecause search results can be ranked according to relevance, search engines and document indexes typically use lemmatization when they process your query and index your documents.\\nBecause search results can be ranked according to relevance, search engines and document indexes typically use lemmatization in their NLP pipeline.\\nThis means a search engine will use lemmatization when they tokenize your search text as well as when they index their collection of documents, such as the web pages they crawl.\\n\\nBut they combine search results for unstemmed versions of words to rank the search results that they present to you.footnote:[Additional metadata is also used to adjust the ranking of search results.\\nDuck Duck Go and other popular web search engines combine more than 400 independent algorithms (including user-contributed algorithms) to rank your search results (https://duck.co/help/results/sources).]\\n\\nFor a search-based chatbot, precision is usually more important than recall.\\nA false positive match can cause your chatbot says something inappropriate.\\nFalse negatives just cause your chatbot to have to humbly admit that it cannot find anything appropriate to say.\\nYour chatbot will sound better if your NLP pipeline first searches for matches to your user\\'s questions using unstemmed, unnormalized words.\\nYour search algorithm can fall back to normalized token matches if it cannot find anything else to say.\\nAnd you can rank these *fallback* matches for normalized tokens lower than the unnormalized token matches.\\nYou can even give your bot humility and transparency by introducing lower ranked responses with a caveat, such as \"I haven\\'t heard a phrase like that before, but using my stemmer I found...\"\\nIn a modern world crowded with blowhard chatbots, your humbler chatbot can make a name for itself and win out!footnote:[\"Nice guys finish first!\" -- M.A. Nowak author of _SuperCooperators_\"]\\n\\nThere are 4 situations when synonym substitution of some sort may make sense.\\n\\n. Search engines\\n. Data augmentation\\n. Scoring the robustness of your NLP\\n. Adversarial NLP\\n\\nSearch engines can improve their recall for rare terms by using synonym substitution.\\nWhen you have limited labeled data, you can often expand your dataset 10 fold (10x) with synonym substitution alone.\\nIf you want to find a lower bound on the accuracy of your model you can aggressively substitute synonyms in your test set to see how robust your model is to these changes.\\nAnd if you are searching for ways to poison or evade detection by an NLP algorithm, synonyms can give you a large number of probing texts to try.\\nYou can imagine that substituting the \"currency\" for the word \"cash\", \"dollars\", or \"$$$$\" might help evade a spam detector.\\n\\n[IMPORTANT]\\nBottom line, try to avoid stemming, lemmatization, case folding, or synonym substitution, unless you have a limited amount of text with contains usages and capitalizations of the words you are interested in.\\nAnd with the explosion of NLP datasets, this is rarely the case for English documents, unless your documents use a lot of jargon or are from a very small subfield of science, technology, or literature.\\nNonetheless, for languages other than English, you may still find uses for lemmatization.\\nThe Stanford information retrieval course dismisses stemming and lemmatization entirely, due to the negligible recall accuracy improvement and the significant reduction in precision.footnote:[See the Stanford NLP Information Retrieval (IR) book section titled \"Stemming and lemmatization\" (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).]\\n\\n\\n\\n== Sentiment\\n\\nWhether you use raw single-word tokens, _n_-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information.\\nAn important part of this information is the word\\'s sentiment -- the overall feeling or emotion that word invokes.\\nThis _sentiment analysis_ -- measuring the sentiment of phrases or chunks of text -- is a common application of NLP.\\nIn many companies it is the main thing an NLP engineer is asked to do.\\n\\nCompanies like to know what users think of their products.\\nSo they often will provide some way for you to give feedback.\\nA star rating on Amazon or Rotten Tomatoes is one way to get quantitative data about how people feel about products they\\'ve purchased.\\nBut a more natural way is to use natural language comments.\\nGiving your user a blank slate (an empty text box) to fill up with comments about your product can produce more detailed feedback.\\n\\nIn the past you would have to read all that feedback.\\nOnly a human can understand something like emotion and sentiment in natural language text, right?\\nHowever, if you had to read thousands of reviews you would see how tedious and error-prone a human reader can be.\\nHumans are remarkably bad at reading feedback, especially criticism or negative feedback.\\nAnd customers are not generally very good at communicating feedback in a way that can get past your natural human triggers and filters.\\n\\nBut machines do not have those biases and emotional triggers.\\nAnd humans are not the only things that can process natural language text and extract information, even meaning from it.\\nAn NLP pipeline can process a large quantity of user feedback quickly and objectively, with less chance for bias.\\nAnd an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\\n\\nAnother common application of sentiment analysis is junk mail and troll message filtering.\\nYou would like your chatbot to be able to measure the sentiment in the chat messages it processes so it can respond appropriately.\\nAnd even more importantly, you want your chatbot to measure its own sentiment of the statements it is about to send out, which you can use to steer your bot to be kind and pro-social with the statements it makes.\\nThe simplest way to do this might be to do what Moms told us to do: If you cannot say something nice, do not say anything at all.\\nSo you need your bot to measure the niceness of everything you are about to say and use that to decide whether to respond.\\n\\nWhat kind of pipeline would you create to measure the sentiment of a block of text and produce this sentiment positivity number?\\nSay you just want to measure the positivity or favorability of a text -- how much someone likes a product or service that they are writing about.\\nSay you want your NLP pipeline and sentiment analysis algorithm to output a single floating point number between -1 and +1.\\nYour algorithm would output +1 for text with positive sentiment like \"Absolutely perfect! Love it! :-) :-) :-)\".\\nAnd your algorithm should output -1 for text with negative sentiment like \"Horrible! Completely useless. :(\".\\nYour NLP pipeline could use values near 0, like say +0.1, for a statement like \"It was OK. Some good and some bad things\".\\n\\nThere are two approaches to sentiment analysis:\\n\\n* A rule-based algorithm composed by a human\\n* A _machine learning_ model learned from data by a machine\\n\\nThe first approach to sentiment analysis uses human-designed rules, sometimes called heuristics, to measure sentiment.\\nA common rule-based approach to sentiment analysis is to find keywords in the text and map each one to numerical scores or weights in a dictionary or \"mapping\" -- a Python `dict`, for example.\\nNow that you know how to do tokenization, you can use stems, lemmas, or _n_-gram tokens in your dictionary, rather than just words.\\nThe \"rule\" in your algorithm would be to add up these scores for each keyword in a document that you can find in your dictionary of sentiment scores.\\nOf course you need to hand-compose this dictionary of keywords and their sentiment scores before you can run this algorithm on a body of text.\\nWe show you how to do this using the VADER algorithm (in `sklearn`) in the upcoming listing.\\n\\nThe second approach, machine learning, relies on a labeled set of statements or documents to train a machine learning model to create those rules.\\nA machine learning sentiment model is trained to process input text and output a numerical value for the sentiment you are trying to measure, like positivity or spamminess or trolliness.\\nFor the machine learning approach, you need a lot of data, text labeled with the \"right\" sentiment score.\\nTwitter feeds are often used for this approach because the hash tags, such as `\\\\#awesome` or `\\\\#happy` or `\\\\#sarcasm`, can often be used to create a \"self-labeled\" dataset.\\nYour company may have product reviews with five-star ratings that you could associate with reviewer comments.\\nYou can use the star ratings as a numerical score for the positivity of each text.\\nWe show you shortly how to process a dataset like this and train a token-based machine learning algorithm called _Naive Bayes_ to measure the positivity of the sentiment in a set of reviews after you are done with VADER.\\n\\n=== VADER -- A rule-based sentiment analyzer\\n\\nHutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms.\\nThey called their algorithm VADER, for **V**alence **A**ware **D**ictionary for\\ns**E**ntiment **R**easoning.footnote:[\"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\" by Hutto and Gilbert (http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).]\\nMany NLP packages implement some form of this algorithm.\\nThe NLTK package has an implementation of the VADER algorithm in `nltk.sentiment.vader`.\\nHutto himself maintains the Python package `vaderSentiment`.\\nYou will go straight to the source and use `vaderSentiment` here.\\n\\nYou will need to `pip install vaderSentiment` to run the following example.footnote:[You can find more detailed installation instructions with the package source code on github (https://github.com/cjhutto/vaderSentiment).]\\nYou have not included it in the `nlpia` package.\\n\\n[source,python]\\n----\\n>>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n>>> sa = SentimentIntensityAnalyzer()\\n>>> sa.lexicon  # <1>\\n{ ...\\n\\':(\\': -1.9,  # <2>\\n\\':)\\': 2.0,\\n...\\n\\'pls\\': 0.3,  # <3>\\n\\'plz\\': 0.3,\\n...\\n\\'great\\': 3.1,\\n... }\\n>>> [(tok, score) for tok, score in sa.lexicon.items()\\n...   if \" \" in tok]  # <4>\\n[(\"( \\'}{\\' )\", 1.6),\\n (\"can\\'t stand\", -2.0),\\n (\\'fed up\\', -1.8),\\n (\\'screwed up\\', -1.5)]\\n>>> sa.polarity_scores(text=\\\\\\n...   \"Python is very readable and it\\'s great for NLP.\")\\n{\\'compound\\': 0.6249, \\'neg\\': 0.0, \\'neu\\': 0.661,\\n\\'pos\\': 0.339}  # <5>\\n>>> sa.polarity_scores(text=\\\\\\n...   \"Python is not a bad choice for most applications.\")\\n{\\'compound\\': 0.431, \\'neg\\': 0.0, \\'neu\\': 0.711,\\n\\'pos\\': 0.289}  # <6>\\n----\\n<1> SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores that we talked about.\\n<2> A tokenizer better be good at dealing with punctuation and emoticons (emojis) for VADER to work well. After all, emoticons are designed to convey a lot of sentiment (emotion).\\n<3> If you use a stemmer (or lemmatizer) in your pipeline, you would need to apply that stemmer to the VADER lexicon, too, combining the scores for all the words that go together in a single stem or lemma.\\n<4> Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually _n_-grams; the other is an emoticon for \"kiss\".\\n<5> The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them together into a compound positivity sentiment.\\n<6> Notice that VADER handles negation pretty well -- \"great\" has a slightly more positive sentiment than \"not bad\". VADER\\'s built-in tokenizer ignores any words that aren\\'t in its lexicon, and it does not consider _n_-grams at all.\\n\\nLet us see how well this rule-based approach does for the example statements we mentioned earlier.\\n\\n[source,python]\\n----\\n>>> corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\\n...           \"Horrible! Completely useless. :(\",\\n...           \"It was OK. Some good and some bad things.\"]\\n>>> for doc in corpus:\\n...     scores = sa.polarity_scores(doc)\\n...     print(\\'{:+}: {}\\'.format(scores[\\'compound\\'], doc))\\n+0.9428: Absolutely perfect! Love it! :-) :-) :-)\\n-0.8768: Horrible! Completely useless. :(\\n+0.3254: It was OK. Some good and some bad things.\\n----\\n\\nThis looks a lot like what you wanted.\\nSo the only drawback is that VADER does not look at all the words in a document.\\nVADER only \"knows\" about the 7,500 words or so that were hard-coded into its algorithm.\\nWhat if you want all the words to help add to the sentiment score?\\nAnd what if you do not want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in `SentimentIntensityAnalyzer.lexicon`?\\nThe rule-based approach might be impossible if you do not understand the language because you would not know what scores to put in the dictionary (lexicon)!\\n\\nThat is what machine learning sentiment analyzers are for.\\n\\n=== Closeness of vectors\\n\\nWhy do we use bags of words rather than bags of characters to represent natural language text?\\nFor a cryptographer trying to decrypt an unknown message, frequency analysis of the characters in the text would be a good way to go.\\nBut for natural language text in your native language, words turn out to be a better representation.\\nYou can see this if you think about what we are using these BOW vectors for.\\n\\nIf you think about it, you have a lot of different ways to measure the closeness of things.\\nYou probably have a good feel for what a close family relative would be.\\nOr the closeness of the cafes where you can meet your friend to collaborate on writing a book about AI.\\nFor cafes your brain probably uses Euclidean distance on the 2D position of the cafes you know about.\\nOr maybe Manhattan or taxi-cab distance.\\n\\nBut do you know how to measure the closeness of two pieces of text?\\nIn chapter 4 you\\'ll learn about edit distances that check the similarity of two strings of characters.\\nBut that doesn\\'t really capture the essence of what you care about.\\n\\nHow close are these sentences to each other, in your mind?\\n\\n> I am now coming over to see you.\\n\\n> I am not coming over to see you.\\n\\nDo you see the difference?\\nWhich one would you prefer to receive an e-mail from your friend.\\nThe words \"now\" and \"not\" are very far apart in meaning.\\nBut they are very close in spelling.\\nThis is an example about how a single character can change the meaning of an entire sentence.\\n\\nIf you just counted up the characters that were different you\\'d get a distance of 1.\\nAnd then you could divide by the length of the longest sentence to make sure your distance value is between 0 and 1.\\nSo your character difference or distance calculation would be 1 divided by 32 which gives 0.03125, or about 3%.\\nThen, to turn a distance into a closeness you just subtract it from 1.\\nSo do you think these two sentences are 0.96875, or about 97% the same?\\nThey mean the opposite.\\nSo we\\'d like a better measure than that.\\n\\nWhat if you compared words instead of characters?\\nIn that case you would have one word out of seven that was changed.\\nThat is a little better than one character out of 32.\\nThe sentences would now have a closeness score of six divided by seven or about 85%.\\nThat\\'s a little lower, which is what we want.\\n\\nFor natural language you don\\'t want your closeness or distance measure to rely only on a count of the differences in individual characters.\\nThis is one reason why you want to use words as your tokens of meaning when processing natural language text.\\n\\nWhat about these two sentences?\\n\\n> She and I will come over to your place at 3:00.\\n\\n> At 3:00, she and I will stop by your apartment.\\n\\nAre these two sentences close to each other in meaning?\\nThey have the exact same length in characters.\\nAnd they use some of the same words, or at least synonyms.\\nBut those words and characters are not in the same order.\\nSo we need to make sure that our representation of the sentences does not rely on the precise position of words in a sentence.\\n\\nBag of words vectors accomplish this by creating a position or slot in a vector for every word you\\'ve seen in your vocabulary.\\nYou may have learned of a few measures of closeness in geometry and linear algebra.\\n\\nAs an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\\nVery smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\\nImagine how difficult that is.\\nImagine trying to remove verb endings like \"ing\" from \"ending\" so you\\'d have a stem called \"end\" to represent both words.\\nAnd you\\'d like to stem the word \"running\" to \"run,\" so those two words are treated the same.\\nAnd that\\'s tricky, because you have remove not only the \"ing\" but also the extra \"n\".\\nBut you want the word \"sing\" to stay whole.\\nYou wouldn\\'t want to remove the \"ing\" ending from \"sing\" or you\\'d end up with a single-letter \"s\".\\n\\nOr imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens\".\\nDo isolated individual letters in a word or parts of a word provide any information at all about that word\\'s meaning?\\nCan the letters be misleading?\\nYes and yes.\\n\\nIn this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\\nLater, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you\\'re interested in.\\nFrom that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\\n\\n=== Count vectorizing\\n\\nIn the previous sections you\\'ve only been concerned with keyword detection.\\nYour vectors indicated the presence or absence of words.\\nIn order to handle longer documents and improve the accuracy of your NLP pipeline, you\\'re going to start counting the occurrences of words in your documents.\\n\\nYou can put these counts into a sort-of histogram.\\nJust like before you will create a vector for each document in your pipeline.\\nOnly instead of 0\\'s and 1\\'s in your vectors there will be counts.\\nThis will improve the accuracy of all the similarity and distance calculations you are doing with these counts.\\nAnd just like normalizing histograms can improve your ability to compare two histograms, normalizing your word counts is also a good idea.\\nOtherwise a really short wikipedia article that use Barak Obama\\'s name only once along side all the other presidents might get as much \"Barack Obama\" credit as a much longer page about Barack Obama that uses his name many times.\\nUsers and Question Answering bots like `qary` trying to answer questions about Obama might get distracted by pages listing all the presidents and might miss the main Barack Obama page entirely.\\nSo it\\'s a good idea to normalize your count vectors by dividing the counts by the total length of the document.\\nThis more fairly represents the distribution of tokens in the document and will create better similarity scores with other documents, including the text from a search query from `qary`.footnote:[Qary is an open source virtual assistant that actually assists you instead of manipulating and misinforming you (https://docs.qary.ai).]\\n\\nEach position in your vector represents the count for one of your keywords.\\nAnd having a small vocabulary keeps this vector small, low-dimensional, and easy to reason about.\\nAnd you can use this _count vectorizing_ approach even for large vocabularies.\\n\\nAnd you can organize these counts of those keywords into\\nYou need to organize the counts into a vector.\\nThis opens up a whole range of powerful tools for doing vector algebra.\\n\\nIn natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\\nNonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\\nThe techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email from a flood of spam that almost made it useless.\\n\\n\\n=== Naive Bayes\\n\\nA Naive Bayes model tries to find keywords in a set of documents that are predictive of your target (output) variable.\\nWhen your target variable is the sentiment you are trying to predict, the model will find words that predict that sentiment.\\nThe nice thing about a Naive Bayes model is that the internal coefficients will map words or tokens to scores just like VADER does.\\nOnly this time you will not have to be limited to just what an individual human decided those scores should be.\\nThe machine will find the \"best\" scores for any problem.\\n\\nFor any machine learning algorithm, you first need to find a dataset.\\nYou need a bunch of text documents that have labels for their positive emotional content (positivity sentiment).\\nHutto compiled four different sentiment datasets for us when he and his collaborators built VADER.\\nYou will load them from the `nlpia` package.footnote:[If you have not already installed `nlpia`, check out the installation instructions at http://gitlab.com/tangibleai/nlpia2.]\\n\\n[source,python]\\n----\\n>>> movies = pd.read_csv(\\'https://proai.org/movie-reviews.csv.gz\\', \\\\\\n...     index_col=0)\\n>>> movies.head().round(2)\\n    sentiment                                               text\\nid                                                              \\n1        2.27  The Rock is destined to be the 21st Century\\'s ...\\n2        3.53  The gorgeously elaborate continuation of \\'\\'The...\\n3       -0.60                     Effective but too tepid biopic\\n4        1.47  If you sometimes like to go to the movies to h...\\n5        1.73  Emerges as something rare, an issue movie that...\\n\\n>>> movies.describe().round(2)\\n       sentiment\\ncount   10605.00\\nmean        0.00  # <1>\\nstd         1.92\\nmin        -3.88  # <2>\\n...\\nmax         3.94  # <3>\\n----\\n<1> Sentiment scores (movie ratings) have been \"centered\" (mean is zero)\\n<2> It looks like the scale starts around -4 for the worst movies\\n<3> Seems like +4 is the maximum rating for the best movies\\n\\nIt looks like the movie reviews have been _centered_: normalized by subtracting the mean so that the new mean will be zero and they aren\\'t biased to one side or the other.\\nAnd it seems the range of movie ratings allowed was -4 to +4.\\n\\nNow you can tokenize all those movie review texts to create a bag of words for each one.\\nIf you put them all into a Pandas DataFrame that will make them easier to work with.\\n\\n[source,python]\\n----\\n>>> import pandas as pd\\n>>> pd.options.display.width = 75  # <1>\\n>>> from nltk.tokenize import casual_tokenize  # <2>\\n>>> bags_of_words = []\\n>>> from collections import Counter  # <3>\\n>>> for text in movies.text:\\n...     bags_of_words.append(Counter(casual_tokenize(text)))\\n>>> df_bows = pd.DataFrame.from_records(bags_of_words)  # <4>\\n>>> df_bows = df_bows.fillna(0).astype(int)  # <5>\\n>>> df_bows.shape  # <6>\\n(10605, 20756)\\n\\n>>> df_bows.head()\\n   !  \"  #  $  %  &  \\' ...  zone  zoning  zzzzzzzzz  ½  élan  –  ’\\n0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\\n1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\\n2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\\n\\n>>> df_bows.head()[list(bags_of_words[0].keys())]\\n   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  .\\n0    1     1   1         1   2   1 ...    1      1   1       1      1  1\\n1    2     0   1         0   0   0 ...    0      0   0       0      0  4\\n2    0     0   0         0   0   0 ...    0      0   0       0      0  0\\n3    0     0   1         0   4   0 ...    0      0   0       0      0  1\\n4    0     0   0         0   0   0 ...    0      0   0       0      0  1\\n----\\n<1> This prints a wide `DataFrame` in the console so they look prettier.\\n<2> NLTK\\'s `casual_tokenize` can handle emoticons, unusual punctuation, and slang better than `TreebankWordTokenizer`\\n<3> `Counter` takes a list (or iterable) of objects and counts them up, returning a `dict` where the keys are the objects (tokens in your case) and the values are the counts.\\n<4> The `from_records()` DataFrame constructor takes a sequence of dict objects. The `dict` keys become columns, and the values for missing keys are set to `NaN`.\\n<5> NumPy and Pandas can only represent NaNs within a float dtype. So fill NaNs with zeros before converting to integers.\\n<6> A BOW table can get really big if you don\\'t do dimension reduction or feature selection.\\n\\nWhen you do not use case normalization, stop word filters, stemming, or lemmatization your vocabulary can be quite huge because you are keeping track of every little difference in spelling or capitalization of words.\\nTry inserting some dimension reduction steps into your pipeline to see how they affect your pipeline\\'s accuracy and the amount of memory required to store all these BOWs.\\n\\nNow you have all the data that a Naive Bayes model needs to find the keywords that predict sentiment from natural language text.\\n\\n[source,python]\\n----\\n>>> from sklearn.naive_bayes import MultinomialNB\\n>>> nb = MultinomialNB()\\n>>> nb = nb.fit(df_bows, movies.sentiment > 0)  # <1>\\n>>> movies[\\'pred_senti\\'] = (\\n...   nb.predict_proba(df_bows))[:, 1] * 8 - 4  # <2>\\n>>> movies[\\'error\\'] = movies.pred_senti - movies.sentiment\\n>>> mae = movies[\\'error\\'].abs().mean().round(1)  # <3>\\n>>> mae\\n1.9\\n----\\n\\nTo create a binary classification label you can use the fact that the centered movie ratings (sentiment labels) are positive (greater than zero) when the sentiment of the review is positive.\\n\\n[source,python]\\n----\\n>>> movies[\\'senti_ispos\\'] = (movies[\\'sentiment\\'] > 0).astype(int)\\n>>> movies[\\'pred_ispos\\'] = (movies[\\'pred_senti\\'] > 0).astype(int)\\n>>> columns = [c for c in movies.columns if \\'senti\\' in c or \\'pred\\' in c]\\n>>> movies[columns].head(8)\\n    sentiment  pred_senti  senti_ispos  pred_ispos\\nid\\n1    2.266667                    4                     1                     1\\n2    3.533333                    4                     1                     1\\n3   -0.600000                   -4                     0                     0\\n4    1.466667                    4                     1                     1\\n5    1.733333                    4                     1                     1\\n6    2.533333                    4                     1                     1\\n7    2.466667                    4                     1                     1\\n8    1.266667                   -4                     1                     0\\n>>> (movies.pred_ispos ==\\n...   movies.senti_ispos).sum() / len(movies)\\n0.9344648750589345  # <4>\\n----\\n<1> Naive Bayes models are classifiers, so you need to convert your output variable (sentiment float) to a discrete label (integer, string, or bool).\\n<2> Convert your discrete classification variable back to a real value between -4 and +4 so you can compare it to the \"ground truth\" sentiment.\\n<3> Average absolute value of the prediction error or mean absolute error (MAE)\\n<4> You got the \"thumbs up\" rating correct 93% of the time.\\n\\nThis is a pretty good start at building a sentiment analyzer with only a few lines of code (and a lot of data).\\nYou did not have to guess at the sentiment associated with a list of 7500 words and hard code them into an algorithm such as VADER.\\nInstead you told the machine the sentiment ratings for whole text snippets.\\nAnd then the machine did all the work to figure out the sentiment associated with each word in those texts.\\nThat is the power of machine learning and NLP!\\n\\nHow well do you think this model will generalize to a completely different set text examples such as product reviews?\\nDo people use the same words to describe things they like in movie and product reviews such as electronics and household goods? \\nProbably not.\\nBut it\\'s a good idea to check the robustness of your language models by running it against challenging text from a different domain.\\nAnd by testing your model on new domains, you can get ideas for more examples and datasets to use in your training and test sets.\\n\\nFirst you need to load the product reviews.\\n\\n[source,python]\\n----\\n>>> products = pd.read_csv(\\'https://proai.org/product-reviews.csv.gz\\')\\n>>> for text in products[\\'text\\']:\\n...     bags_of_words.append(Counter(casual_tokenize(text)))\\n>>> df_product_bows = pd.DataFrame.from_records(bags_of_words)\\n>>> df_product_bows = df_product_bows.fillna(0).astype(int)\\n>>> df_all_bows = df_bows.append(df_product_bows)\\n>>> df_all_bows.columns  # <1>\\nIndex([\\'!\\', \\'\"\\', \\'#\\', \\'#38\\', \\'$\\', \\'%\\', \\'&\\', \\'\\'\\', \\'(\\', \\'(8\\',\\n       ...\\n       \\'zoomed\\', \\'zooming\\', \\'zooms\\', \\'zx\\', \\'zzzzzzzzz\\', \\'~\\', \\'½\\', \\'élan\\',\\n       \\'–\\', \\'’\\'],\\n      dtype=\\'object\\', length=23302)\\n>>> df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]  # <2>\\n>>> df_product_bows.shape\\n(3546, 20756)\\n\\n>>> df_bows.shape  # <3>\\n(10605, 20756)\\n----\\n<1> Your new bags of words have some tokens that were not in the original bags of words DataFrame (23302 columns now instead of 20756 before).\\n<2> You need to make sure your new product DataFrame of bags of words has the exact same columns (tokens) in the exact same order as the original one used to train your Naive Bayes model.\\n<3> The movie bags of words have the same size vocabulary (columns) as for products.\\n\\nNow you need to convert the labels to mimic the binary classification data that you trained your model on.\\n\\n[source,python]\\n----\\n>>> products[\\'senti_ispos\\'] = (products[\\'sentiment\\'] > 0).astype(int)\\n>>> products[\\'pred_ispos\\'] = nb.predict(df_product_bows).astype(int)\\n>>> products.head()\\n    id  sentiment                                               text  senti_ispos\\n0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...                     0\\n1  1_2      -0.15  repost from january 13, 2004 with a better fit...                     0\\n2  1_3      -0.20  does your apex dvd player only play dvd audio ...                     0\\n3  1_4      -0.10  or does it play audio and video but scrolling ...                     0\\n4  1_5      -0.50  before you try to return the player or waste h...                     0\\n\\n>>> tp = products[\\'pred_ispos\\'] == products[\\'senti_ispos\\']  # <1>\\n>>> tp.sum() / len(products)\\n0.5572476029328821\\n----\\n<1> True positive predictions are when both the predicted and true sentiment are positive\\n\\n\\nSo your Naive Bayes model does a  poor job of predicting whether a product review is positive (thumbs up).\\nOne reason for this subpar performance is that your vocabulary from the `casual_tokenize` product texts has 2546 tokens that were not in the movie reviews.\\nThat is about 10% of the tokens in your original movie review tokenization, which means that all those words will not have any weights or scores in your Naive Bayes model.\\nAlso the Naive Bayes model does not deal with negation as well as VADER does.\\nYou would need to incorporate _n_-grams into your tokenizer to connect negation words (such as \"not\" or \"never\") to the positive words they might be used to qualify.\\n\\nWe leave it to you to continue the NLP action by improving on this machine learning model.\\nAnd you can check your progress relative to VADER at each step of the way to see if you think machine learning is a better approach than hard-coding algorithms for NLP.\\n\\n== Review\\n\\n. How does a lemmatizer increase the likelihood that your DuckDuckGo search results contain what you are looking for?\\n. Is there a way to optimally decide the _n_ in the _n_-gram range you use to tokenize your documents?\\n. Does lemmatization, case folding, or stopword removal help or hurt your performance on a model to predict misleading news articles with this Kaggle dataset:\\n. How could your find out the best sizes for the word pieces or sentence pieces for your tokenizer?\\n. Is there a website where you can download the token frequencies for most of the words and n-grams ever published?footnote:[Hint: A company that aspired to \"do no evil\", but now does, created this massive NLP corpus.]\\n. What are the risks and possible benefits of pair coding AI assistants built with NLP? What sort of organizations and algorithms do you trust with your mind and your code?\\n\\n=== Summary\\n\\n* You implemented tokenization and configured a tokenizer for your application.\\n* _n_-gram tokenization helps retain some of the \"word order\" information in a document.\\n* Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\\n* Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\\n* Stop words can contain useful information, and discarding them is not always helpful.\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a382875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'160k'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(text) / 10_000)}0k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80c6d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.64 s ± 33.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "%timeit nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d0417bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'160k'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(text) / 10_000)}0k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5d89356",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15fabc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "= Tokens of thought (natural language words)\n",
       ":chapter: 2\n",
       ":part: 1\n",
       ":imagesdir: .\n",
       ":xrefstyle: short\n",
       ":figure-caption: Figure {chapter}.\n",
       ":listing-caption: Listing {chapter}.\n",
       ":table-caption: Table {chapter}.\n",
       ":stem: latexmath\n",
       "\n",
       "This chapter covers\n",
       "\n",
       "* Parsing your text into words and _n_-grams (tokens)\n",
       "* Tokenizing punctuation, emoticons, and even Chinese characters\n",
       "* Consolidating your vocabulary with stemming, lemmatization, and case folding\n",
       "* Building a structured numerical representation of natural language text\n",
       "* Scoring text for sentiment and prosocial intent\n",
       "* Using character frequency analysis to optimize your token vocabulary\n",
       "* Dealing with variable length sequences of words and tokens\n",
       "\n",
       "\n",
       "So you want to help save the world with the power of natural language processing (NLP)?\n",
       "First your NLP pipeline will need to compute something about text, and for that you'll need a way to represent text in a numerical data structure.\n",
       "The part of an NLP pipeline that breaks up your text to create this structured numerical data is called a _parser_.\n",
       "For many NLP applications, you only need to convert your text to a sequence of words, and that can be enough for searching and classifying text.\n",
       "\n",
       "You will now learn how to split a document, any string, into discrete tokens of meaning.\n",
       "You will be able to parse text documents as small as a single word and as large as an entire Encyclopedia.\n",
       "And they will all produce a consistent representation that you can use to compare them.\n",
       "For this chapter your tokens will be words, punctuation marks, and even pictograms such as Chinese characters, emojis and emoticons.\n",
       "\n",
       "Later in the book you will see that you can use these same techniques to find packets of meaning in any discrete sequence.\n",
       "For example, your tokens could be the ASCII characters represented by a sequence of bytes, perhaps with ASCII emoticons.\n",
       "Or they could be Unicode emojis, mathematical symbols, Egyption, hieroglyphics, pictographs from languages like Kanji  and Cantonese.\n",
       "You could even define the tokens for DNA and RNA sequences with letters for each of the five base nucleotides: adenine (A), guanine (G), cytosine \\(C), thymine (T), and uracil (U).\n",
       "Natural language sequences of tokens are all around you ... and even inside you.\n",
       "\n",
       "Is there something you can do with tokens that doesn't require a lot of complicated deep learning?\n",
       "If you have a good tokenizer you can use it to identify statistics about the occurrence of tokens in a set of documents, such as your blog posts or a business website.\n",
       "Then you can build a search engine in pure Python with just a dictionary to represent to record links to the set of documents where those words occur.\n",
       "That Python dictionary that maps words to document links or pages is called a reverse index.\n",
       "It's just like the index at the back of this book.\n",
       "This is called _information retrieval_ -- a really powerful tool in your NLP toolbox.\n",
       "\n",
       "Statistics about tokens are often all you need for keyword detection, full text search, and information retrieval.\n",
       "You can even build customer support chatbots using text search to find answers to customers' questions in your documentation or FAQ (frequently asked question) lists.\n",
       "A chatbot can't answer your questions until it knows where to look for the answer.\n",
       "Search is the foundation of many state of the art applications such as conversational AI and open domain question answering.\n",
       "A tokenizer forms the foundation for almost all NLP pipelines.\n",
       "\n",
       "=== Tokens of emotion\n",
       "\n",
       "Another practical use for your tokenizer is called _sentiment analysis_, or analysis of text to estimate emotion.\n",
       "You'll see an example of a sentiment analysis pipeline later in this chapter.\n",
       "For now you just need to know how to build a tokenizer.\n",
       "And your tokenizer will almost certainly need to handle the tokens of emotion called _emoticons_ and _emojis_.\n",
       "\n",
       "_Emoticons_ are a textual representations of a writer's mood or facial expression, such as the _smiley_ emoticon: `:-)`.\n",
       "They are kind-of like a modern hieroglyph or picture-word for computer users that only have access to an ASCII terminal for communication.\n",
       "_Emojis_ are the graphical representation of these characters.\n",
       "For example, the smilie emoji has a small yellow circle with two black dots for eyes and a U shaped curve for a mouth.\n",
       "The smiley emoji is a graphical representation of the `:-)` smiley emoticon.\n",
       "\n",
       "Both emojis and emoticons have evolved into their own language.\n",
       "There are hundreds of popular emojis.\n",
       "People have created emojis for everything from company logos to memes and innuendo.\n",
       "Noncommercial social media networks such Mastodon even allow you to create your own custom emojis.footnote:[Mastodon servers you can join (https://proai.org/mastoserv)] footnote:[Mastodon custom emoji documentation (https://docs.joinmastodon.org/methods/custom_emojis/)] \n",
       "\n",
       ".Emojis and Emoticons\n",
       "[NOTE]\n",
       "====\n",
       "_Emoticons_ were first typed into an ASCII text message in 1972 when Carnegie Mellon researchers mistakenly understood a text message about a mercury spill to be a joke.\n",
       "The professor, Dr. Scott E. Fahlman, suggested that `:-)` should be appended to messages that were jokes, and  `:-(` emoticons should be used for serious warning messages. \n",
       "Gosh, how far we've come.\n",
       "====\n",
       "\n",
       "The plural of \"emoji\" is either \"emoji\" (like \"sushi\") or \"emojis\" (like \"Tsunamis\"), however the the Atlantic and NY Times style editors prefer \"emojis\" to avoid ambiguity.\n",
       "Your NLP pipeline will learn what you mean no matter how you type it.\n",
       "\n",
       "image::../images/ch02/wikipedia-smiley-icon.svg[alt=\"Smiley icon from wikipedia article en.wikipedia.org/wiki/Smiley\",align=\"center\",width=100%,link=\"../images/ch02/wikipedia-smiley-icon.svg\"]\n",
       "\n",
       "== What is a token?\n",
       "\n",
       "A token can be almost any chunk of text that you want to treat as a packet of thought and emotion.\n",
       "So you need to break your text into chunks that capture individual thoughts.\n",
       "You may be thinking that _words_ are the obvious choice for tokens.\n",
       "So that's what you will start with here.\n",
       "You'll also learn how to include punctuation marks, emojis, numbers, and other word-like things in your vocabulary of words.\n",
       "Later you'll see that you can use these same techniques to find packets of meaning in any discrete sequence.\n",
       "And later you will learn some even more powerful ways to split discrete sequences into meaningful packets.\n",
       "Your tokenizers will be soon able to analyze and structure any text document or string, from a single word, to a sentence, to an entire book.\n",
       "\n",
       "Think about a collection of documents, called a _corpus_, that you want to process with NLP.\n",
       "Think about the _vocabulary_ that would be important to your NLP algorithm -- the set of tokens you will need to keep track of.\n",
       "For example your tokens could be the characters for ASCII emoticons, if this is what is important in your NLP pipeline for a particular corpus.\n",
       "Or your tokens could be Unicode emojis, mathematical symbols, hieroglyphics, even pictographs like Kanji and Cantonese characters.\n",
       "Your tokenizer and your NLP pipeline would even be useful for the nucleotide sequences of DNA and RNA where your tokens might be A, C, T, G, U, and so on.\n",
       "And neuroscientists sometimes create sequences of discrete symbols to represent neurons firing in your brain when you read text like this sentence.\n",
       "Natural language sequences of tokens are inside you, all around you, and flowing through you.\n",
       "Soon you'll be flowing streams of tokens through your machine learning NLP pipeline.\n",
       "\n",
       "Retrieving tokens from a document will require some string manipulation beyond just the `str.split()` method employed in chapter 1.\n",
       "You'll probably want to split contractions like \"you'll\" into the words that were combined to form them, perhaps \"you\" and \"'ll\", or perhaps \"you\" and \"will.\"\n",
       "You'll want to separate punctuation from words, like quotes at the beginning and end of quoted statements or words, such as those in the previous sentence.\n",
       "And you need to treat some punctuation such as dashes (\"-\") as part of singly-hyphenated compound words such as \"singly-hyphenated.\"\n",
       "\n",
       "Once you have identified the tokens in a document that you would like to include in your vocabulary, you will return to the regular expression toolbox to build a tokenizer.\n",
       "And you can use regular expressions combine different forms of a word into a single token in your vocabulary -- a process called _stemming_.\n",
       "Then you will assemble a vector representation of your documents called a _bag of words_.\n",
       "Finally, you will try to use this bag of words vector to see if it can help you improve upon the basic greeting recognizer at the end of chapter 1.\n",
       "\n",
       "=== Alternative tokens\n",
       "\n",
       "Words aren't the only packets of meaning we could use for our tokens.\n",
       "Think for a moment about what a word or token represents to you.\n",
       "Does it represent a single concept, or some blurry cloud of concepts?\n",
       "Could you always be sure to recognize where a word begins and ends?\n",
       "Are natural language words like programming language keywords that have precise spellings, definitions and grammatical rules for how to use them?\n",
       "Could you write software that reliably recognizes a word?\n",
       "\n",
       "Do you think of \"ice cream\" as one word or two?\n",
       "Or maybe even three?\n",
       "Aren't there at least two entries in your mental dictionary for \"ice\" and \"cream\" that are separate from your entry for the compound word \"ice cream\"?\n",
       "What about the contraction \"don't\"?\n",
       "Should that string of characters be split into one, or two, or even three packets of meaning?\n",
       "\n",
       "You might even want to divide words into even smaller meaningful parts.\n",
       "Word pieces such as the prefix \"pre\", the suffix \"fix\", or the interior syllable \"la\" all have meaning.\n",
       "You can use these word pieces to transfer what you learn about the meaning of one word to another similar word in your vocabulary.\n",
       "Your NLU pipeline can even use these pieces to understand new words.\n",
       "And your NLG pipeline can use the pieces to create new words that succinctly capture ideas or memes circulating in the collective consciousness.\n",
       "\n",
       "Your pipeline could break words into even smaller pieces.\n",
       "Letters, characters, or graphemes footnote:[(https://en.wikipedia.org/wiki/Grapheme)] carry sentiment and meaning too!footnote:[Suzi Park and Hyopil Shin _Grapheme-level Awareness in Word Embeddings for Morphologically Rich Languages_ (https://www.aclweb.org/anthology/L18-1471.pdf)]\n",
       "We haven't yet found the perfect encoding for packets of thought.\n",
       "And machines compute differently than brains.\n",
       "We explain language and concepts to each other in terms of words or terms.\n",
       "But machines can often see patterns in the use of characters that we miss.\n",
       "And for machines to be able to squeeze huge vocabularies into their limited RAM there are more efficient encodings for natural language.\n",
       "\n",
       "The optimal tokens for efficient computation are different from the packets of thought (words) that we humans use.\n",
       "Byte Pair Encoding (BPE), Word Piece Encoding, and Sentence Piece Encoding, each can help machines use natural language more efficiently.\n",
       "BPE finds the optimal groupings of characters (bytes) for your particular set of documents and strings.\n",
       "If you want an *explainable* encoding, use the word tokenizers of the previous sections.\n",
       "If you want more flexible and accurate predictions and generation of text, then BPE, WPE, or SPE may be better for your application.\n",
       "Like the bias variance trade-off, there's often a explainability/accuracy trade-off in NLP.\n",
       "\n",
       "What about invisible or implied words?\n",
       "Can you think of additional words that are implied by the single-word command \"Don't!\"?\n",
       "If you can force yourself to think like a machine and then switch back to thinking like a human, you might realize that there are three invisible words in that command.\n",
       "The single statement \"Don't!\" means \"Don't you do that!\" or \"You, do not do that!\"\n",
       "That's at least three hidden packets of meaning for a total of five tokens you'd like your machine to know about.\n",
       "\n",
       "But don't worry about invisible words for now.\n",
       "All you need for this chapter is a tokenizer that can recognize words that are spelled out.\n",
       "You will worry about implied words and connotation and even meaning itself in chapter 4 and beyond.footnote:[If you want to learn more about exactly what a \"word\" really is, check out the introduction to _The Morphology of Chinese_ by Jerome Packard where he discusses the concept of a \"word\" in detail. The concept of a \"word\" did not exist at all in the Chinese language until the 20th century when it was translated from English grammar into Chinese.]\n",
       "\n",
       "Your NLP pipeline can start with one of these five options as your tokens:\n",
       "\n",
       "1. **Bytes** - ASCII characters\n",
       "2. **Characters** - multi-byte Unicode characters\n",
       "3. **Subwords** (Word pieces) - syllables and common character clusters\n",
       "4. **Words** - dictionary words or their roots (stems, lemmas)\n",
       "5. **Sentence pieces** - short, common word and multi-word pieces\n",
       "\n",
       "As you work your way down this list your vocabulary size increases and your NLP pipeline will need more and more data to train.\n",
       "Character-based NLP pipelines are often used in translation problems or NLG tasks that need to generalize from a modest number of examples.\n",
       "The number of possible words that your pipeline can deal with is called its _vocabulary_.\n",
       "A character-based NLP pipeline typically needs fewer than 200 possible tokens to process many Latin-based languages.\n",
       "That small vocabulary ensures that byte- and character-based NLP pipelines can handle new unseen test examples without too many meaningless OOV (out of vocabulary) tokens.\n",
       "\n",
       "For word-based NLP pipelines your pipeline will need to start paying attention to how often tokens are used before deciding whether to \"count it.\"\n",
       "You don't want you pipeline to do anything meaningful with junk words such `asdf` - the \n",
       "But even if you make sure your pipeline on pays attention to words that occur a lot, you could end up with a vocabulary that's as large as a typical dictionary - 20 to 50 thousand words.\n",
       "\n",
       "Subwords are the optimal token to use for most Deep Learning NLP pipelines.\n",
       "Subword (Word piece) tokenizers are built into many state of the art transformer pipelines.\n",
       "Words are the token of choice for any linguistics project or academic research where your results need to be interpretable and explainable.\n",
       "\n",
       "Sentence pieces take the subword algorithm to the extreme.\n",
       "The sentence piece tokenizer allows your algorithm to combine multiple word pieces together into a single token that can sometimes span multiple words.\n",
       "The only hard limit on sentence pieces is that they do not extend past the end of a sentence.\n",
       "This ensures that the meaning of a token is associated with only a single coherent thought and is useful on single sentences as well as longer documents.W\n",
       "\n",
       "==== _N_-grams\n",
       "\n",
       "No matter which kind of token you use for your pipeline, you will likely extract pairs, triplets, quadruplets, and even quintuplets of tokens.\n",
       "These are called _n_-grams_.footnote:[Pairs of adjacent words are called 2-grams or bigrams. Three words in sequency are called 3-grams or trigrams. Four words in a row are called 4-grams.  5-grams are probably the longest _n_-grams you'll find in an NLP pipeline. Google counts all the 1 to 5-grams in nearly all the books ever written (https://books.google.com/ngrams).]\n",
       "Using _n_-grams enables your machine to know about the token \"ice cream\" as well as the individual tokens \"ice\" and \"cream\" that make it up.\n",
       "Another 2-gram that you'd like to keep together is \"Mr. Smith\".\n",
       "Your tokens and your vector representation of a document will likely want to have a place for \"Mr. Smith\" along with \"Mr.\" and \"Smith.\"\n",
       "\n",
       "You will start with a short list of keywords as your vocabulary.\n",
       "This helps to keep your data structures small and understandable and can make it easier to explain your results.\n",
       "Explainable models create insights that you can use to help your stakeholders, hopefully the users themselves (rather than investors), accomplish their goals.\n",
       "\n",
       "For now, you can just keep track of all the short _n_-grams of words in your vocabulary.\n",
       "But in chapter 3, you will learn how to estimate the importance of words based on their document frequency, or how often they occur.\n",
       "That way you can filter out pairs and triplets of words that rarely occur together.\n",
       "You will find that the approaches we show are not perfect.\n",
       "Feature extraction can rarely retain all the information content of the input data in any machine learning pipeline.\n",
       "That is part of the art of NLP, learning when your tokenizer needs to be adjusted to extract more or different information from your text for your particular applications.\n",
       "\n",
       "In natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\n",
       "Nonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\n",
       "The techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email technology from a flood of spam that almost made it useless.\n",
       "\n",
       "== Challenges (a preview of stemming)\n",
       "\n",
       "As an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\n",
       "Very smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\n",
       "Imagine how difficult that is.\n",
       "Imagine trying to remove verb endings like \"ing\" from \"ending\" so you would have a stem called \"end\" to represent both words.\n",
       "And you would like to stem the word \"running\" to \"run,\" so those two words are treated the same.\n",
       "And that is tricky because you have removed not only the \"ing\" but also the extra \"n.\"\n",
       "But you want the word \"sing\" to stay whole.\n",
       "You would not want to remove the \"ing\" ending from \"sing\" or you would end up with a single-letter \"s.\"\n",
       "\n",
       "Or imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens.\"\n",
       "Do isolated individual letters in a word or parts of a word provide any information at all about that word's meaning?\n",
       "Can the letters be misleading?\n",
       "Yes and yes.\n",
       "\n",
       "In this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\n",
       "Later, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you are interested in.\n",
       "From that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\n",
       "\n",
       "=== Tokenization\n",
       "\n",
       "In NLP, _tokenization_ is a particular kind of document _segmentation_.\n",
       "Segmentation breaks up text into smaller chunks or segments.\n",
       "The segments of text have less information than the whole.\n",
       "Documents can be segmented into paragraphs, paragraphs into sentences, sentences into phrases, and phrases into tokens (usually words and punctuation).\n",
       "In this chapter, we focus on segmenting text into _tokens_ with a _tokenizer_.\n",
       "\n",
       "You may have heard of tokenizers before.\n",
       "If you took a computer science class you likely learned about how programming language compilers work.\n",
       "A tokenizer that is used to compile computer languages is called a _scanner_ or _lexer_.\n",
       "In some cases your computer language parser can work directly on the computer code and doesn't need a tokenizer at all.\n",
       "And for natural language processing, the only parser typically outputs a vector representation, rather than  If the tokenizer functionality is not separated from the compiler, the parser is often called a scannerless _parser_.\n",
       "\n",
       "The set of valid tokens for a particular computer language is called the _vocabulary_ for that language, or more formally its _lexicon_.\n",
       "Linguistics and NLP researchers use the term \"lexicon\" to refer to a set of natural language tokens.\n",
       "The term \"vocabulary\" is the more natural way to refer to a set of natural language words or tokens.\n",
       "So that's what you will use here.\n",
       "\n",
       "The natural language equivalent of a computer language compiler is a natural language parser.\n",
       "A natural language tokenizer is called a _scanner_, or _lexer_, or _lexical analyzer_ in the computer language world.\n",
       "Modern computer language compilers combine the _lexer_ and _parser_ into a single lexer-parser algorithm.\n",
       "The vocabulary of a computer language is usually called a _lexicon_.\n",
       "And computer language compilers sometimes refer to tokens as _symbols_.\n",
       "\n",
       "Here are five important NLP terms.\n",
       "Along side them are some roughly equivalent terms used in computer science when talking about programming language compilers:\n",
       "\n",
       "* _tokenizer_ -- scanner, lexer, lexical analyzer\n",
       "* _vocabulary_ -- lexicon\n",
       "* _parser_ -- compiler\n",
       "* _token_, _term_, _word_, or _n-gram_ -- token or symbol\n",
       "* _statement_ -- statement or expression\n",
       "\n",
       "Tokenization is the first step in an NLP pipeline, so it can have a big impact on the rest of your pipeline.\n",
       "A tokenizer breaks unstructured data, natural language text, into chunks of information which can be counted as discrete elements.\n",
       "These counts of token occurrences in a document can be used directly as a vector representing that document.\n",
       "This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning.\n",
       "These counts can be used directly by a computer to trigger useful actions and responses.\n",
       "Or they might also be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\n",
       "The most common use for bag-of-words vectors created this way is for document retrieval, or search.\n",
       "\n",
       "== Your tokenizer toolbox\n",
       "\n",
       "So each application you encounter you will want to think about which kind of tokenizer is appropriate for your application.\n",
       "And once you decide which kinds of tokens you want to try, you'll need to configure a python package for accomplishing that goal.\n",
       "\n",
       "You can chose from several tokenizer implementations: footnote:[Lysandre explains the various tokenizer options in the Huggingface documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\n",
       "\n",
       ". Python: `str.split`, `re.split`\n",
       ". NLTK: `PennTreebankTokenizer`, `TweetTokenizer`\n",
       ". spaCy: state of the art tokenization is its reason for being\n",
       ". Stanford CoreNLP: linguistically accurate, requires Java interpreter\n",
       ". Huggingface: `BertTokenizer`, a `WordPiece` tokenizer\n",
       "\n",
       "=== The simplest tokenizer\n",
       "\n",
       "The simplest way to tokenize a sentence is to use whitespace within a string as the \"delimiter\" of words. In Python, this can be accomplished with the standard library method `split`, which is available on all `str` object instances as well as on the `str` built-in class itself.\n",
       "\n",
       "Let's say your NLP pipeline needs to parse quotes from WikiQuote.org, and it's having trouble with one titled _The Book Thief_.footnote:[Markus Zusak, _The Book Thief_, p. 85 (https://en.wikiquote.org/wiki/The_Book_Thief)]\n",
       "\n",
       "\n",
       "[[book_thief_sentence_split_py]]\n",
       ".Example quote from _The Book Thief_ split into tokens\n",
       "[source,python]\n",
       "----\n",
       ">>> text = (\"Trust me, though, the words were on their way, and when \"\n",
       "...         \"they arrived, Liesel would hold them in her hands like \"\n",
       "...         \"the clouds, and she would wring them out, like the rain.\")\n",
       ">>> tokens = text.split()\n",
       ">>> tokens[:8]\n",
       "['Trust', 'me,', 'though,', 'the', 'words', 'were', 'on', 'their']\n",
       "----\n",
       "\n",
       "\n",
       ".Tokenized phrase\n",
       "image::../images/ch02/book-thief-split.png[alt=\"Trust|me,|though,|the|words|were|on|their\",align=\"center\",width=100%,link=\"../images/ch02/book-thief-split.png\"]\n",
       "\n",
       "As you can see, this built-in Python method does an OK job of tokenizing this sentence.\n",
       "Its only \"mistake\" is to include commas within the tokens.\n",
       "This would prevent your keyword detector from detecting quite a few important tokens: `['me', 'though', 'way', 'arrived', 'clouds', 'out', \"rain\"]`.\n",
       "Those words \"clouds\" and \"rain\" are pretty important to the meaning of this text.\n",
       "So you'll need to do a bit better with your tokenizer to ensure you can catch all the important words and \"hold\" them like Liesel.\n",
       "\n",
       "=== Rule-based tokenization\n",
       "\n",
       "It turns out there is a simple fix to the challenge of splitting punctuation from words.\n",
       "You can use a regular expression tokenizer to create rules to deal with common punctuation patterns.\n",
       "Here's just one particular regular expression you could use to deal with punctuation \"hanger-ons.\"\n",
       "And while we're at it, this regular expression will be smart about words that have internal punctuation, such as possessive words and contractions that contain apostrophes.\n",
       "\n",
       "You'll use a regular expression to tokenize some text from the book _Blindsight_ by Peter Watts.\n",
       "The text describes how the most _adequate_ humans tend to survive natural selection (and alien invasions).footnote:[Peter Watts, Blindsight, (https://rifters.com/real/Blindsight.htm)]\n",
       "The same goes for your tokenizer.\n",
       "You want to find an _adequate_ tokenizer that solves your problem, not the perfect tokenizer.\n",
       "You probably can't even guess what the _right_ or _fittest_ token is.\n",
       "You will need an accuracy number to evaluate your NLP pipeline with and that will tell you which tokenizer should survive your selection process.\n",
       "The example here should help you start to develop your intuition about applications for regular expression tokenizers.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import re\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
       ">>> texts = [text]\n",
       ">>> texts.append(\"There's no such thing as survival of the fittest. \"\n",
       "...              \"Survival of the most adequate, maybe.\")\n",
       ">>> tokens = list(re.findall(pattern, texts[-1]))\n",
       ">>> tokens[:8]\n",
       "[\"There's\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n",
       ">>> tokens[8:16]\n",
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n",
       ">>> tokens[16:]\n",
       "['maybe', '.']\n",
       "----\n",
       "<1> The _look-ahead_ pattern `(?:\\'\\w+)?` detects whether or not the word contains a single apostrophe followed by 1 or more letters.footnote:[Thank you Wiktor Stribiżew (https://stackoverflow.com/a/43094210/623735).]\n",
       "\n",
       "Much better.\n",
       "Now the tokenizer separates punctuation from the end of a word, but doesn't break up words that contain internal punctuation such as the apostrophe within the token \"There's.\"\n",
       "So all of these words were tokenized the way we wanted: \"There's\", \"fittest\", \"maybe\".\n",
       "And this regular expression tokenizer will work fine on contractions even if they have more than one letter after the apostrophe such as \"can't\", \"she'll\", \"what've\".\n",
       "It will work even typos such as 'can\"t' and \"she,ll\", and \"what`ve\".\n",
       "But this liberal matching of internal punctuation probably isn't what you want if your text contains rare double contractions such as \"couldn't've\", \"ya'll'll\", and \"y'ain't\"\n",
       "\n",
       "[TIP]\n",
       "=====\n",
       "Pro tip: You can accommodate double-contractions with the regular expression `r'\\w+(?:\\'\\w+){0,2}|[^\\w\\s]'`\n",
       "=====\n",
       "\n",
       "This is the main idea to keep in mind.\n",
       "No matter how carefully you craft your tokenizer, it will likely destroy some amount of information in your raw text.\n",
       "As you are cutting up text, you just want to make sure the information you leave on the cutting room floor isn't necessary for your pipeline to do a good job.\n",
       "Also, it helps to think about your downstream NLP algorithms.\n",
       "Later you may configure a case folding, stemming, lemmatizing, synonym substitution, or count vectorizing algorithm.\n",
       "When you do, you'll have to think about what your tokenizer is doing, so your whole pipeline works together to accomplish your desired output.\n",
       "\n",
       "\n",
       "////\n",
       "// too much regex detail?\n",
       "\n",
       "==== How regular expressions work\n",
       "\n",
       "Here is how the regular expression in <<listing_2_7>> works.\n",
       "\n",
       "The square brackets (`[` and `]`) are used to indicate a _character class_, a set of characters.\n",
       "The plus sign after the closing square bracket (`]`) means that a match must contain one or more of the characters inside the square brackets.\n",
       "The `\\s` within the character class is a shortcut to a predefined character class that includes all whitespace characters like those created when you press the `[space]`, `[tab]`, and `[return]` keys.\n",
       "The character class `r'[\\s]'` is equivalent to `r'[ \\t\\r\\n\\f]'`.\n",
       "The six whitespace characters are space (`' '`), tab (`'\\t'`), return (`'\\r'`), newline  (`'\\n'`), and form-feed (`'\\f'`).\n",
       "\n",
       "You did not use any character ranges here, but you may want to later.\n",
       "A character range is a special kind of character class indicated within square brackets and a hyphen like `r'[a-z]'` to match all lowercase letters.\n",
       "The character range `r'[0-9]'` matches any digit 0 through 9 and is equivalent to `r'[0123456789]'`).\n",
       "The regular expression `r'[\\_a-zA-Z]'` would match any underscore character (`r'\\_'`) or letter of the English alphabet (upper or lower case).\n",
       "\n",
       "The hyphen (`-`) right after the opening square bracket is a bit of quirk of regexes.\n",
       "You cannot put a hyphen just anywhere inside your square brackets because the regex parser may think you mean a character range like `r'[0-9]'`.\n",
       "So whenever you want to indicate an actual hyphen (dash) character in your character class, you need to make sure it is the first character after the open square bracket, or you need to escape it with a backslash (`\\`).\n",
       "\n",
       "The `re.split` function goes through each character in the input string (the second argument, `sentence`) left to right looking for any matches based on the \"program\" or \"pattern\" in the regular expression (the first argument, `r'[-\\s.,;!?]+'`).\n",
       "When it finds a match, it breaks the string right before that matched character and right after it, skipping over the matched character or characters.\n",
       "So the `re.split` line will work just like `str.split`, but it will work for any kind of character or multicharacter sequence that matches your regular expression.\n",
       "\n",
       "The parentheses (`(` and `)`) are used to group regular expressions just like they are used to group mathematical, Python, and most other programming language expressions.\n",
       "These parentheses force the regular expression to match the entire expression within the parentheses before moving on to try to match the characters that follow the parentheses.\n",
       "\n",
       "// TODO: TMI?\n",
       "////\n",
       "\n",
       "Take a look at the first few tokens in your lexographically sorted vocabulary for this short text:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import numpy as np  # <1>\n",
       ">>> vocab = sorted(set(tokens))  # <2>\n",
       ">>> ' '.join(vocab[:12])  # <3>\n",
       "\", . Survival There's adequate as fittest maybe most no of such\"\n",
       ">>> num_tokens = len(tokens)\n",
       ">>> num_tokens\n",
       "18\n",
       ">>> vocab_size = len(vocab)\n",
       ">>> vocab_size\n",
       "15\n",
       "----\n",
       "<1> `str.split()` is your quick-and-dirty tokenizer.\n",
       "<2> Coercing the `list` into a `set` ensures that your vocabulary contains only *unique* tokens that you want to keep track of.\n",
       "<3> Sorted lexographically (lexically) so punctuation comes before letters, and capital letters come before lowercase letters.\n",
       "\n",
       "You can see how you may want to consider lowercasing all your tokens so that \"Survival\" is recognized as the same word as \"survival\".\n",
       "And you may want to have a synonym substitution algorithm to replace \"There's\" with \"There is\" for similar reasons.\n",
       "However, this would only work if your tokenizer kept contraction and possessive apostrophes attached to their parent token.\n",
       "\n",
       "[TIP]\n",
       "=====\n",
       "Make sure you take a look at your vocabulary whenever it seems your pipeline isn't working well for a particular text.\n",
       "You may need to revise your tokenizer to make sure it can \"see\" all the tokens it needs to do well for your NLP task.\n",
       "=====\n",
       "\n",
       "\n",
       "=== SpaCy\n",
       "\n",
       "Maybe you don't want your regular expression tokenizer to keep contractions together.\n",
       "Perhaps you'd like to recognize the word \"isn't\" as two separate words, \"is\" and \"n't\".\n",
       "That way you could consolidate the synonyms \"n't\" and \"not\" into a single token.\n",
       "This way your NLP pipeline would understand \"the ice cream isn't bad\" to mean the same thing as \"the ice cream is not bad\".\n",
       "For some applications, such as full text search, intent recognition, and sentiment analysis, you want to be able to *uncontract* or expand contractions like this.\n",
       "By splitting contractions, you can use synonym substitution or contraction expansion to improve the recall of your search engine and the accuracy of your sentiment analysis.\n",
       "\n",
       "[IMPORTANT]\n",
       "====\n",
       "We'll discuss case folding, stemming, lemmatization, and synonym substitution later in this chapter.\n",
       "Be careful about using these techniques for applications such as authorship attribution, style transfer, or text fingerprinting.\n",
       "You want your authorship attribution or style-transfer pipeline to stay true to the author's writing style and the exact spelling of words that they use.\n",
       "====\n",
       "\n",
       "SpaCy integrates a tokenizer directly into its state-of-the-art NLU pipeline.\n",
       "In fact the name \"spaCy\" is based on the word \"space\", as in the separator used in Western languages to separate words.\n",
       "And spaCy adds a lot of additional _tags_ to tokens at the same time that it is applying rules to split tokens apart.\n",
       "So spaCy is often the first and last tokenizer you'll ever need to use.\n",
       "\n",
       "Let's see how spaCy handles our collection of deep thinker quotes:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy  # <1>\n",
       ">>> nlp = spacy.load('en_core_web_sm')  # <2>\n",
       ">>> doc = nlp(texts[-1])\n",
       ">>> type(doc)\n",
       "<class 'spacy.tokens.doc.Doc'>\n",
       "\n",
       ">>> tokens = [tok.text for tok in doc]\n",
       ">>> tokens[:9]\n",
       "['There', \"'s\", 'no', 'such', 'thing', 'as', 'survival', 'of', 'the']\n",
       "\n",
       ">>> tokens[9:17]\n",
       "['fittest', '.', 'Survival', 'of', 'the', 'most', 'adequate', ',']\n",
       "----\n",
       "<1> If this is your first time to use spacy you should download the small language model with `spacy.cli.download('en_core_web_sm')`\n",
       "<2> `sm` stands for \"small\" (17 MB), `md` is medium (45 MB), `lg` is \"large\" (780 MB)\n",
       "\n",
       "That tokenization may be more useful to you if you're comparing your results to academic papers or colleagues at work.\n",
       "Spacy is doing a lot more under the hood.\n",
       "That small language model you downloaded is also identifying sentence breaks with some *sentence boundary detection* rules.\n",
       "A language model is a collection of regular expressions and finite state automata (rules).\n",
       "These rules are a lot like the grammar and spelling rules you learned in English class.\n",
       "They are used in the algorithms that tokenize and label your words with useful things like their part of speech and their position in a syntax tree of relationships between words.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from spacy import displacy\n",
       ">>> sentence = list(doc.sents)[0] # <1>\n",
       ">>> displacy.serve(sentence, style=\"dep\")\n",
       ">>> !firefox 127.0.0.1:5000\n",
       "\n",
       "----\n",
       "<1> The first sentence begins with \"There's no such thing...\"\n",
       "\n",
       "If you browse to your `localhost` on port 5000 you should see a sentence diagram that may be even more correct than what you could produce in school:\n",
       "\n",
       "image::../images/ch02/there-such-thing.png[alt=\"NOUN Survival -> ADV maybe. ADJ adequate -> ADV most\",align=\"center\",width=100%,link=\"../images/ch02/there-such-thing.png\"]\n",
       "\n",
       "You can see that spaCy does a lot more than simply separate text into tokens.\n",
       "It identifies sentence boundaries to automatically segment your text into sentences.\n",
       "And it tags tokens with various attributes like their part of speech (PoS) and even their role within the syntax of a sentence.\n",
       "You can see the lemmas displayed by `displacy`  beneath the literal text for each token.footnote:[nlpia2 source code for chapter 2 (https://proai.org/nlpia2-ch2) has additional spaCy and displacy options and examples.]\n",
       "Later in the chapter we'll explain how lemmatization and case folding and other vocabulary *compression* approaches can be helpful for some applications.\n",
       "\n",
       "So spaCy seems pretty great in terms of accuracy and some \"batteries included\" features, such as all those token tags for lemmas and dependencies.\n",
       "What about speed?\n",
       "\n",
       "=== Tokenizer race\n",
       "\n",
       "SpaCy can parse the AsciiDoc text for a chapter in this book in about 5 seconds.\n",
       "First download the AsciiDoc text file for this chapter:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import requests\n",
       ">>> text = requests.get('https://proai.org/nlpia2-ch2.adoc').text\n",
       ">>> f'{round(len(text) / 10_000)}0k'\n",
       "'160k'\n",
       "----\n",
       "<1> I divided by 10,000 and rounded it, so that Doctests would continue to pass as I revise this text.\n",
       "\n",
       "There were about 160 thousand ASCII characters in this AsciiDoc file where I wrote this sentence that you are reading right now.\n",
       "What does that mean in terms of words-per-second, the standard benchmark for tokenizer speed?\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load('en_core_web_sm')\n",
       ">>> %timeit nlp(text)  # <1>\n",
       "4.67 s ± 45.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
       "\n",
       ">>> f'{round(len(text) / 10_000)}0k'\n",
       "'160k'\n",
       ">>> doc = nlp(text)\n",
       ">>> f'{round(len(list(doc)) / 10_000)}0k'\n",
       "'30k'\n",
       ">>> f'{round(len(doc) / 1_000 / 4.67)}kWPS'  # <2> \n",
       "'7kWPS'\n",
       "----\n",
       "<1> `%timeit` is a magic function within `jupyter notebook`, `jupyter console` or `ipython`\n",
       "<2> kWPS is for thousands of words (tokens) per second\n",
       "\n",
       "That's nearly 5 seconds for about 150,000 characters or 34,000 words of English and Python text or about 7000 words per second.\n",
       "\n",
       "That may seem fast enough for you on your personal projects.\n",
       "But on a medical records summarization project we needed to process thousands of large documents with a comparable amount of text as you find in this entire book.\n",
       "And the latency in our medical record summarization pipeline was a critical metric for the project.\n",
       "So this, full-featured spaCy pipeline would require at least 5 days to process 10,000 books such as NLPIA or typical medical records for 10,000 patients.\n",
       "\n",
       "If that's not fast enough for your application you can disable any of the tagging features of the spaCy pipeline that you do not need.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> nlp.pipe_names  # <1>\n",
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
       ">>> nlp = spacy.load('en_core_web_sm', disable=nlp.pipe_names)\n",
       ">>> %timeit nlp(text)\n",
       "199 ms ± 6.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
       "----\n",
       "<1> The `pipe_names` lists all the currently enabled elements of your spaCy `nlp` pipeline\n",
       "\n",
       "You can disable the pipeline elements you don't need to speed up the tokenizer:\n",
       "\n",
       "- `tok2vec`: word embeddings\n",
       "- `tagger`: part-of-speech (`.pos` and `.pos_`)\n",
       "- `parser`: syntax tree role\n",
       "- `attribute_ruler`: fine-grained POS and other tags\n",
       "- `lemmatizer`: lemma tagger\n",
       "- `ner`: named entity recognition tagger\n",
       "\n",
       "NLTK's `word_tokenize` method is often used as the pace setter in tokenizer benchmark speed comparisons:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import nltk\n",
       ">>> nltk.download('punkt')\n",
       "True\n",
       ">>> from nltk.tokenize import word_tokenize\n",
       ">>> %timeit word_tokenize(text)\n",
       "156 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
       ">>> tokens = word_tokenize(text)\n",
       ">>> f'{round(len(tokens) / 10_000)}0k'\n",
       "'30k'\n",
       "----\n",
       "\n",
       "Could it be that you found a winner for the tokenizer race?\n",
       "Not so fast.\n",
       "Your regular expression tokenizer has some pretty simple rules, so it should run pretty fast as well:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n",
       ">>> tokens = re.findall(pattern, text)  # <1>\n",
       ">>> f'{round(len(tokens) / 10_000)}0k'\n",
       "'30k'\n",
       ">>> %timeit re.findall(pattern, text)\n",
       "8.77 ms ± 29.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
       "----\n",
       "<1> Try precompiling with `re.compile` to learn something about how smart the core Python developers are\n",
       "\n",
       "Now that's not surprising.\n",
       "Regular expressions can be compiled and run very efficiently within low level C routines in Python.\n",
       "\n",
       "[TIP]\n",
       "================\n",
       "Use a regular expression tokenizer when speed is more import than accuracy.\n",
       "If you do not need the additional linguistic tags that spaCy and other pipelines provide your tokenizer doesn't need to waste time trying to figure out those tags.footnote:[Andrew Long, \"Benchmarking Python NLP Tokenizers\" (https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5)]\n",
       "And each time you use a regular expression in the `re` or `regex` packages, a compiled and optimized version of it is cached in RAM.\n",
       "So there's usually no need to _precompile_ (using `re.compile()`) your regexes.\n",
       "================\n",
       "\n",
       "== Wordpiece tokenizers\n",
       "\n",
       "It probably felt natural to think of words as indivisible atomic chunks of meaning and thought.\n",
       "However, you did find some words that didn't clearly split on spaces or punctuation.\n",
       "And many compound words or named entities that you'd like to keep together have spaces within them.\n",
       "So it can help to dig a little deeper and think about the statistics of what makes a word.\n",
       "Think about how we can build up words from neighboring characters instead of cleaving text at separators such as spaces and punctuation.\n",
       "\n",
       "=== Clumping characters into sentence pieces\n",
       "\n",
       "Instead of thinking about breaking strings up into tokens, your tokenizer can look for characters that are used a lot right next to each other, such as \"i\" before \"e\".\n",
       "You can pair up characters and sequences of characters that belong together.footnote:[In many applications the term \"_n_-gram\" refers to character _n_-grams rather than word n-grams. For example the leading relational database PostgreSQL has a Trigram index which tokenizes your text into character 3-grams not word 3-grams. In this book, we use \"_n_-gram\" to refer to sequences of word grams and \"character _n_-grams\" when talking about sequences of characters.]\n",
       "These clumps of characters can become your tokens.\n",
       "An NLP pipeline only pays attention to the statistics of tokens.\n",
       "And hopefully these statistics will line up with our expectations for what a word is.\n",
       "\n",
       "Many of these character sequences will be whole words, or even compound words, but many will be pieces of words.\n",
       "In fact, all _subword tokenizers_ maintain a token within the vocabulary for every individual character in your vocabulary.\n",
       "This means it never needs to use an OOV (Out-of-Vocabulary) token, as long as any new text doesn't contain any new characters it hasn't seen before.\n",
       "Subword tokenizers attempt to optimally clump characters together to create tokens.\n",
       "Using the statistics of character n-gram counts it's possible for these algorithms to identify wordpieces and even sentence pieces that make good tokens.\n",
       "\n",
       "It may seem odd to identify words by clumping characters.\n",
       "But to a machine, the only obvious, consistent division between elements of meaning in a text is the boundary between bytes or characters.\n",
       "And the frequency with which characters are used together can help the machine identify the meaning associated with subword tokens such as individual syllables or parts of compound words.\n",
       "\n",
       "In English, even individual letters have subtle emotion (sentiment) and meaning (semantics) associated with them.\n",
       "However, there are only 26 unique letters in the English language.\n",
       "That doesn't leave room for individual letters to _specialize_ on any one topic or emotion.\n",
       "Nonetheless savvy marketers know that some letters are cooler than others.\n",
       "Brands will try to portray themselves as technologically advanced by choosing names with exotic letters like \"Q\" and \"X\" or \"Z\".\n",
       "This also helps with SEO (Search Engine Optimization) because rarer letters are more easily found among the sea of possible company and product names.\n",
       "Your NLP pipeline will pick up all these hints of meaning, connotation, and intent.\n",
       "Your token counters will provide the machine with the statistics it needs to infer the meaning of clumps of letters that are used together often.\n",
       "\n",
       "The only disadvantage for subword tokenizers is the fact that they must pass through your corpus of text many times before converging on an optimal vocabulary and tokenizer.\n",
       "A subword tokenizer has to be trained or fit to your text just like a CountVectorizer.\n",
       "In fact you'll use a CountVectorizer in the next section to see how subword tokenizers work.\n",
       "\n",
       "There are two main approaches to subword tokenization: BPE (Byte-Pair Encoding) and Wordpiece tokenization.\n",
       "\n",
       "==== BPE\n",
       "\n",
       "In the previous edition of the book we insisted that words were the smallest unit of meaning in English that you need consider.\n",
       "With the rise of Transformers and other deep learning models that use BPE and similar techniques, we've changed our minds.footnote:[Hannes and Cole are probably screaming \"We told you so!\" as they read this.]\n",
       "Character-based subword tokenizers have proven to be more versatile and robust for most NLP problems.\n",
       "By building up a vocabulary from building blocks of Unicode multi-byte characters you can construct a vocabulary that can handle every possible natural language string you'll ever see, all with a vocabulary of as few as 50,000 tokens.\n",
       "\n",
       "You may think that Unicode characters are the smallest packet of meaning in natural language text.\n",
       "To a human, maybe, but to a machine, no way.\n",
       "Just as the BPE name suggests, characters don't have to be your fundamental atom of meaning for your _base vocabulary_.\n",
       "You can split characters into 8-bit bytes.\n",
       "GPT-2 uses a byte-level BPE tokenizer to naturally compose all the unicode characters you need from the bytes that make them up.\n",
       "Though some special rules are required to handle unicode punctuation within a byte-based vocabulary, no other adjustment to the character-based BPE algorithm is required.\n",
       "A byte-level BPE tokenizer allows you to represent all possible texts with a base (minimum) vocabulary size of 256 tokens.\n",
       "The GPT-2 model can achieve state-of-the-art performance with it's default BPE vocabulary of only 50,000 multibyte _merge tokens_ plus 256 individual byte tokens.\n",
       "\n",
       "You can think of the BPE (Byte Pair Encoding) tokenizer algorithm as a matchmaker or the hub in a social network.\n",
       "It connects characters together that appear next to each other a lot.\n",
       "It then creates a new token for these character combinations.\n",
       "And it keeps doing this until it has a many frequently used character sequences as you've allowed in your vocabulary size limit.\n",
       "\n",
       "\n",
       "BPE is transforming the way we think about natural language tokens.\n",
       "NLP engineers are finally letting the data do the talking.\n",
       "Statistical thinking is better than human intuition when building an NLP pipeline.\n",
       "A machine can see how _most_ people use language.\n",
       "You are only familiar with what _you_ mean when you use particular words or syllables.\n",
       "Transformers have now surpassed human readers and writers at some natural language understanding and generation tasks, including finding meaning in subword tokens.\n",
       "\n",
       "One complication you have not yet encounter is the dilemma of what to do when you encounter a new word.\n",
       "In the previous examples, we just keep adding new words to our vocabulary.\n",
       "But in the real world your pipeline will have been trained on an initial corpus of documents that may or may not represent all the kinds of tokens it will ever see.\n",
       "If your initial corpus is missing some of the words that you encounter later on, you will not have a slot in your vocabulary to put your counts of that new word.\n",
       "So when you train you initial pipeline, you will always reserve a slot (dimension) to hold the counts of your _out-of-vocabulary_ (OOV) tokens.\n",
       "So if your original set of documents did not contain the girl's name \"Aphra\", all counts of the name Aphra would be lumped into the OOV dimension as counts of Amandine and other rare words.\n",
       "\n",
       "To give Aphra equal representation in your vector space, you can use BPE.\n",
       "BPE breaks down rare words into smaller pieces to create a _periodic table_ of the elements for natural language in your corpus.\n",
       "So, because \"aphr\" is a common english prefix, your BPE tokenizer would probably give Aphra *two* slots for her counts in your vocabulary: one for \"aphr\" and one for \"a\".\n",
       "Actually, you might actually discover that the vobcabulary slots are for \" aphr\" and \"a \", because BPE keeps track of spaces no differently than any other character in your alphabet.footnote:[Actually, the string representation of tokens used for BPE and Wordpiece tokenizer place marker characters at the beginning or end of the token string indicate the absence of a word boundary (typically a space or punctuation). So you may see the \"aphr##\" token in your BPE vocabulary for the prefix \"aphr\" in aphrodesiac (https://stackoverflow.com/a/55416944/623735)]\n",
       "\n",
       "BPE gives you multilingual flexibility to deal with Hebrew names like Aphra.\n",
       "And it give your pipeline robustness against common misspellings and typos, such as \"aphradesiac.\"\n",
       "Every word, including minority 2-grams such as \"African American\", have representation in the voting system of BPE.footnote:[Discriminatory voting restriction laws have recently been passed in US: (https://proai.org/apnews-wisconsin-restricts-blacks)]\n",
       "Gone are the days of using the kluge of OOV (Out-of-Vocabulary) tokens to handle the rare quirks of human communication.\n",
       "Because of this, state of the art deep learning NLP pipelines such as transformers all use word piece tokenization similar to BPE.footnote:[See chapter 12 for information about another similar tokenizer -- sentence piece tokenizer]\n",
       "\n",
       "BPE preserves some of the meaning of new words by using character tokens and word-piece tokens to spell out any unknown words or parts of words.\n",
       "For example, if \"syzygy\" is not in our vocabulary, we could represent it as the six tokens \"s\", \"y\", \"z\", \"y\", \"g\", and \"y\".\n",
       "Perhaps \"smartz\" could be represented as the two tokens \"smart\" and \"z\".\n",
       "\n",
       "That sounds smart.\n",
       "Let's see how it works on our text corpus:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
       ">>> vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n",
       ">>> vectorizer.fit(texts)\n",
       "CountVectorizer(analyzer='char', ngram_range=(1, 2))\n",
       "\n",
       ">>> bpevocab = vectorizer.get_feature_names()\n",
       ">>> bpevocab[:7]\n",
       "[' ', ' a', ' c', ' f', ' h', ' i', ' l']\n",
       "----\n",
       "\n",
       "We configured the `CountVectorizer` to split the text into all the possible character 1-grams and 2-grams found in the texts.\n",
       "And `CountVectorizer` organizes the vocabulary in lexical order, so n-grams that start with a space character (`' '`) come first.\n",
       "Once the vectorizer knows what tokens it needs to be able to count, it can transform text strings into vectors, with one dimension for every token in your character n-gram vocabulary.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> vectors = vectorizer.transform(texts)\n",
       ">>> df = pd.DataFrame(vectors.todense(), columns=bpevocab)\n",
       ">>> df.index = [t[:8] + '...' for t in texts]\n",
       ">>> df = df.T\n",
       ">>> df['total'] = df.T.sum()\n",
       ">>> df\n",
       "    Trust me...  There's ...  total\n",
       "             31           14     45\n",
       " a            3            2      5\n",
       " c            1            0      1\n",
       " f            0            1      1\n",
       " h            3            0      3\n",
       "..          ...          ...    ...\n",
       "wr            1            0      1\n",
       "y             2            1      3\n",
       "y             1            0      1\n",
       "y,            1            0      1\n",
       "yb            0            1      1\n",
       "<BLANKLINE>\n",
       "[148 rows x 3 columns]\n",
       "----\n",
       "\n",
       "The DataFrame contains a column for each sentence and a row for each character 2-gram.\n",
       "Check out the top four rows where the byte pair (character 2-gram) of \" a\" is seen to occur five times in these two sentences.\n",
       "So even spaces count as \"characters\" when you're building a BPE tokenizer.\n",
       "This is one of the advantages of BPE, it will figure out what your token delimiters are, so it will work even in languages where there is no whitespace between words.\n",
       "And BPE will work on substitution cypher text like ROT13, a toy cypher that rotates the alphabet 13 characters forward.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> df.sort_values('total').tail()\n",
       "        Trust me...  There's ...  total\n",
       "    he           10            3     13\n",
       "    h            14            5     19\n",
       "    t            11            9     20\n",
       "    e            18            8     26\n",
       "                 31           14     45\n",
       "----\n",
       "\n",
       "A BPE tokenizer then finds the most frequent 2-grams and adds them to the permanent vocabulary.\n",
       "Over time it deletes the less frequent character pairs as it gets less and less likely that they won't come up a lot more later in your text.\n",
       "\n",
       "----\n",
       ">>> df['n'] = [len(tok) for tok in bpevocab]\n",
       ">>> df[df['n'] > 1].sort_values('total').tail()\n",
       "    Trust me...  There's ...  total  n\n",
       ",             6            1      7  2\n",
       "e             7            2      9  2\n",
       " t            8            3     11  2\n",
       "th            8            4     12  2\n",
       "he           10            3     13  2\n",
       "----\n",
       "\n",
       "So the next round of preprocessing in the BPE tokenizer would retain the character 2-grams \"he\" and \"th\" and even \" t\" and \"e \".\n",
       "Then the BPE algorithm would make another pass through the text with this smaller character bigram vocabulary.\n",
       "It would look for frequent pairings of these character bigrams with each other and individual characters.\n",
       "This process would continue until the maximum number of tokens is reached and the longest possible character sequences have been incorporated into the vocabulary.\n",
       "\n",
       "[NOTE]\n",
       "====\n",
       "You may see mention of _wordpiece_ tokenizers which are used within some advanced language models such as `BERT` and its derivatives.footnote:[Lysandre Debut explains all the variations on subword tokenizers in the Hugging Face transformers documentation (https://huggingface.co/transformers/tokenizer_summary.html)]\n",
       "It works the same as BPE, but it actually uses the underlying language model to predict the neighboring characters in string.\n",
       "It eliminates the characters from its vocabulary that hurt the accuracy of this language model the least.\n",
       "The math is subtly different and it produces subtly different token vocabularies, but you don't need to select this tokenizer intentionally.\n",
       "The models that use it will come with it built into their pipelines.\n",
       "====\n",
       "\n",
       "One big challenge of BPE-based tokenizers is that they must be trained on your individual corpus.\n",
       "So BPE tokenizers are usually only used for Transformers and Large Language Models (LLMs) which you will learn about in chapter 9.\n",
       "\n",
       "Another challenge of BPE tokenizers is all the book keeping you need to do to keep track of which trained tokenizer goes with each of your trained models.\n",
       "This was one of the big innovations of Huggingface.\n",
       "They made it easy to store and share all the preprocessing data, such as the tokenizer vocabulary, along side the language model.\n",
       "This makes it easier to reuse and share BPE tokenizers. \n",
       "If you want to become an NLP expert, you may want to imitate what they've done at HuggingFace with your own NLP preprocessing pipelines.footnote:[Huggingface documentation on tokenizers (https://huggingface.co/docs/transformers/tokenizer_summary)]\n",
       "\n",
       "== Vectors of tokens\n",
       "\n",
       "Now that you have broken your text into tokens of meaning, what do you do with them?\n",
       "How can you convert them to numbers that will be meaningful to the machine?\n",
       "The simplest most basic thing to do would be to detect whether a particular token you are interested in was present or not.\n",
       "You could hard-code the logic to check for important tokens, called a _keywords_.\n",
       "\n",
       "This might work well for your greeting intent recognizer in chapter 1.\n",
       "Our greeting intent recognizer at the end of chapter 1 looked for words like \"Hi\" and \"Hello\" at the beginning of a text string.\n",
       "Your new tokenized text would help you detect the presence or absence of words such as \"Hi\" and \"Hello\" without getting confused by words like \"Hiking\" and \"Hell.\"\n",
       "With your new tokenizer in place, your NLP pipeline wouldn't misinterpret the word \"Hiking\" as the greeting \"Hi king\":\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> hi_text = 'Hiking home now'\n",
       ">>> hi_text.startswith('Hi')\n",
       "True\n",
       ">>> pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'  # <1>\n",
       ">>> 'Hi' in re.findall(pattern, hi_text)  # <2>\n",
       "False\n",
       ">>> 'Hi' == re.findall(pattern, hi_text)[0]  # <3>\n",
       "False\n",
       "----\n",
       "<1> You can reuse the regular expression pattern from earlier to create a one-line tokenizer\n",
       "<2> 'Hi' is not among the 3 words (tokens) in this phrase\n",
       "<3> 'Hi' is definitely not the first word in this phrase\n",
       "\n",
       "So tokenization can help you reduce the number of false positives in your simple intent recognition pipeline that looks for the presence of greeting words.\n",
       "This is often called keyword detection, because your vocabulary of words is limited to a set of words you think are important.\n",
       "However, it's quite cumbersome to have to think of all the words that might appear in a greeting in order to recognize them all, including slang, misspellngs and typoos.\n",
       "And creating a for loop to iterate through them all would be inefficient.\n",
       "We can use the math of linear algebra and the vectorized operations of `numpy` to speed this process up.\n",
       "\n",
       "In order to detect tokens efficiently you will want to use three new tricks:\n",
       "\n",
       ". matrix and vector representations of documents\n",
       ". vectorized operations in numpy\n",
       ". indexing of discrete vectors\n",
       "\n",
       "You'll first learn the most basic, direct, raw and lossless way to represent words as a matrix, one-hot encoding.\n",
       "\n",
       "=== One-hot Vectors\n",
       "\n",
       "Now that you've successfully split your document into the kinds of words you want, you're ready to create vectors out of them.\n",
       "Vectors of numbers are what we need to do the math or processing of NL*P* on natural language text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> onehot_vectors = np.zeros(\n",
       "...     (len(tokens), vocab_size), int)  # <2>\n",
       ">>> for i, word in enumerate(tokens):\n",
       "...     onehot_vectors[i, vocab.index(word)] = 1  # <3>\n",
       ">>> df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\n",
       ">>> df_onehot.shape\n",
       "(18, 15)\n",
       ">>> df_onehot.iloc[:,:8].replace(0, '')  # <4>\n",
       "    ,  .  Survival  There's  adequate  as  fittest  maybe\n",
       "0                       1\n",
       "1\n",
       "2\n",
       "3\n",
       "4                                   1\n",
       "5\n",
       "6\n",
       "7\n",
       "8                                           1\n",
       "9      1\n",
       "10              1\n",
       "11\n",
       "12\n",
       "13\n",
       "14                               1\n",
       "15  1\n",
       "16                                                1\n",
       "17     1\n",
       "----\n",
       "<2> The table is as wide as your count of unique vocabulary terms and as tall as the length of your document: 18 rows, 15 columns\n",
       "<3> For each token in the sentence, mark the column for it with a `1`.\n",
       "<4> For brevity we're only showing the first 8 columns of the DataFrame and replaced 0's with ''.\n",
       "\n",
       "In this representation of this two-sentence quote, each row is a vector representation of a single word from the text.\n",
       "The table has the 15 columns because this is the number of unique words in your vocabulary.\n",
       "The table has 18 rows, one for each word in the document.\n",
       "A \"1\" in a column indicates a vocabulary word that was present at that position in the document.\n",
       "\n",
       "You can \"read\" a one-hot encoded (vectorized) text from top to bottom.\n",
       "You can tell that the first word in the text was the word \"There's\", because the `1` on the first row is positioned under the column label \"There's\".\n",
       "The next three rows (row indexes 1, 2, and 3) are blank, because we've truncated the table on the right to help it fit on the page.\n",
       "The fifth row of the text, with the 0-offset index number of `4` shows us that the fifth word in the text was the word \"adequate\", because there's a `1` in that column.\n",
       "\n",
       "One-hot vectors are super-sparse, containing only one nonzero value in each row vector.\n",
       "For display, this code replaces the `0`'s with empty strings (`''`), to make it easier to read.\n",
       "But the code did not actually alter the `DataFrame` of data you are processing in your NLP pipeline.\n",
       "The Python code above was just to to make it easier to read, so you can see that it looks a bit like a player piano paper roll, or maybe a music box drum.\n",
       "\n",
       "The Pandas `DataFrame` made this output a little easier to read and interpret.\n",
       "The `DataFrame.columns` keep track of labels for each column.\n",
       "This allows you to label each column in your table with a string, such as the token or word it represents.\n",
       "A `DataFrame` can also keep track of labels for each row in an the `DataFrame.index`, for speedy lookup.\n",
       "\n",
       "[IMPORTANT]\n",
       "====\n",
       "Don't add strings to any `DataFrame` you intend to use in your machine learning pipeline.\n",
       "The purpose of a tokenizer and vectorizer, like this one-hot vectorizer, is to create a numerical array that your NLP pipeline can do math on.\n",
       "You can't do math on strings.\n",
       "====\n",
       "\n",
       "Each row of the table is a binary row vector, and you can see why it's also called a one-hot vector: all but one of the positions (columns) in a row are `0` or blank.\n",
       "Only one column, or position in the vector is \"hot\" (\"1\").\n",
       "A one (`1`) means on, or hot.\n",
       "A zero (`0`) mean off, or absent.\n",
       "\n",
       "One nice feature of this vector representation of words and tabular representation of documents is that no information is lost.\n",
       "The exact sequence of tokens is encoded in the order of the one-hot vectors in the table representing a document.\n",
       "As long as you keep track of which words are indicated by which column, you can reconstruct the original sequence of tokens from this table of one-hot vectors perfectly.\n",
       "And this reconstruction process is 100% accurate even though your tokenizer was only 90% accurate at generating the tokens you thought would be useful.\n",
       "As a result, one-hot word vectors like this are typically used in neural nets, sequence-to-sequence language models, and generative language models.\n",
       "They are a good choice for any model or NLP pipeline that needs to retain all the meaning inherent in the original text.\n",
       "\n",
       "[TIP]\n",
       "====\n",
       "The one-hot encoder (vectorizer) did not discard any information from the text, but our tokenizer did.\n",
       "Our regular expression tokenizer discarded the whitespace characters (`\\s`) that sometimes occur between words.\n",
       "So you could not perfectly reconstruct the original text with a _detokenizer_.\n",
       "Tokenizers like spaCy, however, keep track of these whitespace characters and can in fact detokenize a sequence of tokens perfectly.\n",
       "SpaCy was named for this feature of accurately accounting for white-*space* efficiently and accurately.\n",
       "====\n",
       "\n",
       "This sequence of one-hot vectors is like a digital recording of the original text.\n",
       "If you squint hard enough you might be able to imagine that the matrix of ones and zeros above is a player piano roll.footnote:[See the \"Player piano\" article on Wikipedia (https://en.wikipedia.org/wiki/Player_piano).].\n",
       "Or maybe it's the bumps on the metal drum of a music box.footnote:[See the web page titled \"Music box - Wikipedia\" (https://en.wikipedia.org/wiki/Music_box).]\n",
       "The vocabulary key at the top tells the machine which \"note\" or word to play for each row in the sequence of words or piano music.\n",
       "\n",
       "[[player_piano_roll_jpg]]\n",
       ".Player piano roll\n",
       "image::../images/ch02/piano_roll.jpg[Player piano music roll with parallel sequences of holes running vertically down the paper. The holes meander left and right to indicate the rising and falling of the tones in the melody of a song. Image licensed from Wikimedia CC BY-SA 3.0 (https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg),width=100%,align=\"center\",link=\"https://commons.wikimedia.org/wiki/File:Weltereproduktionsklavier.jpg\"]\n",
       "\n",
       "Unlike a player-piano or a music box, your mechanical word recorder and player is only allowed to use one \"finger\" at a time.\n",
       "It can only play one \"note\" or word at a time.\n",
       "It's one-hot.\n",
       "And there is no variation in the spacing of the words.\n",
       "\n",
       "The important thing is that you've turned a sentence of natural language words into a sequence of numbers, or vectors.\n",
       "Now you can have the computer read and do math on the vectors just like any other vector or list of numbers.\n",
       "This allows your vectors to be input into any natural language processing pipeline that requires this kind of vector.\n",
       "The Deep Learning pipelines of chapter 5 through 10 typically require this representation, because they can be designed to extract \"features\" of meaning from these raw representations of text.\n",
       "And Deep Learning pipelines can generate text from numerical representations of meaning.\n",
       "So the stream of words emanating from your NLG pipelines in later chapters will often be represented by streams of one-hot encoded vectors, just like a player piano might play a song for a less artificial audience in West World.footnote:[West World is a television series about particularly malevolent humans and human-like robots, including one that plays a piano in the main bar.]\n",
       "\n",
       "Now all you need to do is figure out how to build a \"player piano\" that can _understand_ and combine those word vectors in new ways.\n",
       "Ultimately, you'd like your chatbot or NLP pipeline to play us a song, or say something, you haven't heard before.\n",
       "You'll get to do that in chapters 9 and 10 when you learn about recurrent neural networks that are effective for sequences of one-hot encoded tokens like this.\n",
       "\n",
       "This representation of a sentence in one-hot word vectors retains all the detail, grammar, and order of the original sentence.\n",
       "And you have successfully turned words into numbers that a computer can \"understand.\"\n",
       "They are also a particular kind of number that computers like a lot: binary numbers.\n",
       "But this is a big table for a short sentence.\n",
       "If you think about it, you have expanded the file size that would be required to store your document.\n",
       "For a long document this might not be practical.\n",
       "\n",
       "How big is this *lossless* numerical representation of your collection of documents?\n",
       "Your vocabulary size (the length of the vectors) would get huge.\n",
       "The English language contains at least 20,000 common words, millions if you include names and other proper nouns.\n",
       "And your one-hot vector representation requires a new table (matrix) for every document you want to process.\n",
       "This is almost like a raw \"image\" of your document.\n",
       "If you have done any image processing, you know that you need to do dimension reduction if you want to extract useful information from the data.\n",
       "\n",
       "Let's run through the math to give you an appreciation for just how big and unwieldy these \"piano rolls\" are.\n",
       "In most cases, the vocabulary of tokens you'll use in an NLP pipeline will be much more than 10,000 or 20,000 tokens.\n",
       "Sometimes it can be hundreds of thousands or even millions of tokens.\n",
       "Let's assume you have a million tokens in your NLP pipeline vocabulary.\n",
       "And let's say you have a meager 3000 books with 3500 sentences each and 15 words per sentence -- reasonable averages for short books.\n",
       "That's a whole lot of big tables (matrices), one for each book.\n",
       "That would use 157.5 terabytes.\n",
       "You probably couldn't even store that on disk.\n",
       "\n",
       "That is more than a million million bytes, even if you are super-efficient and use only one byte for each number in your matrix.\n",
       "At one byte per cell, you would need nearly 20 terabytes of storage for a small bookshelf of books processed this way.\n",
       "Fortunately you do not ever use this data structure for storing documents.\n",
       "You only use it temporarily, in RAM, while you are processing documents one word at a time.\n",
       "\n",
       "So storing all those zeros, and recording the order of the words in all your documents does not make much sense.\n",
       "It is not practical.\n",
       "And it's not very useful.\n",
       "Your data structure hasn't abstracted or generalized from the natural language text.\n",
       "An NLP pipeline like this doesn't yet do any real feature extraction or dimension reduction to help your machine learning work well in the real world.\n",
       "\n",
       "What you really want to do is compress the meaning of a document down to its essence.\n",
       "You would like to compress your document down to a single vector rather than a big table.\n",
       "And you are willing to give up perfect \"recall.\"\n",
       "You just want to capture most of the meaning (information) in a document, not all of it.\n",
       "\n",
       "=== BOW (Bag-of-Words) Vectors\n",
       "\n",
       "Is there any way to squeeze all those _player piano music rolls_ into a single vector?\n",
       "Vectors are a great way to represent any object.\n",
       "With vectors we could compare documents to each other just be checking the Euclidian distance between them.\n",
       "Vectors allow us to use all your linear algebra tools on natural language.\n",
       "And that's really the goal of NLP, doing math on text.\n",
       "\n",
       "Let us assume you can ignore the order of the words in our texts.\n",
       "For this first cut at a vector representation of text you can just jumble them all up together into a \"bag,\" one bag for each sentence or short document.\n",
       "It turns out just knowing what words are present in a document can give your NLU pipeline a lot of information about what's in it.\n",
       "This is in fact the representation that power big Internet search engine companies.\n",
       "Even for documents several pages long, a bag-of-words vector is useful for summarizing the essence of a document.\n",
       "\n",
       "Let's see what happens when we jumble and count the words in our text from _The Book Thief_:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> bow = sorted(set(re.findall(pattern, text)))\n",
       ">>> bow[:9]\n",
       "[',', '.', 'Liesel', 'Trust', 'and', 'arrived', 'clouds', 'hands', 'her']\n",
       ">>> bow[9:19]\n",
       "['hold', 'in', 'like', 'me', 'on', 'out', 'rain', 'she', 'the', 'their']\n",
       ">>> bow[19:27]\n",
       "['them', 'they', 'though', 'way', 'were', 'when', 'words', 'would']\n",
       "----\n",
       "\n",
       "Even with this jumbled up bag of words, you can get a general sense that this sentence is about:  \"Trust\", \"words\", \"clouds\", \"rain\", and someone named \"Liesel\".\n",
       "One thing you might notice is that Python's `sorted()` puts punctuation before characters, and capitalized words before lowercase words.\n",
       "This is the ordering of characters in the ASCII and Unicode character sets.\n",
       "However, the order of your vocabulary is unimportant.\n",
       "As long as you are consistent across all the documents you tokenize this way, a machine learning pipeline will work equally well with any vocabulary order.\n",
       "\n",
       "You can use this new bag-of-words vector approach to compress the information content for each document into a data structure that is easier to work with.\n",
       "For keyword search, you could **OR** your one-hot word vectors from the player piano roll representation into a binary bag-of-words vector.\n",
       "In the play piano analogy this is like playing several notes of a melody all at once, to create a \"chord\".\n",
       "Rather than \"replaying\" them one at a time in your NLU pipeline, you would create a single bag-of-words vector for each document.\n",
       "\n",
       "You could use this single vector to represent the whole document in a single vector.\n",
       "Because vectors all need to be the same length, your BOW vector would need to be as long your vocabulary size which is the number of unique tokens in your documents.\n",
       "And you could ignore a lot of words that would not be interesting as search terms or keywords.\n",
       "This is why stop words are often ignored when doing BOW tokenization.\n",
       "This is an extremely efficient representation for a search engine index or the first filter for an information retrieval system.\n",
       "Search indexes only need to know the presence or absence of each word in each document to help you find those documents later.\n",
       "\n",
       "This approach turns out to be critical to helping a machine \"understand\" a collection of words as a single mathematical object.\n",
       "And if you limit your tokens to the 10,000 most important words, you can compress your numerical representation of your imaginary 3500 sentence book down to 10 kilobytes, or about 30 megabytes for your imaginary 3000-book corpus.\n",
       "One-hot vector sequences for such a modest-sized corpus would require hundreds of gigabytes.\n",
       "\n",
       "Another advantage of the BOW representation of text is that it allows you to find similar documents in your corpus in constant time (`O(1)`).\n",
       "You can't get any faster than this.\n",
       "BOW vectors are the precursor to a reverse index which is what makes this speed possible.\n",
       "In computer science and software engineering, you are always on the lookout for data structures that enable this kind of speed.\n",
       "All major full text search tools use BOW vectors to find what you're looking for fast.\n",
       "You can see this numerical representation of natural language in EllasticSearch, Solr,footnote:[Apache Solr home page and Java source code (https://solr.apache.org/)] PostgreSQL, and even state of the art web search engines such as Qwant,footnote:[Qwant web search engine based in Europe (https://www.qwant.com/)], SearX,footnote:[SearX git repository (https://github.com/searx/searx) and web search (https://searx.thegpm.org/)], and Wolfram Alpha footnote:[(https://www.wolframalpha.com/)].\n",
       "\n",
       "Fortunately, the words in your vocabulary are sparsely utilized in any given text.\n",
       "And for most bag-of-words applications, we keep the documents short, sometimes just a sentence will do.\n",
       "So rather than hitting all the notes on a piano at once, your bag-of-words vector is more like a broad and pleasant piano chord, a combination of notes (words) that work well together and contain meaning.\n",
       "Your NLG pipeline or chatbot can handle these chords even if there is a lot of \"dissonance\" from words in the same statement that are not normally used together.\n",
       "Even dissonance (odd word usage) is useful information about a statement that a machine learning pipeline can make use of.\n",
       "\n",
       "Here is how you can put the tokens into a binary vector indicating the presence or absence of a particular word in a particular sentence.\n",
       "This vector representation of a set of sentences could be \"indexed\" to indicate which words were used in which document.\n",
       "This index is equivalent to the index you find at the end of many textbooks, except that instead of keeping track of which page a word occurs on, you can keep track of the sentence (or the associated vector) where it occurred.\n",
       "Whereas a textbook index generally only cares about important words relevant to the subject of the book, you keep track of every single word (at least for now).\n",
       "\n",
       "==== Sparse representations\n",
       "\n",
       "You might be thinking that if you process a huge corpus you'll probably end up with thousands or even millions of unique tokens in your vocabulary.\n",
       "This would mean you would have to store a lot of zeros in your vector representation of our 20-token sentence about Liesel.\n",
       "A `dict` would use much less memory than a vector.\n",
       "Any paired mapping of words to their 0/1 values would be more efficient than a vector.\n",
       "But you can't do math on `dict`'s.\n",
       "So this is why CountVectorizer uses a sparse numpy array to hold the counts of words in a word fequency vector.\n",
       "Using a dictionary or sparse array for your vector ensures that it only has to store a 1 when any one of the millions of possible words in your dictionary appear in a particular document.\n",
       "\n",
       "But if you want to look at an individual vector to make sure everything is working correctly, a Pandas `Series` is the way to go.\n",
       "And you will wrap that up in a Pandas DataFrame so you can add more sentences to your binary vector \"corpus\" of quotes.\n",
       "\n",
       "=== Dot product\n",
       "\n",
       "// TODO: some of this may belong in the discussion of keyword matching and one-hot vectors?\n",
       "You'll use the dot product a lot in NLP, so make sure you understand what it is.\n",
       "Skip this section if you can already do dot products in your head.\n",
       "\n",
       "The dot product is also called the _inner product_ because the \"inner\" dimension of the two vectors (the number of elements in each vector) or matrices (the rows of the first matrix and the columns of the second matrix) must be the same because that is where the products happen.\n",
       "This is analogous to an \"inner join\" on two relational database tables.\n",
       "\n",
       "The dot product is also called the _scalar product_ because it produces a single scalar value as its output.\n",
       "This helps distinguish it from the _cross product_, which produces a vector as its output.\n",
       "Obviously, these names reflect the shape of the symbols used to indicate the dot product (latexmath:[\\cdot]) and cross product (latexmath:[\\times]) in formal mathematical notation.\n",
       "The scalar value output by the scalar product can be calculated by multiplying all the elements of one vector by all the elements of a second vector and then adding up those normal multiplication products.\n",
       "\n",
       "Here is a Python snippet you can run in your Pythonic head to make sure you understand what a dot product is:\n",
       "\n",
       "[[example_dot_product_code]]\n",
       ".Example dot product calculation\n",
       "[source,python]\n",
       "----\n",
       ">>> v1 = pd.np.array([1, 2, 3])\n",
       ">>> v2 = pd.np.array([2, 3, 4])\n",
       ">>> v1.dot(v2)\n",
       "20\n",
       ">>> (v1 * v2).sum()  # <1>\n",
       "20\n",
       ">>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # <2>\n",
       "20\n",
       "----\n",
       "<1> Multiplication of NumPy arrays is a \"vectorized\" operation that is very efficient.\n",
       "<2> You should not iterate through vectors this way unless you want to slow down your pipeline.\n",
       "\n",
       "[TIP]\n",
       "================\n",
       "The dot product is equivalent to the _matrix product_, which can be accomplished in NumPy with the `np.matmul()` function or the `@` operator. Since all vectors can be turned into Nx1 or 1xN matrices, you can use this shorthand operator on two column vectors (Nx1) by transposing the first one so their inner dimensions line up, like this: `v1.reshape((-1, 1)).T @ v2.reshape((-1, 1))`, which outputs your scalar product within a 1x1 matrix: `array([[20]])`\n",
       "================\n",
       "\n",
       "// IDEA: Consider talking about BOW overlap to explain cosine similarity\n",
       "\n",
       "This is your first vector space model of natural language documents (sentences).\n",
       "Not only are dot products possible, but other vector operations are defined for these bag-of-word vectors: addition, subtraction, OR, AND, and so on.\n",
       "You can even compute things such as Euclidean distance or the angle between these vectors. This representation of a document as a binary vector has a lot of power.\n",
       "It was a mainstay for document retrieval and search for many years.\n",
       "All modern CPUs have hardwired memory addressing instructions that can efficiently hash, index, and search a large set of binary vectors like this.\n",
       "Though these instructions were built for another purpose (indexing memory locations to retrieve data from RAM), they are equally efficient at binary vector operations for search and retrieval of text.\n",
       "\n",
       "\n",
       "\n",
       "NLTK and Stanford CoreNLP have been around the longest and are the most widely used for comparison of NLP algorithms in academic papers.\n",
       "Even though the Stanford CoreNLP has a Python API, it relies on the Java 8 CoreNLP backend, which must be installed and configured separately.\n",
       "So if you want to publish the results of your work in an academic paper and compare it to what other researchers are doing, you may need to use NLTK.\n",
       "The most common tokenizer used in academia is the PennTreebank tokenizer:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.tokenize import TreebankWordTokenizer\n",
       ">>> texts.append(\n",
       "...   \"If conscience and empathy were impediments to the advancement of \"\n",
       "...   \"self-interest, then we would have evolved to be amoral sociopaths.\"\n",
       "...   )  # <1>\n",
       ">>> tokenizer = TreebankWordTokenizer()\n",
       ">>> tokens = tokenizer.tokenize(texts[-1])[:6]\n",
       ">>> tokens[:8]\n",
       "['If', 'conscience', 'and', 'empathy', 'were', 'impediments', 'to', 'the']\n",
       ">>> tokens[8:16]\n",
       "['advancement', 'of', 'self-interest', ',', 'then', 'we', 'would', 'have']\n",
       ">>> tokens[16:]\n",
       "['evolved', 'to', 'be', 'amoral', 'sociopaths', '.']\n",
       "----\n",
       "<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield in _SuperCooperators_: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\n",
       "\n",
       "// IDEA: Diagram of Nowak quote with vertical bars breaking up sent into words\n",
       "\n",
       "The spaCy Python library contains a natural language processing pipeline that includes a tokenizer.\n",
       "In fact, the name of the package comes from the words \"space\" and \"Cython\".\n",
       "SpaCy was built using the Cython package to speed the tokenization of text, often using the *space* character (\" \") as the delimmiter.\n",
       "SpaCy has become the *multitool* of NLP, because of its versatility and the elegance of its API.\n",
       "To use spaCy, you can start by creating an callable parser object, typically named `nlp`.\n",
       "You can customize your NLP pipeline by modifying the Pipeline elements within that parser object.\n",
       "\n",
       "And spaCy has \"batteries included.\"\n",
       "So even with the default smallest spaCy language model loaded, you can do tokenization and sentence segementation, plus *part-of-speech* and *abstract-syntax-tree* tagging -- all with a single function call.\n",
       "When you call `nlp()` on a string, spaCy tokenizes the text and returns a `Doc` (document) object.\n",
       "A `Doc` object is a container for the sequence of sentences and tokens that it found in the text.\n",
       "\n",
       "\n",
       "// IDEA: example spacy code for tokenization\n",
       "\n",
       "The spaCy package tags each token with their linguistic function to provide you with information about the text's grammatical structure.\n",
       "Each token object within a `Doc` object has attributes that provide these tags.\n",
       "\n",
       "For example:\n",
       "* `token.text` the original text of the word\n",
       "* `token.pos_` grammatical part of speech tag as a human-readable string\n",
       "* `token.pos`  integer for the grammar part of speech tag\n",
       "* `token.dep_` indicates the tokens role in the syntactic dependency tree\n",
       "* `token.dep`  integer corresponding to the syntactic dependency tree location\n",
       "\n",
       "The `.text` attribute provides the original text for the token.\n",
       "This is what is provided when you request the __str__ representation of a token.\n",
       "A spaCy `Doc` object is allowing you to detokenize a document object to recreate the entire input text. i.e., the relation between tokens\n",
       "You can use these functions to examine the text in more depth.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
       ">>> text = \"Nice guys finish first.\"  # <1>\n",
       ">>> doc = nlp(text)\n",
       ">>> for token in doc:\n",
       ">>>     print(f\"{token.text:<11}{token.pos_:<10}{token.dep:<10}\")\n",
       "Nice            ADJ       amod\n",
       "guys            NOUN      nsubj\n",
       "finish          VERB      ROOT\n",
       "first           ADV       advmod\n",
       ".               PUNCT     punct\n",
       "----\n",
       "<1> Martin A. Nowak & Roger Highfield in _SuperCooperators_.footnote:[excerpt from Martin A. Nowak and Roger Highfield SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed. New York: Free Press, 2011.]\n",
       "\n",
       "== Challenging tokens\n",
       "\n",
       "Chinese, Japanese, and other pictograph languages aren't limited to a small small number letters in alphabets used to compose tokens or words.\n",
       "Characters in these traditional languages look more like drawings and are called \"pictographs.\"\n",
       "There are many thousands of unique characters in the Chinese and Japanese languages.\n",
       "And these characters are used much like we use words in alphabet-based languages such as English.\n",
       "But each Chinese character is usually not a complete word on its own.\n",
       "A character's meaning depends on the characters to either side.\n",
       "And words are not delimited with spaces.\n",
       "This makes it challenging to tokenize Chinese text into words or other packets of thought and meaning. \n",
       "\n",
       "The `jieba` package is a Python package you can use to segment traditional Chinese text into words.\n",
       "It supports three segmentation modes: 1) \"full mode\" for retrieving all possible words from a sentence, 2) \"accurate mode\" for cutting the sentence into the most accurate segments, 3) \"search engine mode\" for splitting long words into shorter ones, sort-of like splitting compound words or finding the roots of words in English.\n",
       "In the example below, the Chinese sentence \"西安是一座举世闻名的文化古城\" translates into \"Xi'an is a city famous world-wide for it's ancient culture.\"\n",
       "Or, a more compact and literal translation might be \"Xi'an is a world-famous city for her ancient culture.\"\n",
       "\n",
       "From a grammatical perspective, you can split the sentence into: 西安 (Xi'an), 是 (is), 一座 (a), 举世闻名 (world-famous), 的 (adjective suffix), 文化 (culture), 古城 (ancient city).\n",
       "The character \"座\" is the quantifier meaning \"ancient\" that is normally used to modify the word \"city.\"\n",
       "The `accurate mode` in `jieba` causes it to segment the sentence this way so that you can correctly extract a precise interpretation of the text.\n",
       "\n",
       ".Jieba in accurate mode\n",
       "[source,python]\n",
       "----\n",
       ">>> seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\") # <1>\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> the default mode for jieba is accurate or precise mode\n",
       "\n",
       "Jieba's accurate mode minimizes the total number of tokens or words.\n",
       "This gave you 7 tokens for this short\n",
       "Jieba attempts to keep as many possible characters together.\n",
       "This will reduce the false positive rate or type 1 errors for detecting boundaries between words.\n",
       "\n",
       "In full mode, jieba will attempt to split the text into smaller words, and more of them.\n",
       "\n",
       ".Jieba in full mode\n",
       "[source,python]\n",
       "----\n",
       ">>> import jieba\n",
       "... seg_list = jieba.cut(\"西安是一座举世闻名的文化古城\", cut_all=True)  # <1>\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世', '举世闻名', '闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> `cut_all==True` means \"full mode\"\n",
       "\n",
       "Now you can try search engine mode to see if it's possible to break up these tokens even further:\n",
       "\n",
       ".Jieba in search engine mode\n",
       "[source,python]\n",
       "----\n",
       ">>> seg_list = jieba.cut_for_search(\"西安是一座举世闻名的文化古城\")\n",
       ">>> list(seg_list)\n",
       "['西安', '是', '一座', '举世', '闻名', '举世闻名', '的', '文化', '古城']\n",
       "----\n",
       "<1> Accurate mode is the default mode.\n",
       "\n",
       "Unfortunately later versions of Python (3.5+) aren't supported by Jieba's part-of-speech tagging model.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import jieba\n",
       ">>> from jieba import posseg\n",
       ">>> words = posseg.cut(\"西安是一座举世闻名的文化古城\")\n",
       ">>> jieba.enable_paddle()  # <1>\n",
       ">>> words = posseg.cut(\"西安是一座举世闻名的文化古城\", use_paddle=True)\n",
       ">>> list(words)\n",
       "[pair('西安', 'ns'),\n",
       " pair('是', 'v'),\n",
       " pair('一座', 'm'),\n",
       " pair('举世闻名', 'i'),\n",
       " pair('的', 'uj'),\n",
       " pair('文化', 'n'),\n",
       " pair('古城', 'ns')]\n",
       "----\n",
       "<1> Activate paddle mode\n",
       "\n",
       "You can find more information about jieba at (https://github.com/fxsjy/jieba).\n",
       "SpaCy also contains Chinese language models that do a decent job of segmenting and tagging Chinese text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> spacy.cli.download(\"zh_core_web_sm\")  # <1>\n",
       ">>> nlpzh = spacy.load(\"zh_core_web_sm\")\n",
       ">>> doc = nlpzh(\"西安是一座举世闻名的文化古城\")\n",
       ">>> [(tok.text, tok.pos_) for tok in doc]\n",
       "[('西安', 'PROPN'),\n",
       " ('是', 'VERB'),\n",
       " ('一', 'NUM'),\n",
       " ('座', 'NUM'),\n",
       " ('举世闻名', 'VERB'),\n",
       " ('的', 'PART'),\n",
       " ('文化', 'NOUN'),\n",
       " ('古城', 'NOUN')]\n",
       "----\n",
       "<1> Only need download the Chinese (zh) language model if this is your first time processing Chinese text \n",
       "\n",
       "As you may notice, spaCy provides slightly different tokenization and tagging, which is more attached to the original meaning of each word rather than the context of this sentence.\n",
       "\n",
       "=== A complicated picture\n",
       "\n",
       "Unlike English, there is no concept of stemming or lemmatization in pictographic languages such as Chinese and Japanese (Kanji).\n",
       "However, there’s a related concept.\n",
       "The most essential building blocks of Chinese characters are called _radicals_.\n",
       "To better understand _radicals_, you must first see how Chinese characters are constructed.\n",
       "There are six types of Chinese characters: 1) pictographs, 2) pictophonetic characters, 3) associative compounds, 4) self-explanatory characters, 5) phonetic loan characters, and 6) mutually explanatory characters.\n",
       "The top four categories are the most important and encompass most Chinese characters.\n",
       "\n",
       "1. Pictographs (象形字)\n",
       "2. Pictophonetic characters (形声字)\n",
       "3. Associative compounds (会意字)\n",
       "\n",
       "==== 1. Pictographs (象形字)\n",
       "\n",
       "_Pictographs_ were created from images of real objects, such as the characters for 口 (mouth) and 门 (door).\n",
       "\n",
       "\n",
       "==== 2. Pictophonetic characters (形声字)\n",
       "\n",
       "_Pictophonetic characters_ were created from a radical and a single Chinese character.\n",
       "One part represents its meaning and the other indicates its pronunciation.\n",
       "For example, 妈 (mā, mother) = 女 (female) + 马 (mǎ, horse).\n",
       "Squeezing 女 into 马 gives 妈.\n",
       "The character 女 is the semantic radical that indicates the meaning of the character (female).\n",
       "马 is a single character that has a similar pronunciation (mǎ).\n",
       "You can see that the character for mother (妈) is a combination of the characters for female an\n",
       "This is comparable to the English concept of homophones -- words that sound alike but mean completely different things.\n",
       "But in Chinese use additional characters to disambiguate homophones.\n",
       "The character for female\n",
       "\n",
       "==== 3. Associative compounds (会意字)\n",
       "\n",
       "Associative compounds can be divided into two parts: one symbolizes the image, the other indicates the meaning.\n",
       "\n",
       "For example, 旦 (dawn), the upper part (日) is the sun and the lower part (一) is like the horizon line.\n",
       "\n",
       "\n",
       "==== Self-explanatory characters (指事字)\n",
       "\n",
       "\n",
       "Self-explanatory characters cannot be easily represented by an image, so they are shown by a single abstract symbol.\n",
       "For example, 上 (up), 下 (down).\n",
       "\n",
       "As you can see, procedures like stemming and lemmatization are harder or impossible for many Chinese characters.\n",
       "Separating the parts of a character may radically ;) change its meaning.\n",
       "And there's not prescribed order or rule for combining radicals to create Chinese characters.\n",
       "\n",
       "Nonetheless, some kinds of stemming are harder in English than they are in Chinese\n",
       "For example, automatically removing the pluralization from words like \"we\", \"us\", \"they\" and \"them\" is hard in English but straightforward in Chinese.\n",
       "Chinese uses inflection to construct the plural form of characters, similar to adding s to the end of English words.\n",
       "In Chinese the pluralization suffix character is 们.\n",
       "The character 朋友 (friend) becomes 朋友们 (friends).\n",
       "\n",
       "Even the characters for \"we/us\", \"they/them\", and \"y'all\" use the same pluralization suffix: 我们 (we/us), 他们 (they/them), 你们 (you).\n",
       "But in in English, you can remove the 'ing' or 'ed' from many verbs to get the root word.\n",
       "However, in Chinese, verb conjugation uses an additional character in the front or the end to indicate tense.\n",
       "There's no prescribed rule for verb conjugation.\n",
       "For example, examine the character 学 (learn), 在学 (learning), and 学过 (learned).\n",
       "In Chinese, you can also use a suffix 学 to denote an academic discipline, such as 心理学 (psychology) or 社会学 (sociology).\n",
       "In most cases, you want to keep the integrated Chinese character together rather than reducing it to its components.\n",
       "\n",
       "It turns out this is a good rule of thumb for all languages.\n",
       "Let the data do the talking.\n",
       "Do not stem or lemmatize unless the statistics indicate that it will help your NLP pipeline perform better.\n",
       "Is there not a small amount of meaning that is lost when \"smarter\" and \"smartest\" reduce to \"smart\"?\n",
       "Make sure stemming does not leave your NLP pipeline dumb.\n",
       "\n",
       "Let the statistics of how of how characters and words are used together help you decide how, or if, to decompose any particular word or n-gram.\n",
       "In the next chapter we'll show you some tools like Scikit-Learn's `TfidfVectorizer` that handle all the tedious account required to get this right.\n",
       "\n",
       "\n",
       "==== Contractions\n",
       "\n",
       "// TODO: clean this up\n",
       "You might be wondering why you would want to split the contraction `wasn't` into `was` and `n't`.\n",
       "For some applications, like grammar-based NLP models that use syntax trees, it is important to separate the words `was` and `not` to allow the syntax tree parser to have a consistent, predictable set of tokens with known grammar rules as its input.\n",
       "There are a variety of standard and nonstandard ways to contract words, by reducing contractions to their constituent words, a dependency tree parser or syntax parser only need to be programmed to anticipate the various spellings of individual words rather than all possible contractions.\n",
       "\n",
       "\n",
       "[TIP]\n",
       ".Tokenize informal text from social networks such as Twitter and Facebook\n",
       "====\n",
       "The NLTK library includes a rule-based tokenizer to deal with short, informal, emoji-laced texts from social networks: `casual_tokenize`\n",
       "\n",
       "It handles emojis, emoticons, and usernames.\n",
       "The `reduce_len` option deletes less meaningful character repetitions.\n",
       "The `reduce_len` algorithm retains three repetitions, to approximate the intent and sentiment of the original text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.tokenize.casual import casual_tokenize\n",
       ">>> texts.append(\"@rickrau mind BLOOOOOOOOWWWWWN by latest lex :*) !!!!!!!!\")\n",
       ">>> casual_tokenize(texts[-1], reduce_len=True)\n",
       "['@rickrau', 'mind', 'BLOOOWWWN', 'by', 'latest', 'lex', ':*)', '!', '!', '!']\n",
       "----\n",
       "\n",
       "====\n",
       "\n",
       "=== Extending your vocabulary with _n_-grams\n",
       "\n",
       "Let's revisit that \"ice cream\" problem from the beginning of the chapter.\n",
       "Remember we talked about trying to keep \"ice\" and \"cream\" together.\n",
       "\n",
       "____\n",
       "I scream, you scream, we all scream for ice cream.\n",
       "____\n",
       "\n",
       "But I do not know many people that scream for \"cream\".\n",
       "And nobody screams for \"ice\", unless they're about to slip and fall on it.\n",
       "So you need a way for your word-vectors to keep \"ice\" and \"cream\" together.\n",
       "\n",
       "==== We all gram for _n_-grams\n",
       "\n",
       "An _n_-gram is a sequence containing up to _n_ elements that have been extracted from a sequence of those elements, usually a string.\n",
       "In general the \"elements\" of an _n_-gram can be characters, syllables, words, or even symbols like \"A\", \"D\", and \"G\" used to represent the chemical amino acid markers in a DNA or RNA sequence.footnote:[Linguistic and NLP techniques are often used to glean information from DNA and RNA, this site provides a list of amino acid symbols that can help you translate amino acid language into a human-readable language: \"Amino Acid - Wikipedia\" (https://en.wikipedia.org/wiki/Amino_acid#Table_of_standard_amino_acid_abbreviations_and_properties).]\n",
       "\n",
       "In this book, we're only interested in _n_-grams of words, not characters.footnote:[You may have learned about trigram indexes in your database class or the documentation for PostgreSQL (`postgres`). But these are triplets of characters. They help you quickly retrieve fuzzy matches for strings in a massive database of strings using the `%` and `~*` SQL full text search queries.]\n",
       "So in this book, when we say 2-gram, we mean a pair of words, like \"ice cream\".\n",
       "When we say 3-gram, we mean a triplet of words like \"beyond the pale\" or \"Johann Sebastian Bach\" or \"riddle me this\".\n",
       "_n_-grams do not have to mean something special together, like compound words.\n",
       "They have to be frequent enough together to catch the attention of your token counters.\n",
       "\n",
       "Why bother with _n_-grams?\n",
       "As you saw earlier, when a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of those words.\n",
       "By extending your concept of a token to include multiword tokens, _n_-grams, your NLP pipeline can retain much of the meaning inherent in the order of words in your statements.\n",
       "For example, the meaning-inverting word \"not\" will remain attached to its neighboring words, where it belongs.\n",
       "Without _n_-gram tokenization, it would be free floating.\n",
       "Its meaning would be associated with the entire sentence or document rather than its neighboring words.\n",
       "The 2-gram \"was not\" retains much more of the meaning of the individual words \"not\" and \"was\" than those 1-grams alone in a bag-of-words vector.\n",
       "A bit of the context of a word is retained when you tie it to its neighbor(s) in your pipeline.\n",
       "\n",
       "In the next chapter, we show you how to recognize which of these _n_-grams contain the most information relative to the others, which you can use to reduce the number of tokens (_n_-grams) your NLP pipeline has to keep track of.\n",
       "Otherwise it would have to store and maintain a list of every single word sequence it came across.\n",
       "This prioritization of _n_-grams will help it recognize \"Three Body Problem\" and \"ice cream\", without paying particular attention to \"three bodies\" or \"ice shattered\".\n",
       "In chapter 4, we associate word pairs, and even longer sequences, with their actual meaning, independent of the meaning of their individual words.\n",
       "But for now, you need your tokenizer to generate these sequences, these _n_-grams.\n",
       "\n",
       "==== Stop words\n",
       "\n",
       "Stop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase.\n",
       "Examples of some common stop words include footnote:[A more comprehensive list of stop words for various languages can be found in NLTK's corpora (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip).]\n",
       "\n",
       "* a, an\n",
       "* the, this\n",
       "* and, or\n",
       "* of, on\n",
       "\n",
       "Historically stop words have been excluded from NLP pipelines in order to reduce the computational effort to extract information from a text.\n",
       "Even though the words themselves carry little information, the stop words can provide important relational information as part of an _n_-gram.\n",
       "Consider these two examples:\n",
       "\n",
       "* `Mark reported to the CEO`\n",
       "* `Suzanne reported as the CEO to the board`\n",
       "\n",
       "In your NLP pipeline, you might create 4-grams such as `reported to the CEO` and `reported as the CEO`.\n",
       "If you remove the stop words from the 4-grams, both examples would be reduced to `reported CEO`, and you would lack the information about the professional hierarchy.\n",
       "In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board.\n",
       "Unfortunately, retaining the stop words within your pipeline creates another problem: It increases the length of the _n_-grams required to make use of these connections formed by the otherwise meaningless stop words.\n",
       "This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\n",
       "\n",
       "Designing a filter for stop words depends on your particular application.\n",
       "Vocabulary size will drive the computational complexity and memory requirements of all subsequent steps in the NLP pipeline.\n",
       "But stop words are only a small portion of your total vocabulary size.\n",
       "A typical stop word list has only 100 or so frequent and unimportant words listed in it.\n",
       "But a vocabulary size of 20,000 words would be required to keep track of 95% of the words seen in a large corpus of tweets, blog posts, and news articles.footnote:[See the web page titled \"Analysis of text data and Natural Language Processing\" (http://rstudio-pubs-static.s3.amazonaws.com/41251_4c55dff8747c4850a7fb26fb9a969c8f.html).]\n",
       "And that is just for 1-grams or single-word tokens.\n",
       "A 2-gram vocabulary designed to catch 95% of the 2-grams in a large English corpus will generally have more than 1 million unique 2-gram tokens in it.\n",
       "\n",
       "You may be worried that vocabulary size drives the required size of any training set you must acquire to avoid overfitting to any particular word or combination of words.\n",
       "And you know that the size of your training set drives the amount of processing required to process it all.\n",
       "However, getting rid of 100 stop words out of 20,000 is not going to significantly speed up your work.\n",
       "And for a 2-gram vocabulary, the savings you would achieve by removing stop words is minuscule.\n",
       "In addition, for 2-grams you lose a lot more information when you get rid of stop words arbitrarily, without checking for the frequency of the 2-grams that use those stop words in your text.\n",
       "For example, you might miss mentions of \"The Shining\" as a unique title and instead treat texts about that violent, disturbing movie the same as you treat documents that mention \"Shining Light\" or \"shoe shining\".\n",
       "\n",
       "So if you have sufficient memory and processing bandwidth to run all the NLP steps in your pipeline on the larger vocabulary, you probably do not want to worry about ignoring a few unimportant words here and there.\n",
       "And if you are worried about overfitting a small training set with a large vocabulary, there are better ways to select your vocabulary or reduce your dimensionality than ignoring stop words.\n",
       "Including stop words in your vocabulary allows the document frequency filters (discussed in chapter 3) to more accurately identify and ignore the words and _n_-grams with the least information content within your particular domain.\n",
       "\n",
       "The SpaCy and NLTK packages include a variety of predefined sets of stop words for various use cases. footnote:[The spaCy package contains a list of stopwords that you can modify using this Stack Overflow answer (https://stackoverflow.com/a/51627002/623735)]\n",
       "You probably won't need a broad list of stopwords like the one we created for listing <<listing-broad-stop-words>>, but if you do you'll want to check out the SpaCy and NLTK stopwords lists.\n",
       "And if you need an even broader set of stopwords you can `SearX`\n",
       "footnote:[If you want to help others find SearX you can get in the habbit of saying \"SearX\" (pronounced \"see Ricks\") when talking or writing about doing a web search. You can shift the meaning of words in your world to make it a better place!]\n",
       "footnote:[The NLTK package (https://pypi.org/project/nltk) contains the list of stopwords you'll see in most tutorials]  for SEO companies that maintain lists of stopwords in many languages.\n",
       "\n",
       "If your NLP pipeline relies on a fine-tuned list of stop words to achieve high accuracy, it can be a significant maintenance headache.\n",
       "Humans and machines (search engines) are constantly changing which words they ignore.\n",
       " footnote:[Damien Doyle maintains a list of search engine stopwords ranked by popularity and categorized by language (https://www.ranks.nl/stopwords)]\n",
       "// HL: to Karen, Matko & Ivan: how can I use references (anchors) to refer to the correct listing number?\n",
       "Listing <<listing-broad-stop-words>> uses an exhaustive list from all these lists so you can get a feel for the amount of meaning that can be lost if your hand-crafted list of stop words isn't well crafted and kept up to date.\n",
       "\n",
       "////\n",
       "\n",
       "HL to Ivan and Matko:\n",
       "\n",
       "These are the things I tried based on the examples in Manning's ADOC documentation:\n",
       "\n",
       "[[listing-broad-stop-words]] -> \"Listing 2.6\" (capital L)\n",
       "\n",
       "[#listing-broad-stop-words, reftext={chapter}.{counter:listing}] -> \"2.\" (without the counter:listing integer in my Browser plugin)\n",
       "\n",
       "////\n",
       "[#listing-broad-stop-words, reftext={chapter}.{counter:listing}]\n",
       ".Broad list of stop words\n",
       "[source,python]\n",
       "----\n",
       ">>> import requests\n",
       ">>> url = (\"https://gitlab.com/tangibleai/nlpia/-/raw/master/\"\n",
       "...        \"src/nlpia/data/stopword_lists.json\")\n",
       ">>> response = requests.get(url)\n",
       ">>> stop_words = response.json()['exhaustive']  # <1>\n",
       ">>> tokens = 'the words were just as I remembered them'.split()  # <2>\n",
       ">>> tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
       ">>> print(tokens_without_stopwords)\n",
       "['I', 'remembered']\n",
       "----\n",
       "<1> This exhaustive list of stop words was compiled from various public search engine optimization lists as well as NLP toolkits like spaCy and NLTK.\n",
       "\n",
       "You can see that some words carry more meaning than others.\n",
       "This is a sentence from a short story by Ted Chiang about machines helping us remember our statements so we don't have to rely on flawed memories.footnote:[from Ted Chiang, _Exhalation_, \"Truth of Fact, Truth of Fiction\"]\n",
       "In this phrase you lost two thirds of the words and still retained the bulk of the phrase's meaning.\n",
       "However you can see that an import token \"words\" was discarded by this particular stop words list.\n",
       "You can often get your point across without articles, prepositions, or even forms of the verb \"to be\".\n",
       "Imagine someone doing sign language or in a hurry to write a note to themselves.\n",
       "Which words would they choose to always skip? That is how stop words are chosen.\n",
       "\n",
       "Here's another common stop words list that isn't quite as exhaustive:\n",
       "\n",
       "[[nltk_stop_words_code]]\n",
       ".NLTK list of stop words\n",
       "[source,python]\n",
       "----\n",
       ">>> import nltk\n",
       ">>> nltk.download('stopwords')\n",
       ">>> stop_words = nltk.corpus.stopwords.words('english')\n",
       ">>> len(stop_words)\n",
       "179\n",
       ">>> stop_words[:7]\n",
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']\n",
       ">>> [sw for sw in stopwords if len(sw) == 1]\n",
       "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']\n",
       "----\n",
       "\n",
       "A document that dwells on the first person is pretty boring, and more importantly for you, has low information content.\n",
       "The NLTK package includes pronouns (not just first person ones) in its list of stop words.\n",
       "And these one-letter stop words are even more curious, but they make sense if you have used the NLTK tokenizer and Porter stemmer a lot.\n",
       "These single-letter tokens pop up a lot when contractions are split and stemmed using NLTK tokenizers and stemmers.\n",
       "\n",
       "[WARNING]\n",
       "====\n",
       "The set of English stop words in `sklearn`, `spacy`, `nltk`, and SEO tools are very different, and they are constantly evolving.\n",
       "At the time of this writing, `sklearn` has 318 stop words, NLTK has 179 stop words, spaCy has 326, and our 'exhaustive' SEO list includes 667 stop words.\n",
       "\n",
       "This is a good reason to consider *not* filtering stop words.\n",
       "If you do, others may not be able to reproduce your results.\n",
       "====\n",
       "\n",
       "Depending on how much natural language information you want to discard ;), you can take the union or the intersection of multiple stop word lists for your pipeline.\n",
       "Here are some stop_words lists we found, though we rarely use any of them in production:\n",
       "\n",
       "[[collection_of_stop_words_lists_code]]\n",
       ".Collection of stop words lists\n",
       "[source,python]\n",
       "----\n",
       ">>> resp = requests.get(url)\n",
       ">>> len(resp.json()['exhaustive'])\n",
       "667\n",
       ">>> len(resp.json()['sklearn'])\n",
       "318\n",
       ">>> len(resp.json()['spacy'])\n",
       "326\n",
       ">>> len(resp.json()['nltk'])\n",
       "179\n",
       ">>> len(resp.json()['reuters'])\n",
       "28\n",
       "----\n",
       "\n",
       "=== Normalizing your vocabulary\n",
       "\n",
       "So you have seen how important vocabulary size is to the performance of an NLP pipeline. Another vocabulary reduction technique is to normalize your vocabulary so that tokens that mean similar things are combined into a single, normalized form. Doing so reduces the number of tokens you need to retain in your vocabulary and also improves the association of meaning across those different \"spellings\" of a token or _n_-gram in your corpus. And as we mentioned before, reducing your vocabulary can reduce the likelihood of overfitting.\n",
       "\n",
       "==== Case folding\n",
       "\n",
       "Case folding is when you consolidate multiple \"spellings\" of a word that differ only in their capitalization.\n",
       "So why would we use case folding at all?\n",
       "Words can become case \"denormalized\" when they are capitalized because of their presence at the beginning of a sentence, or when they're written in `ALL CAPS` for emphasis.\n",
       "// TODO: Discuss _ normalization, Unicode normalization, and asciification, diacritics, accented e in resume', etc\n",
       "Undoing this denormalization is called _case normalization_, or more commonly, _case folding_.\n",
       "Normalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline.\n",
       "It helps you consolidate words that are intended to mean (and be spelled) the same thing under a single token.\n",
       "\n",
       "However, some information is often communicated by capitalization of a word -- for example,  'doctor' and 'Doctor' often have different meanings.\n",
       "Often capitalization is used to indicate that a word is a proper noun, the name of a person, place, or thing.\n",
       "You will want to be able to recognize proper nouns as distinct from other words, if named entity recognition is important to your pipeline.\n",
       "However, if tokens are not case normalized, your vocabulary will be approximately twice as large, consume twice as much memory and processing time, and might increase the amount of training data you need to have labeled for your machine learning pipeline to converge to an accurate, general solution.\n",
       "Just as in any other machine learning pipeline, your labeled dataset used for training must be \"representative\" of the space of all possible feature vectors your model must deal with, including variations in capitalization.\n",
       "For 100000-D bag-of-words vectors, you usually must have 100000 labeled examples, and sometimes even more than that, to train a supervised machine learning pipeline without overfitting.\n",
       "In some situations, cutting your vocabulary size by half can sometimes be worth the loss of information content.\n",
       "\n",
       "In Python, you can easily normalize the capitalization of your tokens with a list comprehension.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> tokens = ['House', 'Visitor', 'Center']\n",
       ">>> normalized_tokens = [x.lower() for x in tokens]\n",
       ">>> print(normalized_tokens)\n",
       "['house', 'visitor', 'center']\n",
       "----\n",
       "\n",
       "And if you are certain that you want to normalize the case for an entire document, you can `lower()` the text string in one operation, before tokenization.\n",
       "But this will prevent advanced tokenizers that can split _camel case_ words like \"WordPerfect\", \"FedEx\", or \"stringVariableName.\"footnote:[See the web page titled \"Camel case case - Wikipedia\" (https://en.wikipedia.org/wiki/Camel_case_case).]]\n",
       "Maybe you want WordPerfect to be its own unique thing (token), or maybe you want to reminisce about a more perfect word processing era.\n",
       "It is up to you to decide when and how to apply case folding.\n",
       "\n",
       "With case normalization, you are attempting to return these tokens to their \"normal\" state before grammar rules and their position in a sentence affected their capitalization.\n",
       "The simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Python's built-in `str.lower()`.footnote:[We're assuming the behavior of str.lower() in Python 3. In Python 2, bytes (strings) could be lowercased by just shifting all alpha characters in the ASCII number (`ord`) space, but in Python 3 `str.lower` properly translates characters so it can handle embellished English characters (like the \"acute accent\" diactric mark over the e in resumé) as well as the particulars of capitalization in non-English languages.]\n",
       "Unfortunately this approach will also \"normalize\" away a lot of meaningful capitalization in addition to the less meaningful first-word-in-sentence capitalization you intended to normalize away.\n",
       "A better approach for case normalization is to lowercase only the first word of a sentence and allow all other words to retain their capitalization.\n",
       "\n",
       "Lowercasing on the first word in a sentence preserves the meaning of a proper nouns in the middle of a sentence, like \"Joe\" and \"Smith\" in \"Joe Smith\".\n",
       "And it properly groups words together that belong together, because they are only capitalized when they are at the beginning of a sentence, since they are not proper nouns.\n",
       "This prevents \"Joe\" from being confused with \"coffee\" (\"joe\")footnote:[The trigram \"cup of joe\" (https://en.wiktionary.org/wiki/cup_of_joe) is slang for \"cup of coffee.\"] during tokenization.\n",
       "And this approach prevents the blacksmith connotation of \"smith\" being confused the the proper name \"Smith\" in a sentence like \"A word smith had a cup of joe.\"\n",
       "Even with this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still need to introduce capitalization errors for the rare proper nouns that start a sentence.\n",
       "\"Joe Smith, the word smith, with a cup of joe.\" will produce a different set of tokens than \"Smith the word with a cup of joe, Joe Smith.\"\n",
       "And you may not  want that.\n",
       "In addition, case normalization is useless for languages that do not have a concept of capitalization, like Arabic or Hindi.\n",
       "\n",
       "To avoid this potential loss of information, many NLP pipelines do not normalize for case at all.\n",
       "For many applications, the efficiency gain (in storage and processing) for reducing one's vocabulary size by about half is outweighed by the loss of information for proper nouns.\n",
       "But some information may be \"lost\" even without case normalization.\n",
       "If you do not identify the word \"The\" at the start of a sentence as a stop word, that can be a problem for some applications.\n",
       "Really sophisticated pipelines will detect proper nouns before selectively normalizing the case for words at the beginning of sentences that are clearly not proper nouns.\n",
       "You should implement whatever case normalization approach makes sense for your application.\n",
       "If you do not have a lot of \"Smith\"s and \"word smiths\" in your corpus, and you do not care if they get assigned to the the same tokens, you can just lowercase everything.\n",
       "The best way to find out what works is to try several different approaches, and see which approach gives you the best performance for the objectives of your NLP project.\n",
       "\n",
       "By generalizing your model to work with text that has odd capitalization, case normalization can reduce overfitting for your machine learning pipeline.\n",
       "Case normalization is particularly useful for a search engine.\n",
       "For search, normalization increases the number of matches found for a particular query.\n",
       "This is often called the \"recall\" performance metric for a search engine (or any other classification model).footnote:[Check our Appendix D to learn more about _precision_ and _recall_. Here's a comparison of the recall of various search engines on the Webology site (http://www.webology.org/2005/v2n2/a12.html).]\n",
       "\n",
       "For a search engine without normalization if you searched for \"Age\" you will get a different set of documents than if you searched for \"age.\"\n",
       "\"Age\" would likely occur in phrases like \"New Age\" or \"Age of Reason\".\n",
       "In contrast, \"age\" would be more likely to occur in phrases like \"at the age of\" in your sentence about Thomas Jefferson.\n",
       "By normalizing the vocabulary in your search index (as well as the query), you can ensure that both kinds of documents about \"age\" are returned regardless of the capitalization in the query from the user.\n",
       "\n",
       "However, this additional recall accuracy comes at the cost of precision, returning many documents that the user may not be interested in. Because of this issue, modern search engines allow users to turn off normalization with each query, typically by quoting those words for which they want only exact matches returned. If you are building such a search engine pipeline, in order to accommodate both types of queries you will have to build two indexes for your documents: one with case-normalized _n_-grams, and another with the original capitalization.\n",
       "\n",
       "==== Stemming\n",
       "\n",
       "Another common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms.\n",
       "This normalization, identifying a common stem among various forms of a word, is called stemming.\n",
       "For example, the words `housing` and `houses` share the same stem, `house`.\n",
       "Stemming removes suffixes from words in an attempt to combine words with similar meanings together under their common stem.\n",
       "A stem is not required to be a properly spelled word, but merely a token, or label, representing several possible spellings of a word.\n",
       "\n",
       "A human can easily see that \"house\" and \"houses\" are the singular and plural forms of the same noun.\n",
       "However, you need some way to provide this information to the machine. One of its main benefits is in the compression of the number of words whose meaning your software or language model needs to keep track of.\n",
       "It reduces the size of your vocabulary while limiting the loss of information and meaning, as much as possible.\n",
       "In machine learning this is referred to as dimension reduction.\n",
       "It helps generalize your language model, enabling the model to behave identically for all the words included in a stem.\n",
       "So, as long as your application does not require your machine to distinguish between \"house\" and \"houses\", this stem will reduce your programming or dataset size by half or even more, depending on the aggressiveness of the stemmer you chose.\n",
       "\n",
       "Stemming is important for keyword search or information retrieval.\n",
       "It allows you to search for \"developing houses in Portland\" and get web pages or documents that use both the word \"house\" and \"houses\" and even the word \"housing\" because these words are all stemmed to the \"hous\" token.\n",
       "Likewise you might receive pages with the words \"developer\" and \"development\" rather than \"developing\" because all these words typically reduce to the stem \"develop\".\n",
       "As you can see, this is a \"broadening\" of your search, ensuring that you are less likely to miss a relevant document or web page.\n",
       "This broadening of your search results would be a big improvement in the \"recall\" score for how well your search engine is doing its job at returning all the relevant documents.footnote:[Review Appendix D if you have forgotten how to measure recall or visit the wikipedia page to learn more (https://en.wikipedia.org/wiki/Precision_and_recall).]\n",
       "\n",
       "But stemming could greatly reduce the \"precision\" score for your search engine because it might return many more irrelevant documents along with the relevant ones.\n",
       "In some applications this \"false-positive rate\" (proportion of the pages returned that you do not find useful) can be a problem.\n",
       "So most search engines allow you to turn off stemming and even case normalization by putting quotes around a word or phrase.\n",
       "Quoting indicates that you only want pages containing the exact spelling of a phrase such as \"'Portland Housing Development software'.\"\n",
       "That would return a different sort of document than one that talks about a \"'a Portland software developer's house'.\"\n",
       "And there are times when you want to search for \"Dr. House's calls\" and not \"dr house call\", which might be the effective query if you used a stemmer on that query.\n",
       "\n",
       "Here's a simple stemmer implementation in pure Python that can handle trailing S's.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> def stem(phrase):\n",
       "...     return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
       "...         word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
       ">>> stem('houses')\n",
       "'house'\n",
       ">>> stem(\"Doctor House's calls\")\n",
       "'doctor house call'\n",
       "----\n",
       "\n",
       "The preceding stemmer function follows a few simple rules within that one short regular expression:\n",
       "\n",
       "* If a word ends with more than one `s`, the stem is the word and the suffix is a blank string.\n",
       "* If a word ends with a single `s`, the stem is the word without the `s` and the suffix is the `s`.\n",
       "* If a word does not end on an `s`, the stem is the word and no suffix is returned.\n",
       "\n",
       "The strip method ensures that some possessive words can be stemmed along with plurals.\n",
       "\n",
       "This function works well for regular cases, but is unable to address more complex cases. For example, the rules would fail with words like `dishes` or `heroes`. For more complex cases like these, the NLTK package provides other stemmers.\n",
       "\n",
       "It also does not handle the \"housing\" example from your \"Portland Housing\" search.\n",
       "\n",
       "Two of the most popular stemming algorithms are the Porter and Snowball stemmers.\n",
       "The Porter stemmer is named for the computer scientist Martin Porter.footnote:[See \"An algorithm for suffix stripping\", 1993 (http://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf) by M.F. Porter.]\n",
       "Porter is also also responsible for enhancing the Porter stemmer to create the Snowball stemmer.footnote:[See the web page titled \"Snowball: A language for stemming algorithms\" (http://snowball.tartarus.org/texts/introduction.html).]\n",
       "Porter dedicated much of his lengthy career to documenting and improving stemmers, due to their value in information retrieval (keyword search).\n",
       "These stemmers implement more complex rules than our simple regular expression.\n",
       "This enables the stemmer to handle the complexities of English spelling and word ending rules.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.stem.porter import PorterStemmer\n",
       ">>> stemmer = PorterStemmer()\n",
       ">>> ' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
       "...   \"dish washer's fairly washed dishes\".split()])\n",
       "'dish washer fairli wash dish'\n",
       "----\n",
       "\n",
       "Notice that the Porter stemmer, like the regular expression stemmer, retains the trailing apostrophe (unless you explicitly strip it), which ensures that possessive words will be distinguishable from nonpossessive words.\n",
       "Possessive words are often proper nouns, so this feature can be important for applications where you want to treat names differently than other nouns.\n",
       "\n",
       ".More on the Porter stemmer\n",
       "****\n",
       "Julia Menchavez has graciously shared her translation of Porter's original stemmer algorithm into pure python (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py). If you are ever tempted to develop your own stemmer, consider these 300 lines of code and the lifetime of refinement that Porter put into them.\n",
       "\n",
       "There are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4, 5a, and 5b.\n",
       "Step 1a is a bit like your regular expression for dealing with trailing \"S\"es:footnote:[This is a trivially abbreviated version of Julia Menchavez's implementation `porter-stemmer` on GitHub (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py).]\n",
       "\n",
       "[source,python]\n",
       "----\n",
       "def step1a(self, word):\n",
       "    if word.endswith('sses'):\n",
       "        word = self.replace(word, 'sses', 'ss')  # <1>\n",
       "    elif word.endswith('ies'):\n",
       "        word = self.replace(word, 'ies', 'i')\n",
       "    elif word.endswith('ss'):\n",
       "        word = self.replace(word, 'ss', 'ss')\n",
       "    elif word.endswith('s'):\n",
       "        word = self.replace(word, 's', '')\n",
       "    return word\n",
       "----\n",
       "<1> This is not at all like `str.replace()`. Julia's `self.replace()` modifies only the ending of a word.\n",
       "\n",
       "The remainining seven steps are much more complicated because they have to deal with the complicated English spelling rules for the following:\n",
       "\n",
       "* *Step 1a*: \"s\" and \"es\" endings\n",
       "* *Step 1b*: \"ed\", \"ing\", and \"at\" endings\n",
       "* *Step 1c*: \"y\" endings\n",
       "* *Step 2*: \"nounifying\" endings such as \"ational\", \"tional\", \"ence\", and \"able\"\n",
       "* *Step 3*: adjective endings such as \"icate\",footnote:[Sorry Chick, Porter doesn't like your `obsfucate` username ;)], \"ful\", and \"alize\"\n",
       "* *Step 4*: adjective and noun endings such as \"ive\", \"ible\", \"ent\", and \"ism\"\n",
       "* *Step 5a*: stubborn \"e\" endings, still hanging around\n",
       "* *Step 5b*: trailing double-consonants for which the stem will end in a single \"l\"\n",
       "****\n",
       "\n",
       "Snowball stemmer is more aggressive than the Porter stemmer.\n",
       "Notice that it stems 'fairly' to 'fair', which is more accurate than the Porter stemmer.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from nltk.stem.snowball import SnowballStemmer\n",
       ">>> stemmer = SnowballStemmer(language='english')\n",
       ">>> ' '.join([stemmer.stem(w).strip(\"'\") for w in\n",
       "...   \"dish washer's fairly washed dishes\".split()])\n",
       "'dish washer fair wash dish'\n",
       "----\n",
       "\n",
       "==== Lemmatization\n",
       "\n",
       "If you have access to information about connections between the meanings of various words, you might be able to associate several words together even if their spelling is quite different.\n",
       "This more extensive normalization down to the semantic root of a word -- its lemma -- is called lemmatization.\n",
       "\n",
       "In chapter 12, we show how you can use lemmatization to reduce the complexity of the logic required to respond to a statement with a chatbot.\n",
       "Any NLP pipeline that wants to \"react\" the same for multiple different spellings of the same basic root word can benefit from a lemmatizer.\n",
       "It reduces the number of words you have to respond to, the dimensionality of your language model.\n",
       "Using it can make your model more general, but it can also make your model less precise, because it will treat all spelling variations of a given root word the same.\n",
       "For example \"chat\", \"chatter\", \"chatty\", \"chatting\", and perhaps even \"chatbot\" would all be treated the same in an NLP pipeline with lemmatization, even though they have different meanings.\n",
       "Likewise \"bank\", \"banked\", and \"banking\" would be treated the same by a stemming pipeline despite the river meaning of \"bank\", the motorcycle meaning of \"banked\" and the finance meaning of \"banking.\"\n",
       "\n",
       "As you work through this section, think about words where lemmatization would drastically alter the meaning of a word, perhaps even inverting its meaning and producing the opposite of the intended response from your pipeline.\n",
       "This scenario is called _spoofing_ -- when you try to elicit the wrong response from a machine learning pipeline by cleverly constructing a difficult input.\n",
       "\n",
       "Sometimes lemmatization will be a better way to normalize the words in your vocabulary.\n",
       "You may find that for your application stemming and case folding create stems and tokens that do not take into account a word's meaning.\n",
       "A lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.\n",
       "\n",
       "Some lemmatizers use the word's part of speech (POS) tag in addition to its spelling to help improve accuracy.\n",
       "The POS tag for a word indicates its role in the grammar of a phrase or sentence.\n",
       "For example, the noun POS is for words that refer to \"people, places, or things\" within a phrase.\n",
       "An adjective POS is for a word that modifies or describes a noun.\n",
       "A verb refers to an action.\n",
       "The POS of a word in isolation cannot be determined.\n",
       "The context of a word must be known for its POS to be identified.\n",
       "So some advanced lemmatizers cannot be run on words in isolation.\n",
       "\n",
       "Can you think of ways you can use the part of speech to identify a better \"root\" of a word than stemming could?\n",
       "Consider the word `better`.\n",
       "Stemmers would strip the \"er\" ending from \"better\" and return the stem \"bett\" or \"bet\".\n",
       "However, this would lump the word \"better\" with words like \"betting\", \"bets\", and \"Bet's\", rather than more similar words like \"betterment\", \"best\", or even \"good\" and \"goods\".\n",
       "\n",
       "So lemmatizers are better than stemmers for most applications.\n",
       "Stemmers are only really used in large scale information retrieval applications (keyword search).\n",
       "And if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer.\n",
       "Because the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer.\n",
       "This trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone.footnote:[Thank you Kyle Gorman for pointing this out]\n",
       "\n",
       "How can you identify word lemmas in Python?\n",
       "The NLTK package provides functions for this.\n",
       "Notice that you must tell the WordNetLemmatizer which part of speech you are interested in, if you want to find the most accurate lemma:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> nltk.download('wordnet')\n",
       "True\n",
       ">>> nltk.download('omw-1.4')\n",
       "True\n",
       ">>> from nltk.stem import WordNetLemmatizer\n",
       ">>> lemmatizer = WordNetLemmatizer()\n",
       ">>> lemmatizer.lemmatize(\"better\")  # <1>\n",
       "'better'\n",
       ">>> lemmatizer.lemmatize(\"better\", pos=\"a\")  # <2>\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"good\", pos=\"a\")\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"goods\", pos=\"a\")\n",
       "'goods'\n",
       ">>> lemmatizer.lemmatize(\"goods\", pos=\"n\")\n",
       "'good'\n",
       ">>> lemmatizer.lemmatize(\"goodness\", pos=\"n\")\n",
       "'goodness'\n",
       ">>> lemmatizer.lemmatize(\"best\", pos=\"a\")\n",
       "'best'\n",
       "----\n",
       "<1> The default part of speech is \"n\" for \"noun\"\n",
       "<2> \"a\" indicates the \"adjective\" part of speech\n",
       "\n",
       "You might be surprised that the first attempt to lemmatize the word \"better\" did not change it at all. This is because the part of speech of a word can have a big effect on its meaning. If a POS is not specified for a word, then the NLTK lemmatizer assumes it is a noun. Once you specify the correct POS, 'a' for adjective, the lemmatizer returns the correct lemma. Unfortunately, the NLTK lemmatizer is restricted to the connections within the Princeton WordNet graph of word meanings. So the word \"best\" does not lemmatize to the same root as \"better\". This graph is also missing the connection between \"goodness\" and \"good\". A Porter stemmer, on the other hand, would make this connection by blindly stripping off the \"ness\" ending of all words.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> stemmer.stem('goodness')\n",
       "'good'\n",
       "----\n",
       "\n",
       "You can easily implement lemmatization in spaCy by the following:\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import spacy\n",
       ">>> nlp = spacy.load(\"en_core_web_sm\")\n",
       ">>> doc = nlp(\"better good goods goodness best\")\n",
       ">>> for token in doc:\n",
       ">>> print(token.text, token.lemma_)\n",
       "better well\n",
       "good good\n",
       "goods good\n",
       "goodness goodness\n",
       "best good\n",
       "----\n",
       "Unlike NLTK, spaCy lemmatizes \"better\" to \"well\" by assuming it is an adverb and returns the correct lemma for \"best\" (\"good\").\n",
       "\n",
       "==== Synonym substitution\n",
       "\n",
       "There are five kinds of \"synonyms\" that are sometime helpful in creating a consistent smaller vocabulary to help your NLP pipeline generalize well.\n",
       "\n",
       ". Typo correction\n",
       ". Spelling correction\n",
       ". Synonym substitution\n",
       ". Contraction expansion\n",
       ". Emoji expansion\n",
       "\n",
       "Each of these synonym substitution algorithms can be designed to be more or less agressive.\n",
       "And you will want to think about the language used by your users in your domain.\n",
       "For example, in the legal, technical, or medical fields, it's rarely a good idea to substitute synonyms.\n",
       "A doctor wouldn't want a chatbot telling his patient their \"heart is broken\" because of some synonym substitutions on the heart emoticon (\"<3\").\n",
       "\n",
       "Nonetheless, the use cases for lemmatization and stemming apply to synonym substitution.\n",
       "\n",
       "==== Use cases\n",
       "\n",
       "When should you use a lemmatizer, stemmer, or synonym substitution?\n",
       "Stemmers are generally faster to compute and require less-complex code and datasets.\n",
       "But stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would.\n",
       "Both stemmers and lemmatizers will reduce your vocabulary size and increase the ambiguity of the text.\n",
       "But lemmatizers do a better job retaining as much of the information content as possible based on how the word was used within the text and its intended meaning.\n",
       "As a result, some state of the art NLP packages, such as spaCy, do not provide stemming functions and only offer lemmatization methods.\n",
       "\n",
       "If your application involves search, stemming and lemmatization will improve the recall of your searches by associating more documents with the same query words.\n",
       "However, stemming, lemmatization, and even case folding will usually reduce the precision and accuracy of your search results.\n",
       "These vocabulary compression approaches may cause your information retrieval system (search engine) to return many documents not relevant to the words' original meanings.\n",
       "These are called \"false positives\", a incorrect matches to your search query.\n",
       "Sometimes \"false positives\" are less important than false negatives.\n",
       "A false negative for a search engine is when it fails to list the document you are looking for at all.\n",
       "\n",
       "Because search results can be ranked according to relevance, search engines and document indexes typically use lemmatization when they process your query and index your documents.\n",
       "Because search results can be ranked according to relevance, search engines and document indexes typically use lemmatization in their NLP pipeline.\n",
       "This means a search engine will use lemmatization when they tokenize your search text as well as when they index their collection of documents, such as the web pages they crawl.\n",
       "\n",
       "But they combine search results for unstemmed versions of words to rank the search results that they present to you.footnote:[Additional metadata is also used to adjust the ranking of search results.\n",
       "Duck Duck Go and other popular web search engines combine more than 400 independent algorithms (including user-contributed algorithms) to rank your search results (https://duck.co/help/results/sources).]\n",
       "\n",
       "For a search-based chatbot, precision is usually more important than recall.\n",
       "A false positive match can cause your chatbot says something inappropriate.\n",
       "False negatives just cause your chatbot to have to humbly admit that it cannot find anything appropriate to say.\n",
       "Your chatbot will sound better if your NLP pipeline first searches for matches to your user's questions using unstemmed, unnormalized words.\n",
       "Your search algorithm can fall back to normalized token matches if it cannot find anything else to say.\n",
       "And you can rank these *fallback* matches for normalized tokens lower than the unnormalized token matches.\n",
       "You can even give your bot humility and transparency by introducing lower ranked responses with a caveat, such as \"I haven't heard a phrase like that before, but using my stemmer I found...\"\n",
       "In a modern world crowded with blowhard chatbots, your humbler chatbot can make a name for itself and win out!footnote:[\"Nice guys finish first!\" -- M.A. Nowak author of _SuperCooperators_\"]\n",
       "\n",
       "There are 4 situations when synonym substitution of some sort may make sense.\n",
       "\n",
       ". Search engines\n",
       ". Data augmentation\n",
       ". Scoring the robustness of your NLP\n",
       ". Adversarial NLP\n",
       "\n",
       "Search engines can improve their recall for rare terms by using synonym substitution.\n",
       "When you have limited labeled data, you can often expand your dataset 10 fold (10x) with synonym substitution alone.\n",
       "If you want to find a lower bound on the accuracy of your model you can aggressively substitute synonyms in your test set to see how robust your model is to these changes.\n",
       "And if you are searching for ways to poison or evade detection by an NLP algorithm, synonyms can give you a large number of probing texts to try.\n",
       "You can imagine that substituting the \"currency\" for the word \"cash\", \"dollars\", or \"$$$$\" might help evade a spam detector.\n",
       "\n",
       "[IMPORTANT]\n",
       "Bottom line, try to avoid stemming, lemmatization, case folding, or synonym substitution, unless you have a limited amount of text with contains usages and capitalizations of the words you are interested in.\n",
       "And with the explosion of NLP datasets, this is rarely the case for English documents, unless your documents use a lot of jargon or are from a very small subfield of science, technology, or literature.\n",
       "Nonetheless, for languages other than English, you may still find uses for lemmatization.\n",
       "The Stanford information retrieval course dismisses stemming and lemmatization entirely, due to the negligible recall accuracy improvement and the significant reduction in precision.footnote:[See the Stanford NLP Information Retrieval (IR) book section titled \"Stemming and lemmatization\" (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).]\n",
       "\n",
       "\n",
       "\n",
       "== Sentiment\n",
       "\n",
       "Whether you use raw single-word tokens, _n_-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information.\n",
       "An important part of this information is the word's sentiment -- the overall feeling or emotion that word invokes.\n",
       "This _sentiment analysis_ -- measuring the sentiment of phrases or chunks of text -- is a common application of NLP.\n",
       "In many companies it is the main thing an NLP engineer is asked to do.\n",
       "\n",
       "Companies like to know what users think of their products.\n",
       "So they often will provide some way for you to give feedback.\n",
       "A star rating on Amazon or Rotten Tomatoes is one way to get quantitative data about how people feel about products they've purchased.\n",
       "But a more natural way is to use natural language comments.\n",
       "Giving your user a blank slate (an empty text box) to fill up with comments about your product can produce more detailed feedback.\n",
       "\n",
       "In the past you would have to read all that feedback.\n",
       "Only a human can understand something like emotion and sentiment in natural language text, right?\n",
       "However, if you had to read thousands of reviews you would see how tedious and error-prone a human reader can be.\n",
       "Humans are remarkably bad at reading feedback, especially criticism or negative feedback.\n",
       "And customers are not generally very good at communicating feedback in a way that can get past your natural human triggers and filters.\n",
       "\n",
       "But machines do not have those biases and emotional triggers.\n",
       "And humans are not the only things that can process natural language text and extract information, even meaning from it.\n",
       "An NLP pipeline can process a large quantity of user feedback quickly and objectively, with less chance for bias.\n",
       "And an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\n",
       "\n",
       "Another common application of sentiment analysis is junk mail and troll message filtering.\n",
       "You would like your chatbot to be able to measure the sentiment in the chat messages it processes so it can respond appropriately.\n",
       "And even more importantly, you want your chatbot to measure its own sentiment of the statements it is about to send out, which you can use to steer your bot to be kind and pro-social with the statements it makes.\n",
       "The simplest way to do this might be to do what Moms told us to do: If you cannot say something nice, do not say anything at all.\n",
       "So you need your bot to measure the niceness of everything you are about to say and use that to decide whether to respond.\n",
       "\n",
       "What kind of pipeline would you create to measure the sentiment of a block of text and produce this sentiment positivity number?\n",
       "Say you just want to measure the positivity or favorability of a text -- how much someone likes a product or service that they are writing about.\n",
       "Say you want your NLP pipeline and sentiment analysis algorithm to output a single floating point number between -1 and +1.\n",
       "Your algorithm would output +1 for text with positive sentiment like \"Absolutely perfect! Love it! :-) :-) :-)\".\n",
       "And your algorithm should output -1 for text with negative sentiment like \"Horrible! Completely useless. :(\".\n",
       "Your NLP pipeline could use values near 0, like say +0.1, for a statement like \"It was OK. Some good and some bad things\".\n",
       "\n",
       "There are two approaches to sentiment analysis:\n",
       "\n",
       "* A rule-based algorithm composed by a human\n",
       "* A _machine learning_ model learned from data by a machine\n",
       "\n",
       "The first approach to sentiment analysis uses human-designed rules, sometimes called heuristics, to measure sentiment.\n",
       "A common rule-based approach to sentiment analysis is to find keywords in the text and map each one to numerical scores or weights in a dictionary or \"mapping\" -- a Python `dict`, for example.\n",
       "Now that you know how to do tokenization, you can use stems, lemmas, or _n_-gram tokens in your dictionary, rather than just words.\n",
       "The \"rule\" in your algorithm would be to add up these scores for each keyword in a document that you can find in your dictionary of sentiment scores.\n",
       "Of course you need to hand-compose this dictionary of keywords and their sentiment scores before you can run this algorithm on a body of text.\n",
       "We show you how to do this using the VADER algorithm (in `sklearn`) in the upcoming listing.\n",
       "\n",
       "The second approach, machine learning, relies on a labeled set of statements or documents to train a machine learning model to create those rules.\n",
       "A machine learning sentiment model is trained to process input text and output a numerical value for the sentiment you are trying to measure, like positivity or spamminess or trolliness.\n",
       "For the machine learning approach, you need a lot of data, text labeled with the \"right\" sentiment score.\n",
       "Twitter feeds are often used for this approach because the hash tags, such as `\\#awesome` or `\\#happy` or `\\#sarcasm`, can often be used to create a \"self-labeled\" dataset.\n",
       "Your company may have product reviews with five-star ratings that you could associate with reviewer comments.\n",
       "You can use the star ratings as a numerical score for the positivity of each text.\n",
       "We show you shortly how to process a dataset like this and train a token-based machine learning algorithm called _Naive Bayes_ to measure the positivity of the sentiment in a set of reviews after you are done with VADER.\n",
       "\n",
       "=== VADER -- A rule-based sentiment analyzer\n",
       "\n",
       "Hutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms.\n",
       "They called their algorithm VADER, for **V**alence **A**ware **D**ictionary for\n",
       "s**E**ntiment **R**easoning.footnote:[\"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\" by Hutto and Gilbert (http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).]\n",
       "Many NLP packages implement some form of this algorithm.\n",
       "The NLTK package has an implementation of the VADER algorithm in `nltk.sentiment.vader`.\n",
       "Hutto himself maintains the Python package `vaderSentiment`.\n",
       "You will go straight to the source and use `vaderSentiment` here.\n",
       "\n",
       "You will need to `pip install vaderSentiment` to run the following example.footnote:[You can find more detailed installation instructions with the package source code on github (https://github.com/cjhutto/vaderSentiment).]\n",
       "You have not included it in the `nlpia` package.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
       ">>> sa = SentimentIntensityAnalyzer()\n",
       ">>> sa.lexicon  # <1>\n",
       "{ ...\n",
       "':(': -1.9,  # <2>\n",
       "':)': 2.0,\n",
       "...\n",
       "'pls': 0.3,  # <3>\n",
       "'plz': 0.3,\n",
       "...\n",
       "'great': 3.1,\n",
       "... }\n",
       ">>> [(tok, score) for tok, score in sa.lexicon.items()\n",
       "...   if \" \" in tok]  # <4>\n",
       "[(\"( '}{' )\", 1.6),\n",
       " (\"can't stand\", -2.0),\n",
       " ('fed up', -1.8),\n",
       " ('screwed up', -1.5)]\n",
       ">>> sa.polarity_scores(text=\\\n",
       "...   \"Python is very readable and it's great for NLP.\")\n",
       "{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661,\n",
       "'pos': 0.339}  # <5>\n",
       ">>> sa.polarity_scores(text=\\\n",
       "...   \"Python is not a bad choice for most applications.\")\n",
       "{'compound': 0.431, 'neg': 0.0, 'neu': 0.711,\n",
       "'pos': 0.289}  # <6>\n",
       "----\n",
       "<1> SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores that we talked about.\n",
       "<2> A tokenizer better be good at dealing with punctuation and emoticons (emojis) for VADER to work well. After all, emoticons are designed to convey a lot of sentiment (emotion).\n",
       "<3> If you use a stemmer (or lemmatizer) in your pipeline, you would need to apply that stemmer to the VADER lexicon, too, combining the scores for all the words that go together in a single stem or lemma.\n",
       "<4> Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually _n_-grams; the other is an emoticon for \"kiss\".\n",
       "<5> The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them together into a compound positivity sentiment.\n",
       "<6> Notice that VADER handles negation pretty well -- \"great\" has a slightly more positive sentiment than \"not bad\". VADER's built-in tokenizer ignores any words that aren't in its lexicon, and it does not consider _n_-grams at all.\n",
       "\n",
       "Let us see how well this rule-based approach does for the example statements we mentioned earlier.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
       "...           \"Horrible! Completely useless. :(\",\n",
       "...           \"It was OK. Some good and some bad things.\"]\n",
       ">>> for doc in corpus:\n",
       "...     scores = sa.polarity_scores(doc)\n",
       "...     print('{:+}: {}'.format(scores['compound'], doc))\n",
       "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
       "-0.8768: Horrible! Completely useless. :(\n",
       "+0.3254: It was OK. Some good and some bad things.\n",
       "----\n",
       "\n",
       "This looks a lot like what you wanted.\n",
       "So the only drawback is that VADER does not look at all the words in a document.\n",
       "VADER only \"knows\" about the 7,500 words or so that were hard-coded into its algorithm.\n",
       "What if you want all the words to help add to the sentiment score?\n",
       "And what if you do not want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in `SentimentIntensityAnalyzer.lexicon`?\n",
       "The rule-based approach might be impossible if you do not understand the language because you would not know what scores to put in the dictionary (lexicon)!\n",
       "\n",
       "That is what machine learning sentiment analyzers are for.\n",
       "\n",
       "=== Closeness of vectors\n",
       "\n",
       "Why do we use bags of words rather than bags of characters to represent natural language text?\n",
       "For a cryptographer trying to decrypt an unknown message, frequency analysis of the characters in the text would be a good way to go.\n",
       "But for natural language text in your native language, words turn out to be a better representation.\n",
       "You can see this if you think about what we are using these BOW vectors for.\n",
       "\n",
       "If you think about it, you have a lot of different ways to measure the closeness of things.\n",
       "You probably have a good feel for what a close family relative would be.\n",
       "Or the closeness of the cafes where you can meet your friend to collaborate on writing a book about AI.\n",
       "For cafes your brain probably uses Euclidean distance on the 2D position of the cafes you know about.\n",
       "Or maybe Manhattan or taxi-cab distance.\n",
       "\n",
       "But do you know how to measure the closeness of two pieces of text?\n",
       "In chapter 4 you'll learn about edit distances that check the similarity of two strings of characters.\n",
       "But that doesn't really capture the essence of what you care about.\n",
       "\n",
       "How close are these sentences to each other, in your mind?\n",
       "\n",
       "> I am now coming over to see you.\n",
       "\n",
       "> I am not coming over to see you.\n",
       "\n",
       "Do you see the difference?\n",
       "Which one would you prefer to receive an e-mail from your friend.\n",
       "The words \"now\" and \"not\" are very far apart in meaning.\n",
       "But they are very close in spelling.\n",
       "This is an example about how a single character can change the meaning of an entire sentence.\n",
       "\n",
       "If you just counted up the characters that were different you'd get a distance of 1.\n",
       "And then you could divide by the length of the longest sentence to make sure your distance value is between 0 and 1.\n",
       "So your character difference or distance calculation would be 1 divided by 32 which gives 0.03125, or about 3%.\n",
       "Then, to turn a distance into a closeness you just subtract it from 1.\n",
       "So do you think these two sentences are 0.96875, or about 97% the same?\n",
       "They mean the opposite.\n",
       "So we'd like a better measure than that.\n",
       "\n",
       "What if you compared words instead of characters?\n",
       "In that case you would have one word out of seven that was changed.\n",
       "That is a little better than one character out of 32.\n",
       "The sentences would now have a closeness score of six divided by seven or about 85%.\n",
       "That's a little lower, which is what we want.\n",
       "\n",
       "For natural language you don't want your closeness or distance measure to rely only on a count of the differences in individual characters.\n",
       "This is one reason why you want to use words as your tokens of meaning when processing natural language text.\n",
       "\n",
       "What about these two sentences?\n",
       "\n",
       "> She and I will come over to your place at 3:00.\n",
       "\n",
       "> At 3:00, she and I will stop by your apartment.\n",
       "\n",
       "Are these two sentences close to each other in meaning?\n",
       "They have the exact same length in characters.\n",
       "And they use some of the same words, or at least synonyms.\n",
       "But those words and characters are not in the same order.\n",
       "So we need to make sure that our representation of the sentences does not rely on the precise position of words in a sentence.\n",
       "\n",
       "Bag of words vectors accomplish this by creating a position or slot in a vector for every word you've seen in your vocabulary.\n",
       "You may have learned of a few measures of closeness in geometry and linear algebra.\n",
       "\n",
       "As an example of why feature extraction from text is hard, consider _stemming_ -- grouping the various inflections of a word into the same \"bucket\" or cluster.\n",
       "Very smart people spent their careers developing algorithms for grouping inflected forms of words together based only on their spelling.\n",
       "Imagine how difficult that is.\n",
       "Imagine trying to remove verb endings like \"ing\" from \"ending\" so you'd have a stem called \"end\" to represent both words.\n",
       "And you'd like to stem the word \"running\" to \"run,\" so those two words are treated the same.\n",
       "And that's tricky, because you have remove not only the \"ing\" but also the extra \"n\".\n",
       "But you want the word \"sing\" to stay whole.\n",
       "You wouldn't want to remove the \"ing\" ending from \"sing\" or you'd end up with a single-letter \"s\".\n",
       "\n",
       "Or imagine trying to discriminate between a pluralizing \"s\" at the end of a word like \"words\" and a normal \"s\" at the end of words like \"bus\" and \"lens\".\n",
       "Do isolated individual letters in a word or parts of a word provide any information at all about that word's meaning?\n",
       "Can the letters be misleading?\n",
       "Yes and yes.\n",
       "\n",
       "In this chapter we show you how to make your NLP pipeline a bit smarter by dealing with these word spelling challenges using conventional stemming approaches.\n",
       "Later, in chapter 5, we show you statistical clustering approaches that only require you to amass a collection of natural language text containing the words you're interested in.\n",
       "From that collection of text, the statistics of word usage will reveal \"semantic stems\" (actually, more useful clusters of words like lemmas or synonyms), without any hand-crafted regular expressions or stemming rules.\n",
       "\n",
       "=== Count vectorizing\n",
       "\n",
       "In the previous sections you've only been concerned with keyword detection.\n",
       "Your vectors indicated the presence or absence of words.\n",
       "In order to handle longer documents and improve the accuracy of your NLP pipeline, you're going to start counting the occurrences of words in your documents.\n",
       "\n",
       "You can put these counts into a sort-of histogram.\n",
       "Just like before you will create a vector for each document in your pipeline.\n",
       "Only instead of 0's and 1's in your vectors there will be counts.\n",
       "This will improve the accuracy of all the similarity and distance calculations you are doing with these counts.\n",
       "And just like normalizing histograms can improve your ability to compare two histograms, normalizing your word counts is also a good idea.\n",
       "Otherwise a really short wikipedia article that use Barak Obama's name only once along side all the other presidents might get as much \"Barack Obama\" credit as a much longer page about Barack Obama that uses his name many times.\n",
       "Users and Question Answering bots like `qary` trying to answer questions about Obama might get distracted by pages listing all the presidents and might miss the main Barack Obama page entirely.\n",
       "So it's a good idea to normalize your count vectors by dividing the counts by the total length of the document.\n",
       "This more fairly represents the distribution of tokens in the document and will create better similarity scores with other documents, including the text from a search query from `qary`.footnote:[Qary is an open source virtual assistant that actually assists you instead of manipulating and misinforming you (https://docs.qary.ai).]\n",
       "\n",
       "Each position in your vector represents the count for one of your keywords.\n",
       "And having a small vocabulary keeps this vector small, low-dimensional, and easy to reason about.\n",
       "And you can use this _count vectorizing_ approach even for large vocabularies.\n",
       "\n",
       "And you can organize these counts of those keywords into\n",
       "You need to organize the counts into a vector.\n",
       "This opens up a whole range of powerful tools for doing vector algebra.\n",
       "\n",
       "In natural language processing, composing a numerical vector from text is a particularly \"lossy\" feature extraction process.\n",
       "Nonetheless the bag-of-words (BOW) vectors retain enough of the information content of the text to produce useful and interesting machine learning models.\n",
       "The techniques for sentiment analyzers at the end of this chapter are the exact same techniques Google used to save email from a flood of spam that almost made it useless.\n",
       "\n",
       "\n",
       "=== Naive Bayes\n",
       "\n",
       "A Naive Bayes model tries to find keywords in a set of documents that are predictive of your target (output) variable.\n",
       "When your target variable is the sentiment you are trying to predict, the model will find words that predict that sentiment.\n",
       "The nice thing about a Naive Bayes model is that the internal coefficients will map words or tokens to scores just like VADER does.\n",
       "Only this time you will not have to be limited to just what an individual human decided those scores should be.\n",
       "The machine will find the \"best\" scores for any problem.\n",
       "\n",
       "For any machine learning algorithm, you first need to find a dataset.\n",
       "You need a bunch of text documents that have labels for their positive emotional content (positivity sentiment).\n",
       "Hutto compiled four different sentiment datasets for us when he and his collaborators built VADER.\n",
       "You will load them from the `nlpia` package.footnote:[If you have not already installed `nlpia`, check out the installation instructions at http://gitlab.com/tangibleai/nlpia2.]\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> movies = pd.read_csv('https://proai.org/movie-reviews.csv.gz', \\\n",
       "...     index_col=0)\n",
       ">>> movies.head().round(2)\n",
       "    sentiment                                               text\n",
       "id                                                              \n",
       "1        2.27  The Rock is destined to be the 21st Century's ...\n",
       "2        3.53  The gorgeously elaborate continuation of ''The...\n",
       "3       -0.60                     Effective but too tepid biopic\n",
       "4        1.47  If you sometimes like to go to the movies to h...\n",
       "5        1.73  Emerges as something rare, an issue movie that...\n",
       "\n",
       ">>> movies.describe().round(2)\n",
       "       sentiment\n",
       "count   10605.00\n",
       "mean        0.00  # <1>\n",
       "std         1.92\n",
       "min        -3.88  # <2>\n",
       "...\n",
       "max         3.94  # <3>\n",
       "----\n",
       "<1> Sentiment scores (movie ratings) have been \"centered\" (mean is zero)\n",
       "<2> It looks like the scale starts around -4 for the worst movies\n",
       "<3> Seems like +4 is the maximum rating for the best movies\n",
       "\n",
       "It looks like the movie reviews have been _centered_: normalized by subtracting the mean so that the new mean will be zero and they aren't biased to one side or the other.\n",
       "And it seems the range of movie ratings allowed was -4 to +4.\n",
       "\n",
       "Now you can tokenize all those movie review texts to create a bag of words for each one.\n",
       "If you put them all into a Pandas DataFrame that will make them easier to work with.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> import pandas as pd\n",
       ">>> pd.options.display.width = 75  # <1>\n",
       ">>> from nltk.tokenize import casual_tokenize  # <2>\n",
       ">>> bags_of_words = []\n",
       ">>> from collections import Counter  # <3>\n",
       ">>> for text in movies.text:\n",
       "...     bags_of_words.append(Counter(casual_tokenize(text)))\n",
       ">>> df_bows = pd.DataFrame.from_records(bags_of_words)  # <4>\n",
       ">>> df_bows = df_bows.fillna(0).astype(int)  # <5>\n",
       ">>> df_bows.shape  # <6>\n",
       "(10605, 20756)\n",
       "\n",
       ">>> df_bows.head()\n",
       "   !  \"  #  $  %  &  ' ...  zone  zoning  zzzzzzzzz  ½  élan  –  ’\n",
       "0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n",
       "1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0\n",
       "2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0\n",
       "\n",
       ">>> df_bows.head()[list(bags_of_words[0].keys())]\n",
       "   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  .\n",
       "0    1     1   1         1   2   1 ...    1      1   1       1      1  1\n",
       "1    2     0   1         0   0   0 ...    0      0   0       0      0  4\n",
       "2    0     0   0         0   0   0 ...    0      0   0       0      0  0\n",
       "3    0     0   1         0   4   0 ...    0      0   0       0      0  1\n",
       "4    0     0   0         0   0   0 ...    0      0   0       0      0  1\n",
       "----\n",
       "<1> This prints a wide `DataFrame` in the console so they look prettier.\n",
       "<2> NLTK's `casual_tokenize` can handle emoticons, unusual punctuation, and slang better than `TreebankWordTokenizer`\n",
       "<3> `Counter` takes a list (or iterable) of objects and counts them up, returning a `dict` where the keys are the objects (tokens in your case) and the values are the counts.\n",
       "<4> The `from_records()` DataFrame constructor takes a sequence of dict objects. The `dict` keys become columns, and the values for missing keys are set to `NaN`.\n",
       "<5> NumPy and Pandas can only represent NaNs within a float dtype. So fill NaNs with zeros before converting to integers.\n",
       "<6> A BOW table can get really big if you don't do dimension reduction or feature selection.\n",
       "\n",
       "When you do not use case normalization, stop word filters, stemming, or lemmatization your vocabulary can be quite huge because you are keeping track of every little difference in spelling or capitalization of words.\n",
       "Try inserting some dimension reduction steps into your pipeline to see how they affect your pipeline's accuracy and the amount of memory required to store all these BOWs.\n",
       "\n",
       "Now you have all the data that a Naive Bayes model needs to find the keywords that predict sentiment from natural language text.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> from sklearn.naive_bayes import MultinomialNB\n",
       ">>> nb = MultinomialNB()\n",
       ">>> nb = nb.fit(df_bows, movies.sentiment > 0)  # <1>\n",
       ">>> movies['pred_senti'] = (\n",
       "...   nb.predict_proba(df_bows))[:, 1] * 8 - 4  # <2>\n",
       ">>> movies['error'] = movies.pred_senti - movies.sentiment\n",
       ">>> mae = movies['error'].abs().mean().round(1)  # <3>\n",
       ">>> mae\n",
       "1.9\n",
       "----\n",
       "\n",
       "To create a binary classification label you can use the fact that the centered movie ratings (sentiment labels) are positive (greater than zero) when the sentiment of the review is positive.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> movies['senti_ispos'] = (movies['sentiment'] > 0).astype(int)\n",
       ">>> movies['pred_ispos'] = (movies['pred_senti'] > 0).astype(int)\n",
       ">>> columns = [c for c in movies.columns if 'senti' in c or 'pred' in c]\n",
       ">>> movies[columns].head(8)\n",
       "    sentiment  pred_senti  senti_ispos  pred_ispos\n",
       "id\n",
       "1    2.266667                    4                     1                     1\n",
       "2    3.533333                    4                     1                     1\n",
       "3   -0.600000                   -4                     0                     0\n",
       "4    1.466667                    4                     1                     1\n",
       "5    1.733333                    4                     1                     1\n",
       "6    2.533333                    4                     1                     1\n",
       "7    2.466667                    4                     1                     1\n",
       "8    1.266667                   -4                     1                     0\n",
       ">>> (movies.pred_ispos ==\n",
       "...   movies.senti_ispos).sum() / len(movies)\n",
       "0.9344648750589345  # <4>\n",
       "----\n",
       "<1> Naive Bayes models are classifiers, so you need to convert your output variable (sentiment float) to a discrete label (integer, string, or bool).\n",
       "<2> Convert your discrete classification variable back to a real value between -4 and +4 so you can compare it to the \"ground truth\" sentiment.\n",
       "<3> Average absolute value of the prediction error or mean absolute error (MAE)\n",
       "<4> You got the \"thumbs up\" rating correct 93% of the time.\n",
       "\n",
       "This is a pretty good start at building a sentiment analyzer with only a few lines of code (and a lot of data).\n",
       "You did not have to guess at the sentiment associated with a list of 7500 words and hard code them into an algorithm such as VADER.\n",
       "Instead you told the machine the sentiment ratings for whole text snippets.\n",
       "And then the machine did all the work to figure out the sentiment associated with each word in those texts.\n",
       "That is the power of machine learning and NLP!\n",
       "\n",
       "How well do you think this model will generalize to a completely different set text examples such as product reviews?\n",
       "Do people use the same words to describe things they like in movie and product reviews such as electronics and household goods? \n",
       "Probably not.\n",
       "But it's a good idea to check the robustness of your language models by running it against challenging text from a different domain.\n",
       "And by testing your model on new domains, you can get ideas for more examples and datasets to use in your training and test sets.\n",
       "\n",
       "First you need to load the product reviews.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> products = pd.read_csv('https://proai.org/product-reviews.csv.gz')\n",
       ">>> for text in products['text']:\n",
       "...     bags_of_words.append(Counter(casual_tokenize(text)))\n",
       ">>> df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
       ">>> df_product_bows = df_product_bows.fillna(0).astype(int)\n",
       ">>> df_all_bows = df_bows.append(df_product_bows)\n",
       ">>> df_all_bows.columns  # <1>\n",
       "Index(['!', '\"', '#', '#38', '$', '%', '&', ''', '(', '(8',\n",
       "       ...\n",
       "       'zoomed', 'zooming', 'zooms', 'zx', 'zzzzzzzzz', '~', '½', 'élan',\n",
       "       '–', '’'],\n",
       "      dtype='object', length=23302)\n",
       ">>> df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]  # <2>\n",
       ">>> df_product_bows.shape\n",
       "(3546, 20756)\n",
       "\n",
       ">>> df_bows.shape  # <3>\n",
       "(10605, 20756)\n",
       "----\n",
       "<1> Your new bags of words have some tokens that were not in the original bags of words DataFrame (23302 columns now instead of 20756 before).\n",
       "<2> You need to make sure your new product DataFrame of bags of words has the exact same columns (tokens) in the exact same order as the original one used to train your Naive Bayes model.\n",
       "<3> The movie bags of words have the same size vocabulary (columns) as for products.\n",
       "\n",
       "Now you need to convert the labels to mimic the binary classification data that you trained your model on.\n",
       "\n",
       "[source,python]\n",
       "----\n",
       ">>> products['senti_ispos'] = (products['sentiment'] > 0).astype(int)\n",
       ">>> products['pred_ispos'] = nb.predict(df_product_bows).astype(int)\n",
       ">>> products.head()\n",
       "    id  sentiment                                               text  senti_ispos\n",
       "0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...                     0\n",
       "1  1_2      -0.15  repost from january 13, 2004 with a better fit...                     0\n",
       "2  1_3      -0.20  does your apex dvd player only play dvd audio ...                     0\n",
       "3  1_4      -0.10  or does it play audio and video but scrolling ...                     0\n",
       "4  1_5      -0.50  before you try to return the player or waste h...                     0\n",
       "\n",
       ">>> tp = products['pred_ispos'] == products['senti_ispos']  # <1>\n",
       ">>> tp.sum() / len(products)\n",
       "0.5572476029328821\n",
       "----\n",
       "<1> True positive predictions are when both the predicted and true sentiment are positive\n",
       "\n",
       "\n",
       "So your Naive Bayes model does a  poor job of predicting whether a product review is positive (thumbs up).\n",
       "One reason for this subpar performance is that your vocabulary from the `casual_tokenize` product texts has 2546 tokens that were not in the movie reviews.\n",
       "That is about 10% of the tokens in your original movie review tokenization, which means that all those words will not have any weights or scores in your Naive Bayes model.\n",
       "Also the Naive Bayes model does not deal with negation as well as VADER does.\n",
       "You would need to incorporate _n_-grams into your tokenizer to connect negation words (such as \"not\" or \"never\") to the positive words they might be used to qualify.\n",
       "\n",
       "We leave it to you to continue the NLP action by improving on this machine learning model.\n",
       "And you can check your progress relative to VADER at each step of the way to see if you think machine learning is a better approach than hard-coding algorithms for NLP.\n",
       "\n",
       "== Review\n",
       "\n",
       ". How does a lemmatizer increase the likelihood that your DuckDuckGo search results contain what you are looking for?\n",
       ". Is there a way to optimally decide the _n_ in the _n_-gram range you use to tokenize your documents?\n",
       ". Does lemmatization, case folding, or stopword removal help or hurt your performance on a model to predict misleading news articles with this Kaggle dataset:\n",
       ". How could your find out the best sizes for the word pieces or sentence pieces for your tokenizer?\n",
       ". Is there a website where you can download the token frequencies for most of the words and n-grams ever published?footnote:[Hint: A company that aspired to \"do no evil\", but now does, created this massive NLP corpus.]\n",
       ". What are the risks and possible benefits of pair coding AI assistants built with NLP? What sort of organizations and algorithms do you trust with your mind and your code?\n",
       "\n",
       "=== Summary\n",
       "\n",
       "* You implemented tokenization and configured a tokenizer for your application.\n",
       "* _n_-gram tokenization helps retain some of the \"word order\" information in a document.\n",
       "* Normalization and stemming consolidate words into groups that improve the \"recall\" for search engines but reduce precision.\n",
       "* Lemmatization and customized tokenizers like `casual_tokenize()` can improve precision and reduce information loss.\n",
       "* Stop words can contain useful information, and discarding them is not always helpful."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6036d293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40k'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(list(doc)) / 10_000)}0k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2b666fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8kWPS'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(doc) / 1_000 / 4.67)}kWPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42700b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4321f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad899a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f5eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3089a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/maer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f62bfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6017a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b538c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3c87c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=',\n",
       " 'Tokens',\n",
       " 'of',\n",
       " 'thought',\n",
       " '(',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'words',\n",
       " ')',\n",
       " ':',\n",
       " 'chapter',\n",
       " ':',\n",
       " '2',\n",
       " ':',\n",
       " 'part',\n",
       " ':',\n",
       " '1',\n",
       " ':',\n",
       " 'imagesdir',\n",
       " ':',\n",
       " '.',\n",
       " ':',\n",
       " 'xrefstyle',\n",
       " ':',\n",
       " 'short',\n",
       " ':',\n",
       " 'figure-caption',\n",
       " ':',\n",
       " 'Figure',\n",
       " '{',\n",
       " 'chapter',\n",
       " '}',\n",
       " '.',\n",
       " ':',\n",
       " 'listing-caption',\n",
       " ':',\n",
       " 'Listing',\n",
       " '{',\n",
       " 'chapter',\n",
       " '}',\n",
       " '.',\n",
       " ':',\n",
       " 'table-caption',\n",
       " ':',\n",
       " 'Table',\n",
       " '{',\n",
       " 'chapter',\n",
       " '}',\n",
       " '.',\n",
       " ':',\n",
       " 'stem',\n",
       " ':',\n",
       " 'latexmath',\n",
       " 'This',\n",
       " 'chapter',\n",
       " 'covers',\n",
       " '*',\n",
       " 'Parsing',\n",
       " 'your',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " 'and',\n",
       " '_n_-grams',\n",
       " '(',\n",
       " 'tokens',\n",
       " ')',\n",
       " '*',\n",
       " 'Tokenizing',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'emoticons',\n",
       " ',',\n",
       " 'and',\n",
       " 'even',\n",
       " 'Chinese',\n",
       " 'characters',\n",
       " '*',\n",
       " 'Consolidating',\n",
       " 'your',\n",
       " 'vocabulary',\n",
       " 'with',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'and',\n",
       " 'case',\n",
       " 'folding',\n",
       " '*',\n",
       " 'Building',\n",
       " 'a',\n",
       " 'structured',\n",
       " 'numerical',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'text',\n",
       " '*',\n",
       " 'Scoring',\n",
       " 'text',\n",
       " 'for',\n",
       " 'sentiment',\n",
       " 'and',\n",
       " 'prosocial',\n",
       " 'intent',\n",
       " '*',\n",
       " 'Using',\n",
       " 'character',\n",
       " 'frequency',\n",
       " 'analysis',\n",
       " 'to',\n",
       " 'optimize',\n",
       " 'your',\n",
       " 'token',\n",
       " 'vocabulary',\n",
       " '*',\n",
       " 'Dealing',\n",
       " 'with',\n",
       " 'variable',\n",
       " 'length',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'words',\n",
       " 'and',\n",
       " 'tokens',\n",
       " 'So',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'help',\n",
       " 'save',\n",
       " 'the',\n",
       " 'world',\n",
       " 'with',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " '?',\n",
       " 'First',\n",
       " 'your',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'will',\n",
       " 'need',\n",
       " 'to',\n",
       " 'compute',\n",
       " 'something',\n",
       " 'about',\n",
       " 'text',\n",
       " ',',\n",
       " 'and',\n",
       " 'for',\n",
       " 'that',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'need',\n",
       " 'a',\n",
       " 'way',\n",
       " 'to',\n",
       " 'represent',\n",
       " 'text',\n",
       " 'in',\n",
       " 'a',\n",
       " 'numerical',\n",
       " 'data',\n",
       " 'structure',\n",
       " '.',\n",
       " 'The',\n",
       " 'part',\n",
       " 'of',\n",
       " 'an',\n",
       " 'NLP',\n",
       " 'pipeline',\n",
       " 'that',\n",
       " 'breaks',\n",
       " 'up',\n",
       " 'your',\n",
       " 'text',\n",
       " 'to',\n",
       " 'create',\n",
       " 'this',\n",
       " 'structured',\n",
       " 'numerical',\n",
       " 'data',\n",
       " 'is',\n",
       " 'called',\n",
       " 'a',\n",
       " '_parser_',\n",
       " '.',\n",
       " 'For',\n",
       " 'many',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " ',',\n",
       " 'you',\n",
       " 'only',\n",
       " 'need',\n",
       " 'to',\n",
       " 'convert',\n",
       " 'your',\n",
       " 'text',\n",
       " 'to',\n",
       " 'a',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'searching',\n",
       " 'and',\n",
       " 'classifying',\n",
       " 'text',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'now',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'to',\n",
       " 'split',\n",
       " 'a',\n",
       " 'document',\n",
       " ',',\n",
       " 'any',\n",
       " 'string',\n",
       " ',',\n",
       " 'into',\n",
       " 'discrete',\n",
       " 'tokens',\n",
       " 'of',\n",
       " 'meaning',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'parse',\n",
       " 'text',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'small',\n",
       " 'as',\n",
       " 'a',\n",
       " 'single',\n",
       " 'word',\n",
       " 'and',\n",
       " 'as',\n",
       " 'large',\n",
       " 'as',\n",
       " 'an',\n",
       " 'entire',\n",
       " 'Encyclopedia',\n",
       " '.',\n",
       " 'And',\n",
       " 'they',\n",
       " 'will',\n",
       " 'all',\n",
       " 'produce',\n",
       " 'a',\n",
       " 'consistent',\n",
       " 'representation',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'to',\n",
       " 'compare',\n",
       " 'them',\n",
       " '.',\n",
       " 'For',\n",
       " 'this',\n",
       " 'chapter',\n",
       " 'your',\n",
       " 'tokens',\n",
       " 'will',\n",
       " 'be',\n",
       " 'words',\n",
       " ',',\n",
       " 'punctuation',\n",
       " 'marks',\n",
       " ',',\n",
       " 'and',\n",
       " 'even',\n",
       " 'pictograms',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Chinese',\n",
       " 'characters',\n",
       " ',',\n",
       " 'emojis',\n",
       " 'and',\n",
       " 'emoticons',\n",
       " '.',\n",
       " 'Later',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book',\n",
       " 'you',\n",
       " 'will',\n",
       " 'see',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'these',\n",
       " 'same',\n",
       " 'techniques',\n",
       " 'to',\n",
       " 'find',\n",
       " 'packets',\n",
       " 'of',\n",
       " 'meaning',\n",
       " 'in',\n",
       " 'any',\n",
       " 'discrete',\n",
       " 'sequence',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'your',\n",
       " 'tokens',\n",
       " 'could',\n",
       " 'be',\n",
       " 'the',\n",
       " 'ASCII',\n",
       " 'characters',\n",
       " 'represented',\n",
       " 'by',\n",
       " 'a',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'bytes',\n",
       " ',',\n",
       " 'perhaps',\n",
       " 'with',\n",
       " 'ASCII',\n",
       " 'emoticons',\n",
       " '.',\n",
       " 'Or',\n",
       " 'they',\n",
       " 'could',\n",
       " 'be',\n",
       " 'Unicode',\n",
       " 'emojis',\n",
       " ',',\n",
       " 'mathematical',\n",
       " 'symbols',\n",
       " ',',\n",
       " 'Egyption',\n",
       " ',',\n",
       " 'hieroglyphics',\n",
       " ',',\n",
       " 'pictographs',\n",
       " 'from',\n",
       " 'languages',\n",
       " 'like',\n",
       " 'Kanji',\n",
       " 'and',\n",
       " 'Cantonese',\n",
       " '.',\n",
       " 'You',\n",
       " 'could',\n",
       " 'even',\n",
       " 'define',\n",
       " 'the',\n",
       " 'tokens',\n",
       " 'for',\n",
       " 'DNA',\n",
       " 'and',\n",
       " 'RNA',\n",
       " 'sequences',\n",
       " 'with',\n",
       " 'letters',\n",
       " 'for',\n",
       " 'each',\n",
       " 'of',\n",
       " 'the',\n",
       " 'five',\n",
       " 'base',\n",
       " 'nucleotides',\n",
       " ':',\n",
       " 'adenine',\n",
       " '(',\n",
       " 'A',\n",
       " ')',\n",
       " ',',\n",
       " 'guanine',\n",
       " '(',\n",
       " 'G',\n",
       " ')',\n",
       " ',',\n",
       " 'cytosine',\n",
       " '\\\\',\n",
       " '(',\n",
       " 'C',\n",
       " ')',\n",
       " ',',\n",
       " 'thymine',\n",
       " '(',\n",
       " 'T',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'uracil',\n",
       " '(',\n",
       " 'U',\n",
       " ')',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'tokens',\n",
       " 'are',\n",
       " 'all',\n",
       " 'around',\n",
       " 'you',\n",
       " '...',\n",
       " 'and',\n",
       " 'even',\n",
       " 'inside',\n",
       " 'you',\n",
       " '.',\n",
       " 'Is',\n",
       " 'there',\n",
       " 'something',\n",
       " 'you',\n",
       " 'can',\n",
       " 'do',\n",
       " 'with',\n",
       " 'tokens',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'require',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'complicated',\n",
       " 'deep',\n",
       " 'learning',\n",
       " '?',\n",
       " 'If',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'tokenizer',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'it',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'statistics',\n",
       " 'about',\n",
       " 'the',\n",
       " 'occurrence',\n",
       " 'of',\n",
       " 'tokens',\n",
       " 'in',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'your',\n",
       " 'blog',\n",
       " 'posts',\n",
       " 'or',\n",
       " 'a',\n",
       " 'business',\n",
       " 'website',\n",
       " '.',\n",
       " 'Then',\n",
       " 'you',\n",
       " 'can',\n",
       " 'build',\n",
       " 'a',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'in',\n",
       " 'pure',\n",
       " 'Python',\n",
       " 'with',\n",
       " 'just',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'to',\n",
       " 'represent',\n",
       " 'to',\n",
       " 'record',\n",
       " 'links',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents',\n",
       " 'where',\n",
       " 'those',\n",
       " 'words',\n",
       " 'occur',\n",
       " '.',\n",
       " 'That',\n",
       " 'Python',\n",
       " 'dictionary',\n",
       " 'that',\n",
       " 'maps',\n",
       " 'words',\n",
       " 'to',\n",
       " 'document',\n",
       " 'links',\n",
       " 'or',\n",
       " 'pages',\n",
       " 'is',\n",
       " 'called',\n",
       " 'a',\n",
       " 'reverse',\n",
       " 'index',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'like',\n",
       " 'the',\n",
       " 'index',\n",
       " 'at',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'this',\n",
       " 'book',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'called',\n",
       " '_information',\n",
       " 'retrieval_',\n",
       " '--',\n",
       " 'a',\n",
       " 'really',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'your',\n",
       " 'NLP',\n",
       " 'toolbox',\n",
       " '.',\n",
       " 'Statistics',\n",
       " 'about',\n",
       " 'tokens',\n",
       " 'are',\n",
       " 'often',\n",
       " 'all',\n",
       " 'you',\n",
       " 'need',\n",
       " 'for',\n",
       " 'keyword',\n",
       " 'detection',\n",
       " ',',\n",
       " 'full',\n",
       " 'text',\n",
       " 'search',\n",
       " ',',\n",
       " 'and',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'even',\n",
       " 'build',\n",
       " 'customer',\n",
       " 'support',\n",
       " 'chatbots',\n",
       " 'using',\n",
       " 'text',\n",
       " 'search',\n",
       " 'to',\n",
       " 'find',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'customers',\n",
       " \"'\",\n",
       " 'questions',\n",
       " 'in',\n",
       " 'your',\n",
       " 'documentation',\n",
       " 'or',\n",
       " 'FAQ',\n",
       " '(',\n",
       " 'frequently',\n",
       " 'asked',\n",
       " 'question',\n",
       " ')',\n",
       " 'lists',\n",
       " '.',\n",
       " 'A',\n",
       " 'chatbot',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'answer',\n",
       " 'your',\n",
       " 'questions',\n",
       " 'until',\n",
       " 'it',\n",
       " 'knows',\n",
       " 'where',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'the',\n",
       " 'answer',\n",
       " '.',\n",
       " 'Search',\n",
       " 'is',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'of',\n",
       " 'many',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'conversational',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'open',\n",
       " 'domain',\n",
       " 'question',\n",
       " 'answering',\n",
       " '.',\n",
       " 'A',\n",
       " 'tokenizer',\n",
       " 'forms',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'for',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'NLP',\n",
       " 'pipelines',\n",
       " '.',\n",
       " '===',\n",
       " 'Tokens',\n",
       " 'of',\n",
       " 'emotion',\n",
       " 'Another',\n",
       " 'practical',\n",
       " 'use',\n",
       " 'for',\n",
       " 'your',\n",
       " 'tokenizer',\n",
       " 'is',\n",
       " 'called',\n",
       " '_sentiment',\n",
       " 'analysis_',\n",
       " ',',\n",
       " 'or',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'text',\n",
       " 'to',\n",
       " 'estimate',\n",
       " 'emotion',\n",
       " '.',\n",
       " 'You',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'pipeline',\n",
       " 'later',\n",
       " 'in',\n",
       " 'this',\n",
       " 'chapter',\n",
       " '.',\n",
       " 'For',\n",
       " 'now',\n",
       " 'you',\n",
       " 'just',\n",
       " 'need',\n",
       " 'to',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'build',\n",
       " 'a',\n",
       " 'tokenizer',\n",
       " '.',\n",
       " 'And',\n",
       " 'your',\n",
       " 'tokenizer',\n",
       " 'will',\n",
       " 'almost',\n",
       " 'certainly',\n",
       " 'need',\n",
       " 'to',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'tokens',\n",
       " 'of',\n",
       " 'emotion',\n",
       " 'called',\n",
       " '_emoticons_',\n",
       " 'and',\n",
       " '_emojis_',\n",
       " '.',\n",
       " '_Emoticons_',\n",
       " 'are',\n",
       " 'a',\n",
       " 'textual',\n",
       " 'representations',\n",
       " 'of',\n",
       " 'a',\n",
       " 'writer',\n",
       " \"'s\",\n",
       " 'mood',\n",
       " 'or',\n",
       " 'facial',\n",
       " 'expression',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " '_smiley_',\n",
       " 'emoticon',\n",
       " ':',\n",
       " '`',\n",
       " ':',\n",
       " '-',\n",
       " ')',\n",
       " '`',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'kind-of',\n",
       " 'like',\n",
       " 'a',\n",
       " 'modern',\n",
       " 'hieroglyph',\n",
       " 'or',\n",
       " 'picture-word',\n",
       " 'for',\n",
       " 'computer',\n",
       " 'users',\n",
       " 'that',\n",
       " 'only',\n",
       " 'have',\n",
       " 'access',\n",
       " 'to',\n",
       " 'an',\n",
       " 'ASCII',\n",
       " 'terminal',\n",
       " 'for',\n",
       " 'communication',\n",
       " '.',\n",
       " '_Emojis_',\n",
       " 'are',\n",
       " 'the',\n",
       " 'graphical',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'these',\n",
       " 'characters',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'the',\n",
       " 'smilie',\n",
       " 'emoji',\n",
       " 'has',\n",
       " 'a',\n",
       " 'small',\n",
       " 'yellow',\n",
       " 'circle',\n",
       " 'with',\n",
       " 'two',\n",
       " 'black',\n",
       " 'dots',\n",
       " 'for',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'a',\n",
       " 'U',\n",
       " 'shaped',\n",
       " 'curve',\n",
       " 'for',\n",
       " 'a',\n",
       " 'mouth',\n",
       " '.',\n",
       " 'The',\n",
       " 'smiley',\n",
       " 'emoji',\n",
       " 'is',\n",
       " 'a',\n",
       " 'graphical',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'the',\n",
       " '`',\n",
       " ':',\n",
       " '-',\n",
       " ')',\n",
       " '`',\n",
       " 'smiley',\n",
       " 'emoticon',\n",
       " '.',\n",
       " 'Both',\n",
       " 'emojis',\n",
       " 'and',\n",
       " 'emoticons',\n",
       " 'have',\n",
       " 'evolved',\n",
       " 'into',\n",
       " 'their',\n",
       " 'own',\n",
       " 'language',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'popular',\n",
       " 'emojis',\n",
       " '.',\n",
       " 'People',\n",
       " 'have',\n",
       " 'created',\n",
       " 'emojis',\n",
       " 'for',\n",
       " 'everything',\n",
       " 'from',\n",
       " 'company',\n",
       " 'logos',\n",
       " 'to',\n",
       " 'memes',\n",
       " 'and',\n",
       " 'innuendo',\n",
       " '.',\n",
       " 'Noncommercial',\n",
       " 'social',\n",
       " 'media',\n",
       " 'networks',\n",
       " 'such',\n",
       " 'Mastodon',\n",
       " 'even',\n",
       " 'allow',\n",
       " 'you',\n",
       " 'to',\n",
       " 'create',\n",
       " 'your',\n",
       " 'own',\n",
       " 'custom',\n",
       " 'emojis.footnote',\n",
       " ':',\n",
       " '[',\n",
       " 'Mastodon',\n",
       " 'servers',\n",
       " 'you',\n",
       " 'can',\n",
       " 'join',\n",
       " '(',\n",
       " 'https',\n",
       " ':',\n",
       " '//proai.org/mastoserv',\n",
       " ')',\n",
       " ']',\n",
       " 'footnote',\n",
       " ':',\n",
       " '[',\n",
       " 'Mastodon',\n",
       " 'custom',\n",
       " 'emoji',\n",
       " 'documentation',\n",
       " '(',\n",
       " 'https',\n",
       " ':',\n",
       " '//docs.joinmastodon.org/methods/custom_emojis/',\n",
       " ')',\n",
       " ']',\n",
       " '.Emojis',\n",
       " 'and',\n",
       " 'Emoticons',\n",
       " '[',\n",
       " 'NOTE',\n",
       " ']',\n",
       " '====',\n",
       " '_Emoticons_',\n",
       " 'were',\n",
       " 'first',\n",
       " 'typed',\n",
       " 'into',\n",
       " 'an',\n",
       " 'ASCII',\n",
       " 'text',\n",
       " 'message',\n",
       " 'in',\n",
       " '1972',\n",
       " 'when',\n",
       " 'Carnegie',\n",
       " 'Mellon',\n",
       " 'researchers',\n",
       " 'mistakenly',\n",
       " 'understood',\n",
       " 'a',\n",
       " 'text',\n",
       " 'message',\n",
       " 'about',\n",
       " 'a',\n",
       " 'mercury',\n",
       " 'spill',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'joke',\n",
       " '.',\n",
       " 'The',\n",
       " 'professor',\n",
       " ',',\n",
       " 'Dr.',\n",
       " 'Scott',\n",
       " 'E.',\n",
       " 'Fahlman',\n",
       " ',',\n",
       " 'suggested',\n",
       " 'that',\n",
       " '`',\n",
       " ':',\n",
       " '-',\n",
       " ')',\n",
       " '`',\n",
       " 'should',\n",
       " 'be',\n",
       " 'appended',\n",
       " 'to',\n",
       " 'messages',\n",
       " 'that',\n",
       " 'were',\n",
       " 'jokes',\n",
       " ',',\n",
       " 'and',\n",
       " '`',\n",
       " ':',\n",
       " '-',\n",
       " '(',\n",
       " '`',\n",
       " 'emoticons',\n",
       " 'should',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'serious',\n",
       " 'warning',\n",
       " 'messages',\n",
       " '.',\n",
       " 'Gosh',\n",
       " ',',\n",
       " 'how',\n",
       " 'far',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'come',\n",
       " '.',\n",
       " '====',\n",
       " 'The',\n",
       " 'plural',\n",
       " 'of',\n",
       " '``',\n",
       " 'emoji',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'either',\n",
       " '``',\n",
       " 'emoji',\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6b53a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30k'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(tokens) / 10_000)}0k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef87c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3313a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "813a3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 929 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ff0e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d549350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30k'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{round(len(tokens) / 10_000)}0k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cfb98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "098a0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall(pattern, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6fbc783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.48 ms ± 88.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "f'{round(len(tokens) / 10_000)}0k'\n",
    "%timeit re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1266bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb8ebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a70bdbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/linuxbrew/.linuxbrew/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/maer/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aec42a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58b1f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "993c9188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 2))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8beb3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "? CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e243fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7eca013",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpevocab = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c1a7c3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', ' a', ' f', ' m', ' n', ' o', ' s'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpevocab[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b14c53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb6ce7db",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.DataFrame(vectors.todense(), columns=bpevocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b49e4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [t[:8] + '...' for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c58efee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>'</th>\n",
       "      <th>'s</th>\n",
       "      <th>...</th>\n",
       "      <th>tt</th>\n",
       "      <th>u</th>\n",
       "      <th>ua</th>\n",
       "      <th>uc</th>\n",
       "      <th>ur</th>\n",
       "      <th>v</th>\n",
       "      <th>va</th>\n",
       "      <th>vi</th>\n",
       "      <th>y</th>\n",
       "      <th>yb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>There's ...</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  a   f   m   n   o   s   t  '  's  ...  tt  u  ua  uc  ur  v  \\\n",
       "There's ...  14   2   1   2   1   2   3   3  1   1  ...   1  4   1   1   2  4   \n",
       "\n",
       "             va  vi  y  yb  \n",
       "There's ...   2   2  1   1  \n",
       "\n",
       "[1 rows x 83 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ba432f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69553f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total'] = df.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bfa69229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>There's ...</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>va</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vi</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yb</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    There's ...  total\n",
       "             14     14\n",
       " a            2      2\n",
       " f            1      1\n",
       " m            2      2\n",
       " n            1      1\n",
       "..          ...    ...\n",
       "v             4      4\n",
       "va            2      2\n",
       "vi            2      2\n",
       "y             1      1\n",
       "yb            1      1\n",
       "\n",
       "[83 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62487aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>There's ...</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   There's ...  total\n",
       "a            6      6\n",
       "s            7      7\n",
       "e            8      8\n",
       "t            9      9\n",
       "            14     14"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('total').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d39a94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n'] = [len(tok) for tok in bpevocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3776d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>There's ...</th>\n",
       "      <th>total</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>su</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>th</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    There's ...  total  n\n",
       "su            3      3  2\n",
       " t            3      3  2\n",
       " s            3      3  2\n",
       "he            3      3  2\n",
       "th            4      4  2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['n'] > 1].sort_values('total').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7e8d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_text = 'Hiking home now'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9c14e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_text.startswith('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7450b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\w+(?:\\'\\w+)?|[^\\w\\s]'\n",
    "'Hi' in re.findall(pattern, hi_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a3693691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Hi' == re.findall(pattern, hi_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb91af69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hiking', 'home', 'now']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, hi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41fccee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f4ba837",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_vectors = np.zeros(\n",
    "    (len(tokens), vocab_size), int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d00244b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "010e63aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'=' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m----> 2\u001b[0m     onehot_vectors[i, \u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: '=' is not in list"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(tokens):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211af5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad193",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df_onehot.shape\n",
    "df_onehot.iloc[:,:8].replace(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6462c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
